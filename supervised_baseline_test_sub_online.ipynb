{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6e3dd2-888c-4541-b02f-d3caaae44ede",
   "metadata": {
    "id": "3f6e3dd2-888c-4541-b02f-d3caaae44ede"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import pprint\n",
    "import typing\n",
    "import random\n",
    "import joblib\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import dcor\n",
    "import math\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import scipy.stats as stats\n",
    "import scipy.special as special\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LassoCV\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from collections import Counter, defaultdict\n",
    "from econml.dml import CausalForestDML, LinearDML\n",
    "from causallearn.graph.GraphNode import GraphNode\n",
    "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.search.ScoreBased.ExactSearch import bic_exact_search\n",
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "from causallearn.search.PermutationBased.GRaSP import grasp\n",
    "import semopy\n",
    "from semopy import Model\n",
    "from semopy.inspector import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98414dd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98414dd1",
    "outputId": "d805a278-6422-4544-cce9-5060ad26d85a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n"
     ]
    }
   ],
   "source": [
    "import crunch\n",
    "crunch = crunch.load_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd05a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DAG的工具函数\"\"\"\n",
    "def graph_nodes_representation(graph, nodelist):\n",
    "    \"\"\"\n",
    "    Create an alternative representation of a graph which is hashable\n",
    "    and equivalent graphs have the same hash.\n",
    "\n",
    "    Python cannot PROPERLY use nx.Graph/DiGraph as key for\n",
    "    dictionaries, because two equivalent graphs with just different\n",
    "    order of the nodes would result in different keys. This is\n",
    "    undesirable here.\n",
    "\n",
    "    So here we transform the graph into an equivalent form that is\n",
    "    based on a specific nodelist and that is hashable. In this way,\n",
    "    two equivalent graphs, once transformed, will result in identical\n",
    "    keys.\n",
    "\n",
    "    So we use the following trick: extract the adjacency matrix\n",
    "    (with nodes in a fixed order) and then make a hashable thing out\n",
    "    of it, through tuple(array.flatten()):\n",
    "    \"\"\"\n",
    "\n",
    "    # This get the adjacency matrix with nodes in a given order, as\n",
    "    # numpy array (which is not hashable):\n",
    "    adjacency_matrix = nx.adjacency_matrix(graph, nodelist=nodelist).todense()\n",
    "\n",
    "    # This transforms the numpy array into a hashable object:\n",
    "    hashable = tuple(adjacency_matrix.flatten())\n",
    "\n",
    "    return hashable\n",
    "\n",
    "def create_graph_label():\n",
    "    \"\"\"\n",
    "    Create a dictionary from graphs to labels, in two formats.\n",
    "    \"\"\"\n",
    "    graph_label = {\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]):             \"Cause of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]):             \"Cause of Y\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]):             \"Consequence of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]):             \"Consequence of Y\",\n",
    "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}):                \"Independent\",\n",
    "    }\n",
    "\n",
    "    nodelist = [\"v\", \"X\", \"Y\"]\n",
    "\n",
    "    # This is an equivalent alternative to graph_label but in a form\n",
    "    # for which two equivalent graphs have the same key:\n",
    "    adjacency_label = {\n",
    "        graph_nodes_representation(graph, nodelist): label\n",
    "        for graph, label in graph_label.items()\n",
    "    }\n",
    "\n",
    "    return graph_label, adjacency_label\n",
    "\n",
    "def get_labels(adjacency_matrix, adjacency_label):\n",
    "    \"\"\"\n",
    "    Transform an adjacency_matrix (as pd.DataFrame) into a dictionary of variable:label\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
    "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]  # this is not hashable\n",
    "        key = tuple(submatrix.values.flatten())  # this is hashable and a compatible with adjacency_label\n",
    "    \n",
    "        result[variable] = adjacency_label[key]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf77b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \"\"\"数据增强-旧\"\"\"\n",
    "# def apply_mapping(df, mapping):\n",
    "#     df_new = df.copy()\n",
    "#     # 创建临时映射以避免冲突\n",
    "#     temp_mapping = {k: f'_temp_{k}' for k in mapping.keys()}\n",
    "#     df_new.rename(columns=temp_mapping, inplace=True)\n",
    "#     if df_new.shape[0] == df_new.shape[1]:  # 如果是方阵，如标签矩阵\n",
    "#         df_new.rename(index=temp_mapping, inplace=True)\n",
    "#     # 应用最终映射\n",
    "#     final_mapping = {f'_temp_{k}': v for k, v in mapping.items()}\n",
    "#     df_new.rename(columns=final_mapping, inplace=True)\n",
    "#     if df_new.shape[0] == df_new.shape[1]:\n",
    "#         df_new.rename(index=final_mapping, inplace=True)\n",
    "#     return df_new\n",
    "\n",
    "# def check_duplicate_columns(df):\n",
    "#     \"\"\"检查是否存在重复的列名\"\"\"\n",
    "#     return df.columns.duplicated().any()\n",
    "\n",
    "# def augment_data(X_train, y_train, augment_factor=1.5):\n",
    "#     \"\"\"\n",
    "#     扩增数据集。\n",
    "    \n",
    "#     参数:\n",
    "#     - X_train: dict, 原始特征矩阵，键为样本ID，值为DataFrame\n",
    "#     - y_train: dict, 原始标签矩阵，键为样本ID，值为DataFrame\n",
    "#     - augment_factor: float, 数据扩增的倍数，例如2.5表示每个样本生成2到3个增强样本\n",
    "#     \"\"\"\n",
    "#     new_X_train = X_train.copy()\n",
    "#     new_y_train = y_train.copy()\n",
    "    \n",
    "#     for sample_id in X_train.keys():\n",
    "#         X = X_train[sample_id]\n",
    "#         y = y_train[sample_id]\n",
    "#         variables = list(X.columns)\n",
    "#         dim = len(variables)\n",
    "#         # 提取因果关系对\n",
    "#         edges = []\n",
    "#         for u in y.index:\n",
    "#             for v in y.columns:\n",
    "#                 if y.loc[u, v] == 1:\n",
    "#                     edges.append((u, v))\n",
    "#         # 根据维度决定使用哪些边\n",
    "#         if dim >= 4:\n",
    "#             edges_no_XY = [(u, v) for (u, v) in edges if u not in ['X', 'Y'] and v not in ['X', 'Y']]\n",
    "#             edges_to_use_base = edges_no_XY\n",
    "#         else:\n",
    "#             edges_to_use_base = edges\n",
    "\n",
    "#         # 计算每个样本需要生成的增强样本数量\n",
    "#         integer_part = math.floor(augment_factor)\n",
    "#         fractional_part = augment_factor - integer_part\n",
    "        \n",
    "#         for augment_num in range(integer_part):\n",
    "#             # 生成整数部分的增强样本\n",
    "#             if dim >= 4:\n",
    "#                 edges_to_use = edges_no_XY.copy()\n",
    "#             else:\n",
    "#                 edges_to_use = edges.copy()\n",
    "            \n",
    "#             if not edges_to_use:\n",
    "#                 # 没有边可用，直接复制原始数据\n",
    "#                 new_sample_id = f'{augment_num}{sample_id}'\n",
    "#                 new_X_train[new_sample_id] = X.copy()\n",
    "#                 new_y_train[new_sample_id] = y.copy()\n",
    "#                 continue\n",
    "            \n",
    "#             attempts = 0\n",
    "#             success = False\n",
    "#             while attempts < 3 and not success:\n",
    "#                 if not edges_to_use:\n",
    "#                     break  # 没有合适的边，跳出循环\n",
    "#                 u, v = random.choice(edges_to_use)\n",
    "#                 mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
    "#                 # 应用映射到特征矩阵和标签矩阵\n",
    "#                 X_new = apply_mapping(X, mapping)\n",
    "#                 y_new = apply_mapping(y, mapping)\n",
    "#                 # 检查特征矩阵是否有重复列\n",
    "#                 if check_duplicate_columns(X_new):\n",
    "#                     attempts += 1\n",
    "#                     edges_to_use.remove((u, v))  # 移除当前选择，避免重复尝试\n",
    "#                     continue  # 重试\n",
    "#                 else:\n",
    "#                     # 没有重复列，存储新的数据\n",
    "#                     new_sample_id = f'{augment_num}{sample_id}'\n",
    "#                     new_X_train[new_sample_id] = X_new\n",
    "#                     new_y_train[new_sample_id] = y_new\n",
    "#                     success = True\n",
    "#             if not success:\n",
    "#                 # 没有找到合适的映射，复制原始数据\n",
    "#                 new_sample_id = f'{augment_num}{sample_id}'\n",
    "#                 new_X_train[new_sample_id] = X.copy()\n",
    "#                 new_y_train[new_sample_id] = y.copy()\n",
    "        \n",
    "#         # 处理小数部分\n",
    "#         if fractional_part > 0:\n",
    "#             if random.random() < fractional_part:\n",
    "#                 augment_num = integer_part  # 例如，2.5 -> 2\n",
    "#                 if dim >= 4:\n",
    "#                     edges_to_use = edges_no_XY.copy()\n",
    "#                 else:\n",
    "#                     edges_to_use = edges.copy()\n",
    "                \n",
    "#                 if not edges_to_use:\n",
    "#                     # 没有边可用，直接复制原始数据\n",
    "#                     new_sample_id = f'{augment_num}{sample_id}'\n",
    "#                     new_X_train[new_sample_id] = X.copy()\n",
    "#                     new_y_train[new_sample_id] = y.copy()\n",
    "#                 else:\n",
    "#                     attempts = 0\n",
    "#                     success = False\n",
    "#                     while attempts < 3 and not success:\n",
    "#                         if not edges_to_use:\n",
    "#                             break  # 没有合适的边，跳出循环\n",
    "#                         u, v = random.choice(edges_to_use)\n",
    "#                         mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
    "#                         # 应用映射到特征矩阵和标签矩阵\n",
    "#                         X_new = apply_mapping(X, mapping)\n",
    "#                         y_new = apply_mapping(y, mapping)\n",
    "#                         # 检查特征矩阵是否有重复列\n",
    "#                         if check_duplicate_columns(X_new):\n",
    "#                             attempts += 1\n",
    "#                             edges_to_use.remove((u, v))  # 移除当前选择，避免重复尝试\n",
    "#                             continue  # 重试\n",
    "#                         else:\n",
    "#                             # 没有重复列，存储新的数据\n",
    "#                             new_sample_id = f'{augment_num}{sample_id}'\n",
    "#                             new_X_train[new_sample_id] = X_new\n",
    "#                             new_y_train[new_sample_id] = y_new\n",
    "#                             success = True\n",
    "#                     if not success:\n",
    "#                         # 没有找到合适的映射，复制原始数据\n",
    "#                         new_sample_id = f'{augment_num}{sample_id}'\n",
    "#                         new_X_train[new_sample_id] = X.copy()\n",
    "#                         new_y_train[new_sample_id] = y.copy()\n",
    "    \n",
    "#     return new_X_train, new_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c98332ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"数据增强-新\"\"\"\n",
    "def apply_mapping(df, mapping, rename_index=False):\n",
    "    df_new = df.copy()\n",
    "    # 创建临时映射以避免冲突\n",
    "    temp_mapping = {k: f'_temp_{k}' for k in mapping.keys()}\n",
    "    df_new.rename(columns=temp_mapping, inplace=True)\n",
    "    if rename_index:\n",
    "        df_new.rename(index=temp_mapping, inplace=True)\n",
    "    # 应用最终映射\n",
    "    final_mapping = {temp_mapping[k]: mapping[k] for k in mapping.keys()}\n",
    "    df_new.rename(columns=final_mapping, inplace=True)\n",
    "    if rename_index:\n",
    "        df_new.rename(index=final_mapping, inplace=True)\n",
    "    return df_new\n",
    "\n",
    "def check_duplicate_columns(df):\n",
    "    \"\"\"检查是否存在重复的列名\"\"\"\n",
    "    return df.columns.duplicated().any()\n",
    "\n",
    "def augment_data(X_train, y_train, augment_factor=1.5):\n",
    "    \"\"\"\n",
    "    扩增数据集。\n",
    "\n",
    "    参数:\n",
    "    - X_train: dict, 原始特征矩阵，键为样本ID，值为DataFrame\n",
    "    - y_train: dict, 原始标签矩阵，键为样本ID，值为DataFrame\n",
    "    - augment_factor: float, 数据扩增的倍数，例如2.5表示每个样本生成2到3个增强样本\n",
    "    \"\"\"\n",
    "    new_X_train = X_train.copy()\n",
    "    new_y_train = y_train.copy()\n",
    "    failure_counts = {dim: 0 for dim in range(3, 11)}  # 初始化失败计数器\n",
    "\n",
    "    for sample_id in X_train.keys():\n",
    "        X = X_train[sample_id]\n",
    "        y = y_train[sample_id]\n",
    "        variables = list(map(str, X.columns))\n",
    "        dim = len(variables)\n",
    "\n",
    "        # 将变量名都转换为字符串类型\n",
    "        X.columns = X.columns.astype(str)\n",
    "        if set(X.index) == set(X.columns):\n",
    "            X.index = X.index.astype(str)\n",
    "        y.columns = y.columns.astype(str)\n",
    "        y.index = y.index.astype(str)\n",
    "\n",
    "        # 第一步：将 'X' 和 'Y' 重命名为未使用的数字\n",
    "        used_variables = set(X.columns) | set(y.index) | set(y.columns)\n",
    "        all_numbers = set(map(str, range(10)))  # '0' 到 '9' 的字符串集合\n",
    "        unused_numbers = list(all_numbers - used_variables)\n",
    "\n",
    "        mapping_XY = {}\n",
    "        if 'X' in used_variables:\n",
    "            if unused_numbers:\n",
    "                new_X_name = unused_numbers.pop()\n",
    "                mapping_XY['X'] = new_X_name\n",
    "            else:\n",
    "                # 没有未使用的数字，无法重命名 'X'\n",
    "                failure_counts[dim] += 1\n",
    "                continue  # 跳过此样本\n",
    "        if 'Y' in used_variables:\n",
    "            if unused_numbers:\n",
    "                new_Y_name = unused_numbers.pop()\n",
    "                mapping_XY['Y'] = new_Y_name\n",
    "            else:\n",
    "                # 没有未使用的数字，无法重命名 'Y'\n",
    "                failure_counts[dim] += 1\n",
    "                continue  # 跳过此样本\n",
    "        if mapping_XY:\n",
    "            X = X.rename(columns=mapping_XY)\n",
    "            if set(X.index) == set(X.columns):\n",
    "                X = X.rename(index=mapping_XY)\n",
    "            y = y.rename(index=mapping_XY, columns=mapping_XY)\n",
    "            # 更新变量名\n",
    "            variables = list(X.columns)\n",
    "\n",
    "        # 重新提取因果关系对\n",
    "        edges = []\n",
    "        for u in y.index:\n",
    "            for v in y.columns:\n",
    "                if y.loc[u, v] == 1:\n",
    "                    edges.append((u, v))\n",
    "\n",
    "        # 计算每个样本需要生成的增强样本数量\n",
    "        integer_part = math.floor(augment_factor)\n",
    "        fractional_part = augment_factor - integer_part\n",
    "        edges_to_use = edges.copy()\n",
    "        \n",
    "        edges_to_use.remove((mapping_XY['X'], mapping_XY['Y']))\n",
    "        for augment_num in range(integer_part):\n",
    "            \n",
    "            # 移除之前映射的 (X, Y)\n",
    "\n",
    "            if not edges_to_use:\n",
    "                # 没有边可用，计数失败次数\n",
    "                failure_counts[dim] += 1\n",
    "                continue\n",
    "\n",
    "            attempts = 0\n",
    "            success = False\n",
    "            while attempts < 3 and not success:\n",
    "                if not edges_to_use:\n",
    "                    break  # 没有合适的边，跳出循环\n",
    "                u, v = random.choice(edges_to_use)\n",
    "                edges_to_use.remove((u, v))\n",
    "                mapping = {u: 'X', v: 'Y'}\n",
    "                # 应用映射到特征矩阵和标签矩阵\n",
    "                X_new = apply_mapping(X, mapping)\n",
    "                y_new = apply_mapping(y, mapping, rename_index=True)\n",
    "                # 检查特征矩阵是否有重复列\n",
    "                if check_duplicate_columns(X_new):\n",
    "                    attempts += 1\n",
    "                    continue  # 重试\n",
    "                else:\n",
    "                    # 没有重复列，存储新的数据\n",
    "                    new_sample_id = f'{augment_num}{sample_id}'\n",
    "                    new_X_train[new_sample_id] = X_new\n",
    "                    new_y_train[new_sample_id] = y_new\n",
    "                    success = True\n",
    "            if not success:\n",
    "                # 计数失败次数\n",
    "                failure_counts[dim] += 1\n",
    "\n",
    "        # 处理小数部分\n",
    "        if fractional_part > 0:\n",
    "            if random.random() < fractional_part:\n",
    "                augment_num = integer_part\n",
    "                if not edges_to_use:\n",
    "                    failure_counts[dim] += 1\n",
    "                else:\n",
    "                    attempts = 0\n",
    "                    success = False\n",
    "                    while attempts < 3 and not success:\n",
    "                        if not edges_to_use:\n",
    "                            break  # 没有合适的边，跳出循环\n",
    "                        u, v = random.choice(edges_to_use)\n",
    "                        edges_to_use.remove((u, v))\n",
    "                        mapping = {u: 'X', v: 'Y'}\n",
    "                        # 应用映射到特征矩阵和标签矩阵\n",
    "                        X_new = apply_mapping(X, mapping)\n",
    "                        y_new = apply_mapping(y, mapping, rename_index=True)\n",
    "                        # 检查特征矩阵是否有重复列\n",
    "                        if check_duplicate_columns(X_new):\n",
    "                            attempts += 1\n",
    "                            continue  # 重试\n",
    "                        else:\n",
    "                            # 没有重复列，存储新的数据\n",
    "                            new_sample_id = f'{augment_num}{sample_id}'\n",
    "                            new_X_train[new_sample_id] = X_new\n",
    "                            new_y_train[new_sample_id] = y_new\n",
    "                            success = True\n",
    "                    if not success:\n",
    "                        # 计数失败次数\n",
    "                        failure_counts[dim] += 1\n",
    "\n",
    "    return new_X_train, new_y_train, failure_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d7d3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download data\\X_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_train.pickle (1523944532 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_train.pickle (7017735 bytes)\n",
      "already exists: file length match\n",
      "download data\\X_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_test_reduced.pickle (122341879 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_test_reduced.pickle (562930 bytes)\n",
      "already exists: file length match\n",
      "download data\\example_prediction.parquet from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/example_prediction_reduced.parquet (668981 bytes)\n",
      "already exists: file length match\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test = crunch.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0920048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Copula Entropy的工具函数\"\"\"\n",
    "###  Cite: https://github.com/majianthu/pycopent\n",
    "from scipy.special import digamma\n",
    "from scipy.stats import rankdata as rank \n",
    "from scipy.spatial.distance import cdist\n",
    "from math import gamma, log, pi\n",
    "from numpy import array, abs, max, hstack, vstack, ones, zeros, cov, matrix, where\n",
    "from numpy.random import uniform, normal as rnorm\n",
    "from numpy.linalg import det\n",
    "from multiprocessing.pool import Pool,ThreadPool\n",
    "import sys\n",
    "\n",
    "##### constructing empirical copula density [1]\n",
    "def construct_empirical_copula(x):\n",
    "\t(N,d) = x.shape\t\n",
    "\txc = zeros([N,d]) \n",
    "\tfor i in range(0,d):\n",
    "\t\txc[:,i] = rank(x[:,i]) / N\n",
    "\t\n",
    "\treturn xc\n",
    "##### Estimating entropy with kNN method [2]\n",
    "def entknn(x, k = 3, dtype = 'chebychev'):\n",
    "\t(N,d) = x.shape\n",
    "\t\n",
    "\tg1 = digamma(N) - digamma(k)\n",
    "\t\n",
    "\tif dtype == 'euclidean':\n",
    "\t\tcd = pi**(d/2) / 2**d / gamma(1+d/2)\n",
    "\telse:\t# (chebychev) maximum distance\n",
    "\t\tcd = 1;\n",
    "\n",
    "\tlogd = 0\n",
    "\tdists = cdist(x, x, dtype)\n",
    "\tdists.sort()\n",
    "\tfor i in range(0,N):\n",
    "\t\tlogd = logd + log( 2 * dists[i,k] ) * d / N\n",
    "\n",
    "\treturn (g1 + log(cd) + logd)\n",
    "##### 2-step Nonparametric estimation of copula entropy [1]\n",
    "def copent(x, k = 3, dtype = 'chebychev', log0 = False):\n",
    "\txarray = array(x)\n",
    "\n",
    "\tif log0:\n",
    "\t\t(N,d) = xarray.shape\n",
    "\t\tmax1 = max(abs(xarray), axis = 0)\n",
    "\t\tfor i in range(0,d):\n",
    "\t\t\tif max1[i] == 0:\n",
    "\t\t\t\txarray[:,i] = rnorm(0,1,N)\n",
    "\t\t\telse:\n",
    "\t\t\t\txarray[:,i] = xarray[:,i] + rnorm(0,1,N) * max1[i] * 0.000005\n",
    "\n",
    "\txc = construct_empirical_copula(xarray)\n",
    "\n",
    "\ttry:\n",
    "\t\treturn -entknn(xc, k, dtype)\n",
    "\texcept ValueError: # log0 error\n",
    "\t\treturn copent(x, k, dtype, log0 = True)\n",
    "\n",
    "##### conditional independence test [3]\n",
    "##### to test independence of (x,y) conditioned on z\n",
    "def ci(x, y, z, k = 3, dtype = 'chebychev'):\n",
    "\txyz = vstack((x,y,z)).T\n",
    "\tyz = vstack((y,z)).T\n",
    "\txz = vstack((x,z)).T\n",
    "\treturn copent(xyz,k,dtype) - copent(yz,k,dtype) - copent(xz,k,dtype)\n",
    "##### estimating transfer entropy from y to x with lag [3]\n",
    "def transent(x, y, lag = 1, k = 3, dtype = 'chebychev'):\n",
    "\txlen = len(x)\n",
    "\tylen = len(y)\n",
    "\tif (xlen > ylen):\n",
    "\t\tl = ylen\n",
    "\telse:\n",
    "\t\tl = xlen\n",
    "\tif (l < (lag + k + 1)):\n",
    "\t\treturn 0\n",
    "\tx1 = x[0:(l-lag)]\n",
    "\tx2 = x[lag:l]\n",
    "\ty = y[0:(l-lag)]\n",
    "\treturn ci(x2,y,x1,k,dtype)\n",
    "##### multivariate normality test [4]\n",
    "def mvnt(x, k = 3, dtype = 'chebychev'):\n",
    "\treturn -0.5 * log(det(cov(x.T))) - copent(x,k,dtype)\n",
    "##### two-sample test [5]\n",
    "def tst(s0,s1,n=12, k = 3, dtype = 'chebychev'):\n",
    "\t(N0,d0) = s0.shape\n",
    "\t(N1,d1) = s1.shape\n",
    "\tx = vstack((s0,s1))\n",
    "\tstat1 = 0\n",
    "\tfor i in range(0,n):\n",
    "\t\ty1 = vstack((ones([N0,1]),ones([N1,1])*2)) + uniform(0, 0.0000001,[N0+N1,1])\n",
    "\t\ty0 = ones([N0+N1,1]) + uniform(0,0.0000001,[N0+N1,1])\n",
    "\t\tstat1 = stat1 + copent(hstack((x,y1)),k,dtype) - copent(hstack((x,y0)),k,dtype)\n",
    "\treturn stat1/n\n",
    "##### single change point detection [6]\n",
    "def init(X,N,K,DTYPE):\n",
    "\tglobal x,n,k,dtype\n",
    "\tx = X\n",
    "\tn = N\n",
    "\tk = K\n",
    "\tdtype = DTYPE\n",
    "\n",
    "def tsti(i):\n",
    "\ts0 = x[0:(i+1),:]\n",
    "\ts1 = x[(i+2):,:]\n",
    "\treturn tst(s0,s1,n,k,dtype)\n",
    "\t\n",
    "def cpd(x, thd = 0.13, n = 30, k = 3, dtype = 'chebychev'):\n",
    "\tx = matrix(x)\n",
    "\tlen1 = x.shape[0]\n",
    "\tif len1 == 1:\n",
    "\t\tlen1 = x.shape[1]\n",
    "\t\tx = x.T\n",
    "\tpos = -1\n",
    "\tmaxstat = 0\n",
    "\tif sys.platform.startswith(\"win\"): # \"win\"\n",
    "\t\tpool = ThreadPool(initializer = init, initargs=(x,n,k,dtype))\n",
    "\telse: # \"linux\" or \"darwin\"\n",
    "\t\tpool = Pool(initializer = init, initargs=(x,n,k,dtype))\n",
    "\tstat1 = [0] + pool.map(tsti,range(len1-2)) + [0]\n",
    "\tpool.close()\n",
    "\tif(max(stat1) > thd):\n",
    "\t\tmaxstat = max(stat1)\n",
    "\t\tpos = where(stat1 == maxstat)[0][0]+1\n",
    "\treturn pos, maxstat, stat1\n",
    "##### multiple change point detection [6]\n",
    "def mcpd(x, maxp = 5, thd = 0.13, minseglen = 10, n = 30, k = 3, dtype = 'chebychev'):\n",
    "\tx = matrix(x)\n",
    "\tlen1 = x.shape[0]\n",
    "\tif len1 == 1:\n",
    "\t\tlen1 = x.shape[1]\n",
    "\t\tx = x.T\n",
    "\tmaxstat = []\n",
    "\tpos = []\n",
    "\tbisegs = matrix([0,len1-1])\n",
    "\tfor i in range(0,maxp):\n",
    "\t\tif i >= bisegs.shape[0]:\n",
    "\t\t\tbreak\n",
    "\t\trpos, rmaxstat, _ = cpd(x[bisegs[i,0]:bisegs[i,1],:],thd,n,k,dtype)\n",
    "\t\tif rpos > -1 :\n",
    "\t\t\trpos = rpos + bisegs[i,0]\n",
    "\t\t\tmaxstat.append(rmaxstat)\n",
    "\t\t\tpos.append(rpos)\n",
    "\t\t\tif (rpos - bisegs[i,0]) > minseglen :\n",
    "\t\t\t\tbisegs = vstack((bisegs,[bisegs[i,0],rpos-1]))\n",
    "\t\t\tif (bisegs[i,1] - rpos +1) > minseglen :\n",
    "\t\t\t\tbisegs = vstack((bisegs,[rpos,bisegs[i,1]]))\n",
    "\treturn pos,maxstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1cac741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PPS\"\"\"\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, f1_score\n",
    "from pandas.api.types import (\n",
    "    is_numeric_dtype,\n",
    "    is_bool_dtype,\n",
    "    is_object_dtype,\n",
    "    is_categorical_dtype,\n",
    "    is_string_dtype,\n",
    "    is_datetime64_any_dtype,\n",
    "    is_timedelta64_dtype,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_model_cv_score_(\n",
    "    df, target, feature, task, cross_validation, random_seed, **kwargs\n",
    "):\n",
    "    \"Calculates the mean model score based on cross-validation\"\n",
    "    # Sources about the used methods:\n",
    "    # https://scikit-learn.org/stable/modules/tree.html\n",
    "    # https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "    metric = task[\"metric_key\"]\n",
    "    model = task[\"model\"]\n",
    "    # shuffle the rows - this is important for cross-validation\n",
    "    # because the cross-validation just takes the first n lines\n",
    "    # if there is a strong pattern in the rows eg 0,0,0,0,1,1,1,1\n",
    "    # then this will lead to problems because the first cv sees mostly 0 and the later 1\n",
    "    # this approach might be wrong for timeseries because it might leak information\n",
    "    df = df.sample(frac=1, random_state=random_seed, replace=False)\n",
    "\n",
    "    # preprocess target\n",
    "    if task[\"type\"] == \"classification\":\n",
    "        label_encoder = preprocessing.LabelEncoder()\n",
    "        df[target] = label_encoder.fit_transform(df[target])\n",
    "        target_series = df[target]\n",
    "    else:\n",
    "        target_series = df[target]\n",
    "\n",
    "    # preprocess feature\n",
    "    if _dtype_represents_categories(df[feature]):\n",
    "        one_hot_encoder = preprocessing.OneHotEncoder()\n",
    "        array = df[feature].__array__()\n",
    "        sparse_matrix = one_hot_encoder.fit_transform(array.reshape(-1, 1))\n",
    "        feature_input = sparse_matrix\n",
    "    else:\n",
    "        # reshaping needed because there is only 1 feature\n",
    "        array = df[feature].values\n",
    "        if not isinstance(array, np.ndarray):  # e.g Int64 IntegerArray\n",
    "            array = array.to_numpy()\n",
    "        feature_input = array.reshape(-1, 1)\n",
    "\n",
    "    # Cross-validation is stratifiedKFold for classification, KFold for regression\n",
    "    # CV on one core (n_job=1; default) has shown to be fastest\n",
    "    scores = cross_val_score(\n",
    "        model, feature_input, target_series.to_numpy(), cv=cross_validation, scoring=metric\n",
    "    )\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def _normalized_mae_score(model_mae, naive_mae):\n",
    "    \"Normalizes the model MAE score, given the baseline score\"\n",
    "    # # Value range of MAE is [0, infinity), 0 is best\n",
    "    # 10, 5 ==> 0 because worse than naive\n",
    "    # 10, 20 ==> 0.5\n",
    "    # 5, 20 ==> 0.75 = 1 - (mae/base_mae)\n",
    "    if model_mae > naive_mae:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 - (model_mae / naive_mae)\n",
    "\n",
    "\n",
    "def _mae_normalizer(df, y, model_score, **kwargs):\n",
    "    \"In case of MAE, calculates the baseline score for y and derives the PPS.\"\n",
    "    df[\"naive\"] = df[y].median()\n",
    "    baseline_score = mean_absolute_error(df[y].to_numpy(), df[\"naive\"].to_numpy())  # true, pred\n",
    "\n",
    "    ppscore = _normalized_mae_score(abs(model_score), baseline_score)\n",
    "    return ppscore, baseline_score\n",
    "\n",
    "\n",
    "def _normalized_f1_score(model_f1, baseline_f1):\n",
    "    \"Normalizes the model F1 score, given the baseline score\"\n",
    "    # # F1 ranges from 0 to 1\n",
    "    # # 1 is best\n",
    "    # 0.5, 0.7 ==> 0 because model is worse than naive baseline\n",
    "    # 0.75, 0.5 ==> 0.5\n",
    "    #\n",
    "    if model_f1 < baseline_f1:\n",
    "        return 0\n",
    "    else:\n",
    "        scale_range = 1.0 - baseline_f1  # eg 0.3\n",
    "        f1_diff = model_f1 - baseline_f1  # eg 0.1\n",
    "        return f1_diff / scale_range  # 0.1/0.3 = 0.33\n",
    "\n",
    "\n",
    "def _f1_normalizer(df, y, model_score, random_seed):\n",
    "    \"In case of F1, calculates the baseline score for y and derives the PPS.\"\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    df[\"truth\"] = label_encoder.fit_transform(df[y])\n",
    "    df[\"most_common_value\"] = df[\"truth\"].value_counts().index[0]\n",
    "    random = df[\"truth\"].sample(frac=1, random_state=random_seed)\n",
    "\n",
    "    baseline_score = max(\n",
    "        f1_score(df[\"truth\"], df[\"most_common_value\"], average=\"weighted\"),\n",
    "        f1_score(df[\"truth\"], random, average=\"weighted\"),\n",
    "    )\n",
    "\n",
    "    ppscore = _normalized_f1_score(model_score, baseline_score)\n",
    "    return ppscore, baseline_score\n",
    "\n",
    "\n",
    "VALID_CALCULATIONS = {\n",
    "    \"regression\": {\n",
    "        \"type\": \"regression\",\n",
    "        \"is_valid_score\": True,\n",
    "        \"model_score\": -1,\n",
    "        \"baseline_score\": -1,\n",
    "        \"ppscore\": -1,\n",
    "        \"metric_name\": \"mean absolute error\",\n",
    "        \"metric_key\": \"neg_mean_absolute_error\",\n",
    "        \"model\": tree.DecisionTreeRegressor(),\n",
    "        \"score_normalizer\": _mae_normalizer,\n",
    "    },\n",
    "    \"classification\": {\n",
    "        \"type\": \"classification\",\n",
    "        \"is_valid_score\": True,\n",
    "        \"model_score\": -1,\n",
    "        \"baseline_score\": -1,\n",
    "        \"ppscore\": -1,\n",
    "        \"metric_name\": \"weighted F1\",\n",
    "        \"metric_key\": \"f1_weighted\",\n",
    "        \"model\": tree.DecisionTreeClassifier(),\n",
    "        \"score_normalizer\": _f1_normalizer,\n",
    "    },\n",
    "    \"predict_itself\": {\n",
    "        \"type\": \"predict_itself\",\n",
    "        \"is_valid_score\": True,\n",
    "        \"model_score\": 1,\n",
    "        \"baseline_score\": 0,\n",
    "        \"ppscore\": 1,\n",
    "        \"metric_name\": None,\n",
    "        \"metric_key\": None,\n",
    "        \"model\": None,\n",
    "        \"score_normalizer\": None,\n",
    "    },\n",
    "    \"target_is_constant\": {\n",
    "        \"type\": \"target_is_constant\",\n",
    "        \"is_valid_score\": True,\n",
    "        \"model_score\": 1,\n",
    "        \"baseline_score\": 1,\n",
    "        \"ppscore\": 0,\n",
    "        \"metric_name\": None,\n",
    "        \"metric_key\": None,\n",
    "        \"model\": None,\n",
    "        \"score_normalizer\": None,\n",
    "    },\n",
    "    \"target_is_id\": {\n",
    "        \"type\": \"target_is_id\",\n",
    "        \"is_valid_score\": True,\n",
    "        \"model_score\": 0,\n",
    "        \"baseline_score\": 0,\n",
    "        \"ppscore\": 0,\n",
    "        \"metric_name\": None,\n",
    "        \"metric_key\": None,\n",
    "        \"model\": None,\n",
    "        \"score_normalizer\": None,\n",
    "    },\n",
    "    \"feature_is_id\": {\n",
    "        \"type\": \"feature_is_id\",\n",
    "        \"is_valid_score\": True,\n",
    "        \"model_score\": 0,\n",
    "        \"baseline_score\": 0,\n",
    "        \"ppscore\": 0,\n",
    "        \"metric_name\": None,\n",
    "        \"metric_key\": None,\n",
    "        \"model\": None,\n",
    "        \"score_normalizer\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "INVALID_CALCULATIONS = [\n",
    "    \"target_is_datetime\",\n",
    "    \"target_data_type_not_supported\",\n",
    "    \"empty_dataframe_after_dropping_na\",\n",
    "    \"unknown_error\",\n",
    "]\n",
    "\n",
    "\n",
    "def _dtype_represents_categories(series) -> bool:\n",
    "    \"Determines if the dtype of the series represents categorical values\"\n",
    "    return (\n",
    "        is_bool_dtype(series)\n",
    "        or is_object_dtype(series)\n",
    "        or is_string_dtype(series)\n",
    "        or is_categorical_dtype(series)\n",
    "    )\n",
    "\n",
    "\n",
    "def _determine_case_and_prepare_df(df, x, y, sample=5_000, random_seed=123):\n",
    "    \"Returns str with the name of the determined case based on the columns x and y\"\n",
    "    if x == y:\n",
    "        return df, \"predict_itself\"\n",
    "\n",
    "    df = df[[x, y]]\n",
    "    # IDEA: log.warning when values have been dropped\n",
    "    df = df.dropna()\n",
    "\n",
    "    if len(df) == 0:\n",
    "        return df, \"empty_dataframe_after_dropping_na\"\n",
    "        # IDEA: show warning\n",
    "        # raise Exception(\n",
    "        #     \"After dropping missing values, there are no valid rows left\"\n",
    "        # )\n",
    "\n",
    "    df = _maybe_sample(df, sample, random_seed=random_seed)\n",
    "\n",
    "    if _feature_is_id(df, x):\n",
    "        return df, \"feature_is_id\"\n",
    "\n",
    "    category_count = df[y].value_counts().count()\n",
    "    if category_count == 1:\n",
    "        # it is helpful to separate this case in order to save unnecessary calculation time\n",
    "        return df, \"target_is_constant\"\n",
    "    if _dtype_represents_categories(df[y]) and (category_count == len(df[y])):\n",
    "        # it is important to separate this case in order to save unnecessary calculation time\n",
    "        return df, \"target_is_id\"\n",
    "\n",
    "    if _dtype_represents_categories(df[y]):\n",
    "        return df, \"classification\"\n",
    "    if is_numeric_dtype(df[y]):\n",
    "        # this check needs to be after is_bool_dtype (which is part of _dtype_represents_categories) because bool is considered numeric by pandas\n",
    "        return df, \"regression\"\n",
    "\n",
    "    if is_datetime64_any_dtype(df[y]) or is_timedelta64_dtype(df[y]):\n",
    "        # IDEA: show warning\n",
    "        # raise TypeError(\n",
    "        #     f\"The target column {y} has the dtype {df[y].dtype} which is not supported. A possible solution might be to convert {y} to a string column\"\n",
    "        # )\n",
    "        return df, \"target_is_datetime\"\n",
    "\n",
    "    # IDEA: show warning\n",
    "    # raise Exception(\n",
    "    #     f\"Could not infer a valid task based on the target {y}. The dtype {df[y].dtype} is not yet supported\"\n",
    "    # )  # pragma: no cover\n",
    "    return df, \"target_data_type_not_supported\"\n",
    "\n",
    "\n",
    "def _feature_is_id(df, x):\n",
    "    \"Returns Boolean if the feature column x is an ID\"\n",
    "    if not _dtype_represents_categories(df[x]):\n",
    "        return False\n",
    "\n",
    "    category_count = df[x].value_counts().count()\n",
    "    return category_count == len(df[x])\n",
    "\n",
    "\n",
    "def _maybe_sample(df, sample, random_seed=None):\n",
    "    \"\"\"\n",
    "    Maybe samples the rows of the given df to have at most `sample` rows\n",
    "    If sample is `None` or falsy, there will be no sampling.\n",
    "    If the df has fewer rows than the sample, there will be no sampling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe that might be sampled\n",
    "    sample : int or `None`\n",
    "        Number of rows to be sampled\n",
    "    random_seed : int or `None`\n",
    "        Random seed that is forwarded to pandas.DataFrame.sample as `random_state`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame after potential sampling\n",
    "    \"\"\"\n",
    "    if sample and len(df) > sample:\n",
    "        # this is a problem if x or y have more than sample=5000 categories\n",
    "        # TODO: dont sample when the problem occurs and show warning\n",
    "        df = df.sample(sample, random_state=random_seed, replace=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _is_column_in_df(column, df):\n",
    "    try:\n",
    "        return column in df.columns\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _score(\n",
    "    df, x, y, task, sample, cross_validation, random_seed, invalid_score, catch_errors\n",
    "):\n",
    "    df, case_type = _determine_case_and_prepare_df(\n",
    "        df, x, y, sample=sample, random_seed=random_seed\n",
    "    )\n",
    "    task = _get_task(case_type, invalid_score)\n",
    "\n",
    "    if case_type in [\"classification\", \"regression\"]:\n",
    "        model_score = _calculate_model_cv_score_(\n",
    "            df,\n",
    "            target=y,\n",
    "            feature=x,\n",
    "            task=task,\n",
    "            cross_validation=cross_validation,\n",
    "            random_seed=random_seed,\n",
    "        )\n",
    "        # IDEA: the baseline_scores do sometimes change significantly, e.g. for F1 and thus change the PPS\n",
    "        # we might want to calculate the baseline_score 10 times and use the mean in order to have less variance\n",
    "        ppscore, baseline_score = task[\"score_normalizer\"](\n",
    "            df, y, model_score, random_seed=random_seed\n",
    "        )\n",
    "    else:\n",
    "        model_score = task[\"model_score\"]\n",
    "        baseline_score = task[\"baseline_score\"]\n",
    "        ppscore = task[\"ppscore\"]\n",
    "\n",
    "    return {\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"ppscore\": ppscore,\n",
    "        \"case\": case_type,\n",
    "        \"is_valid_score\": task[\"is_valid_score\"],\n",
    "        \"metric\": task[\"metric_name\"],\n",
    "        \"baseline_score\": baseline_score,\n",
    "        \"model_score\": abs(model_score),  # sklearn returns negative mae\n",
    "        \"model\": task[\"model\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def pps_score(\n",
    "    df,\n",
    "    x,\n",
    "    y,\n",
    "    task=\"NOT_SUPPORTED_ANYMORE\",\n",
    "    sample=5_000,\n",
    "    cross_validation=4,\n",
    "    random_seed=123,\n",
    "    invalid_score=0,\n",
    "    catch_errors=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the Predictive Power Score (PPS) for \"x predicts y\"\n",
    "    The score always ranges from 0 to 1 and is data-type agnostic.\n",
    "\n",
    "    A score of 0 means that the column x cannot predict the column y better than a naive baseline model.\n",
    "    A score of 1 means that the column x can perfectly predict the column y given the model.\n",
    "    A score between 0 and 1 states the ratio of how much potential predictive power the model achieved compared to the baseline model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe that contains the columns x and y\n",
    "    x : str\n",
    "        Name of the column x which acts as the feature\n",
    "    y : str\n",
    "        Name of the column y which acts as the target\n",
    "    sample : int or `None`\n",
    "        Number of rows for sampling. The sampling decreases the calculation time of the PPS.\n",
    "        If `None` there will be no sampling.\n",
    "    cross_validation : int\n",
    "        Number of iterations during cross-validation. This has the following implications:\n",
    "        For example, if the number is 4, then it is possible to detect patterns when there are at least 4 times the same observation. If the limit is increased, the required minimum observations also increase. This is important, because this is the limit when sklearn will throw an error and the PPS cannot be calculated\n",
    "    random_seed : int or `None`\n",
    "        Random seed for the parts of the calculation that require random numbers, e.g. shuffling or sampling.\n",
    "        If the value is set, the results will be reproducible. If the value is `None` a new random number is drawn at the start of each calculation.\n",
    "    invalid_score : any\n",
    "        The score that is returned when a calculation is invalid, e.g. because the data type was not supported.\n",
    "    catch_errors : bool\n",
    "        If `True` all errors will be catched and reported as `unknown_error` which ensures convenience. If `False` errors will be raised. This is helpful for inspecting and debugging errors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        A dict that contains multiple fields about the resulting PPS.\n",
    "        The dict enables introspection into the calculations that have been performed under the hood\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\n",
    "            f\"The 'df' argument should be a pandas.DataFrame but you passed a {type(df)}\\nPlease convert your input to a pandas.DataFrame\"\n",
    "        )\n",
    "    if not _is_column_in_df(x, df):\n",
    "        raise ValueError(\n",
    "            f\"The 'x' argument should be the name of a dataframe column but the variable that you passed is not a column in the given dataframe.\\nPlease review the column name or your dataframe\"\n",
    "        )\n",
    "    if len(df[[x]].columns) >= 2:\n",
    "        raise AssertionError(\n",
    "            f\"The dataframe has {len(df[[x]].columns)} columns with the same column name {x}\\nPlease adjust the dataframe and make sure that only 1 column has the name {x}\"\n",
    "        )\n",
    "    if not _is_column_in_df(y, df):\n",
    "        raise ValueError(\n",
    "            f\"The 'y' argument should be the name of a dataframe column but the variable that you passed is not a column in the given dataframe.\\nPlease review the column name or your dataframe\"\n",
    "        )\n",
    "    if len(df[[y]].columns) >= 2:\n",
    "        raise AssertionError(\n",
    "            f\"The dataframe has {len(df[[y]].columns)} columns with the same column name {y}\\nPlease adjust the dataframe and make sure that only 1 column has the name {y}\"\n",
    "        )\n",
    "\n",
    "    if random_seed is None:\n",
    "        from random import random\n",
    "\n",
    "        random_seed = int(random() * 1000)\n",
    "\n",
    "    try:\n",
    "        return _score(\n",
    "            df,\n",
    "            x,\n",
    "            y,\n",
    "            task,\n",
    "            sample,\n",
    "            cross_validation,\n",
    "            random_seed,\n",
    "            invalid_score,\n",
    "            catch_errors,\n",
    "        )\n",
    "    except Exception as exception:\n",
    "        if catch_errors:\n",
    "            case_type = \"unknown_error\"\n",
    "            task = _get_task(case_type, invalid_score)\n",
    "            return {\n",
    "                \"x\": x,\n",
    "                \"y\": y,\n",
    "                \"ppscore\": task[\"ppscore\"],\n",
    "                \"case\": case_type,\n",
    "                \"is_valid_score\": task[\"is_valid_score\"],\n",
    "                \"metric\": task[\"metric_name\"],\n",
    "                \"baseline_score\": task[\"baseline_score\"],\n",
    "                \"model_score\": task[\"model_score\"],  # sklearn returns negative mae\n",
    "                \"model\": task[\"model\"],\n",
    "            }\n",
    "        else:\n",
    "            raise exception\n",
    "\n",
    "\n",
    "def _get_task(case_type, invalid_score):\n",
    "    VALID_CALCULATIONS = {\n",
    "    \"regression\": {\n",
    "        \"type\": \"regression\",\n",
    "        \"is_valid_score\": True,\n",
    "        \"model_score\": -1,\n",
    "        \"baseline_score\": -1,\n",
    "        \"ppscore\": -1,\n",
    "        \"metric_name\": \"mean absolute error\",\n",
    "        \"metric_key\": \"neg_mean_absolute_error\",\n",
    "        \"model\": tree.DecisionTreeRegressor(),\n",
    "        \"score_normalizer\": _mae_normalizer,\n",
    "    },\n",
    "    \"classification\": {\n",
    "        \"type\": \"classification\",\n",
    "        \"is_valid_score\": True,\n",
    "        \"model_score\": -1,\n",
    "        \"baseline_score\": -1,\n",
    "        \"ppscore\": -1,\n",
    "        \"metric_name\": \"weighted F1\",\n",
    "        \"metric_key\": \"f1_weighted\",\n",
    "        \"model\": tree.DecisionTreeClassifier(),\n",
    "        \"score_normalizer\": _f1_normalizer,\n",
    "    },\n",
    "    \"predict_itself\": {\n",
    "        \"type\": \"predict_itself\",\n",
    "        \"is_valid_score\": True,\n",
    "        \"model_score\": 1,\n",
    "        \"baseline_score\": 0,\n",
    "        \"ppscore\": 1,\n",
    "        \"metric_name\": None,\n",
    "        \"metric_key\": None,\n",
    "        \"model\": None,\n",
    "        \"score_normalizer\": None,\n",
    "    },\n",
    "    \"target_is_constant\": {\n",
    "        \"type\": \"target_is_constant\",\n",
    "        \"is_valid_score\": True,\n",
    "        \"model_score\": 1,\n",
    "        \"baseline_score\": 1,\n",
    "        \"ppscore\": 0,\n",
    "        \"metric_name\": None,\n",
    "        \"metric_key\": None,\n",
    "        \"model\": None,\n",
    "        \"score_normalizer\": None,\n",
    "    },\n",
    "    \"target_is_id\": {\n",
    "        \"type\": \"target_is_id\",\n",
    "        \"is_valid_score\": True,\n",
    "        \"model_score\": 0,\n",
    "        \"baseline_score\": 0,\n",
    "        \"ppscore\": 0,\n",
    "        \"metric_name\": None,\n",
    "        \"metric_key\": None,\n",
    "        \"model\": None,\n",
    "        \"score_normalizer\": None,\n",
    "    },\n",
    "    \"feature_is_id\": {\n",
    "        \"type\": \"feature_is_id\",\n",
    "        \"is_valid_score\": True,\n",
    "        \"model_score\": 0,\n",
    "        \"baseline_score\": 0,\n",
    "        \"ppscore\": 0,\n",
    "        \"metric_name\": None,\n",
    "        \"metric_key\": None,\n",
    "        \"model\": None,\n",
    "        \"score_normalizer\": None,\n",
    "        },\n",
    "    }\n",
    "    INVALID_CALCULATIONS = [\n",
    "    \"target_is_datetime\",\n",
    "    \"target_data_type_not_supported\",\n",
    "    \"empty_dataframe_after_dropping_na\",\n",
    "    \"unknown_error\",\n",
    "    ]   \n",
    "    if case_type in VALID_CALCULATIONS.keys():\n",
    "        return VALID_CALCULATIONS[case_type]\n",
    "    elif case_type in INVALID_CALCULATIONS:\n",
    "        return {\n",
    "            \"type\": case_type,\n",
    "            \"is_valid_score\": False,\n",
    "            \"model_score\": invalid_score,\n",
    "            \"baseline_score\": invalid_score,\n",
    "            \"ppscore\": invalid_score,\n",
    "            \"metric_name\": None,\n",
    "            \"metric_key\": None,\n",
    "            \"model\": None,\n",
    "            \"score_normalizer\": None,\n",
    "        }\n",
    "    raise Exception(f\"case_type {case_type} is not supported\")\n",
    "\n",
    "\n",
    "def _format_list_of_dicts(scores, output, sorted):\n",
    "    \"\"\"\n",
    "    Format list of score dicts `scores`\n",
    "    - maybe sort by ppscore\n",
    "    - maybe return pandas.Dataframe\n",
    "    - output can be one of [\"df\", \"list\"]\n",
    "    \"\"\"\n",
    "    if sorted:\n",
    "        scores.sort(key=lambda item: item[\"ppscore\"], reverse=True)\n",
    "\n",
    "    if output == \"df\":\n",
    "        df_columns = [\n",
    "            \"x\",\n",
    "            \"y\",\n",
    "            \"ppscore\",\n",
    "            \"case\",\n",
    "            \"is_valid_score\",\n",
    "            \"metric\",\n",
    "            \"baseline_score\",\n",
    "            \"model_score\",\n",
    "            \"model\",\n",
    "        ]\n",
    "        data = {column: [score[column] for score in scores] for column in df_columns}\n",
    "        scores = pd.DataFrame.from_dict(data)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def pps_predictors(df, y, output=\"df\", sorted=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculate the Predictive Power Score (PPS) of all the features in the dataframe\n",
    "    against a target column\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe that contains the data\n",
    "    y : str\n",
    "        Name of the column y which acts as the target\n",
    "    output: str - potential values: \"df\", \"list\"\n",
    "        Control the type of the output. Either return a pandas.DataFrame (df) or a list with the score dicts\n",
    "    sorted: bool\n",
    "        Whether or not to sort the output dataframe/list by the ppscore\n",
    "    kwargs:\n",
    "        Other key-word arguments that shall be forwarded to the pps.score method,\n",
    "        e.g. `sample, `cross_validation, `random_seed, `invalid_score`, `catch_errors`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame or list of Dict\n",
    "        Either returns a tidy dataframe or a list of all the PPS dicts. This can be influenced\n",
    "        by the output argument\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\n",
    "            f\"The 'df' argument should be a pandas.DataFrame but you passed a {type(df)}\\nPlease convert your input to a pandas.DataFrame\"\n",
    "        )\n",
    "    if not _is_column_in_df(y, df):\n",
    "        raise ValueError(\n",
    "            f\"The 'y' argument should be the name of a dataframe column but the variable that you passed is not a column in the given dataframe.\\nPlease review the column name or your dataframe\"\n",
    "        )\n",
    "    if len(df[[y]].columns) >= 2:\n",
    "        raise AssertionError(\n",
    "            f\"The dataframe has {len(df[[y]].columns)} columns with the same column name {y}\\nPlease adjust the dataframe and make sure that only 1 column has the name {y}\"\n",
    "        )\n",
    "    if not output in [\"df\", \"list\"]:\n",
    "        raise ValueError(\n",
    "            f\"\"\"The 'output' argument should be one of [\"df\", \"list\"] but you passed: {output}\\nPlease adjust your input to one of the valid values\"\"\"\n",
    "        )\n",
    "    if not sorted in [True, False]:\n",
    "        raise ValueError(\n",
    "            f\"\"\"The 'sorted' argument should be one of [True, False] but you passed: {sorted}\\nPlease adjust your input to one of the valid values\"\"\"\n",
    "        )\n",
    "\n",
    "    scores = [pps_score(df, column, y, **kwargs) for column in df if column != y]\n",
    "\n",
    "    return _format_list_of_dicts(scores=scores, output=output, sorted=sorted)\n",
    "\n",
    "\n",
    "def pps_matrix(df, output=\"df\", sorted=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculate the Predictive Power Score (PPS) matrix for all columns in the dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe that contains the data\n",
    "    output: str - potential values: \"df\", \"list\"\n",
    "        Control the type of the output. Either return a pandas.DataFrame (df) or a list with the score dicts\n",
    "    sorted: bool\n",
    "        Whether or not to sort the output dataframe/list by the ppscore\n",
    "    kwargs:\n",
    "        Other key-word arguments that shall be forwarded to the pps.score method,\n",
    "        e.g. `sample, `cross_validation, `random_seed, `invalid_score`, `catch_errors`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame or list of Dict\n",
    "        Either returns a tidy dataframe or a list of all the PPS dicts. This can be influenced\n",
    "        by the output argument\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\n",
    "            f\"The 'df' argument should be a pandas.DataFrame but you passed a {type(df)}\\nPlease convert your input to a pandas.DataFrame\"\n",
    "        )\n",
    "    if not output in [\"df\", \"list\"]:\n",
    "        raise ValueError(\n",
    "            f\"\"\"The 'output' argument should be one of [\"df\", \"list\"] but you passed: {output}\\nPlease adjust your input to one of the valid values\"\"\"\n",
    "        )\n",
    "    if not sorted in [True, False]:\n",
    "        raise ValueError(\n",
    "            f\"\"\"The 'sorted' argument should be one of [True, False] but you passed: {sorted}\\nPlease adjust your input to one of the valid values\"\"\"\n",
    "        )\n",
    "\n",
    "    scores = [pps_score(df, x, y, **kwargs) for x in df for y in df]\n",
    "\n",
    "    return _format_list_of_dicts(scores=scores, output=output, sorted=sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b3a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"工具函数\"\"\"\n",
    "###### 回归的工具函数\n",
    "def Squared_term(dataset, variables):\n",
    "    for var in variables:\n",
    "        dataset[f'{var}_squared_term'] = dataset[var] ** 2\n",
    "    return dataset\n",
    "\n",
    "def Interaction_term(dataset, variables):\n",
    "    for i in range(len(variables)):\n",
    "        for j in range(i + 1, len(variables)):\n",
    "            dataset[f'{variables[i]}_{variables[j]}'] = dataset[variables[i]] * dataset[variables[j]]\n",
    "    return dataset\n",
    "\n",
    "def Cos_Sin_term(dataset, variables):\n",
    "    for var in variables:\n",
    "        dataset[f'{var}_cos_term'] = np.cos(dataset[var])\n",
    "        dataset[f'{var}_sin_term'] = np.sin(dataset[var])\n",
    "    return dataset\n",
    "\n",
    "def Piecewise_term(dataset, variables):\n",
    "    for var in variables:\n",
    "        dataset[f'{var}_piecewise_term'] = np.maximum(dataset[var] - np.median(dataset[var]), 0)\n",
    "    return dataset\n",
    "\n",
    "###### 因果发现算法的工具函数\n",
    "def handle_multicollinearity(data, epsilon=1e-7, corr_threshold=0.99):\n",
    "    \"\"\"\n",
    "    检查数据中的多重共线性，并在需要时向存在多重共线性的列添加随机扰动。\n",
    "    \n",
    "    参数:\n",
    "    - data (np.ndarray): 输入的数据矩阵，形状为 (样本数, 特征数)。\n",
    "    - epsilon (float): 添加的随机扰动的尺度，默认值为1e-10。\n",
    "    - corr_threshold (float): 判断高相关性的阈值，默认值为0.95。\n",
    "    \n",
    "    返回:\n",
    "    - data (np.ndarray): 处理后的数据矩阵。\n",
    "    \"\"\"\n",
    "    # 计算相关系数矩阵\n",
    "    corr_matrix = np.corrcoef(data, rowvar=False)\n",
    "    n_cols = corr_matrix.shape[0]\n",
    "\n",
    "    # 使用集合存储所有涉及多重共线性的列索引，避免重复\n",
    "    high_corr_indices = set()\n",
    "    \n",
    "    # 遍历相关系数矩阵的上三角部分，寻找高相关的列对\n",
    "    for i in range(n_cols):\n",
    "        for j in range(i+1, n_cols):\n",
    "            if np.abs(corr_matrix[i, j]) > corr_threshold:\n",
    "                high_corr_indices.add(i)\n",
    "                high_corr_indices.add(j)\n",
    "    \n",
    "    if high_corr_indices:\n",
    "        sorted_indices = sorted(high_corr_indices)\n",
    "        # print(f\"检测到多重共线性，涉及的列索引: {sorted_indices}。正在添加随机扰动...\")\n",
    "        \n",
    "        # 生成与高相关性列对应的随机扰动\n",
    "        noise = np.random.normal(0, epsilon, (data.shape[0], len(sorted_indices)))\n",
    "        \n",
    "        # 将扰动添加到相应的列\n",
    "        data[:, sorted_indices] += noise\n",
    "    \n",
    "    return data\n",
    "\n",
    "def convert_bidirectional_to_dag(bidirectional_adj: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    将双向编码的邻接矩阵转换为单向编码的邻接矩阵（DAG表示）。\n",
    "    \n",
    "    在双向编码的邻接矩阵中：\n",
    "    - [j, i] = 1 且 [i, j] = -1 表示 i → j\n",
    "    - [j, i] = 1 且 [i, j] = 1 表示 i ↔ j（互为因果）\n",
    "    - 其他情况 [i, j] = 0 表示无边\n",
    "    \n",
    "    转换后，单向编码的邻接矩阵将：\n",
    "    - [i, j] = 1 表示 i → j\n",
    "    - [i, j] = 1 且 [j, i] = 1 表示 i ↔ j\n",
    "    - [i, j] = 0 表示无边\n",
    "    \n",
    "    参数:\n",
    "    - bidirectional_adj (pd.DataFrame): 双向编码的邻接矩阵，使用 1 和 -1 表示有向边。\n",
    "    \n",
    "    返回:\n",
    "    - pd.DataFrame: 单向编码的邻接矩阵（DAG表示）。\n",
    "    \"\"\"\n",
    "    # 确保输入是一个方阵\n",
    "    if bidirectional_adj.shape[0] != bidirectional_adj.shape[1]:\n",
    "        raise ValueError(\"输入的邻接矩阵必须是方阵（行数等于列数）。\")\n",
    "    \n",
    "    # 确保行列索引一致\n",
    "    if not bidirectional_adj.index.equals(bidirectional_adj.columns):\n",
    "        raise ValueError(\"邻接矩阵的行索引和列索引必须相同。\")\n",
    "    \n",
    "    # 初始化一个全零的邻接矩阵\n",
    "    single_direction_adj = pd.DataFrame(0, index=bidirectional_adj.index, columns=bidirectional_adj.columns)\n",
    "    \n",
    "    # 遍历每一对变量，确定有向边\n",
    "    for source in bidirectional_adj.columns:\n",
    "        for target in bidirectional_adj.index:\n",
    "            if source == target:\n",
    "                # 根据DAG定义，通常不允许自环，因此设置为0\n",
    "                single_direction_adj.at[source, target] = 0\n",
    "                continue\n",
    "            \n",
    "            # 检查双向编码的邻接矩阵来确定有向边\n",
    "            if bidirectional_adj.at[target, source] == 1:\n",
    "                if bidirectional_adj.at[source, target] == -1:\n",
    "                    # 表示 source → target\n",
    "                    single_direction_adj.at[source, target] = 1\n",
    "                elif bidirectional_adj.at[source, target] == 1:\n",
    "                    # 表示 source ↔ target（互为因果）\n",
    "                    single_direction_adj.at[source, target] = 1\n",
    "                    single_direction_adj.at[target, source] = 1\n",
    "                # 如果 [source, target] == 0 或其他情况，不设置边\n",
    "    return single_direction_adj\n",
    "\n",
    "def convert_dag_with_strength_to_dag(adj: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    将表示因果影响强度的邻接矩阵数据框转换为二元邻接矩阵数据框。\n",
    "    无论存在正向影响还是负向影响，都将其转换为1；不存在影响则为0。\n",
    "    \n",
    "    参数:\n",
    "    - adj (pd.DataFrame): 原始的因果影响强度邻接矩阵，元素可以是正数、负数或0。\n",
    "    \n",
    "    返回:\n",
    "    - pd.DataFrame: 二元邻接矩阵，元素为0或1。\n",
    "    \"\"\"\n",
    "    # 确保输入是一个方阵\n",
    "    if adj.shape[0] != adj.shape[1]:\n",
    "        raise ValueError(\"输入的邻接矩阵必须是方阵（行数等于列数）。\")\n",
    "    \n",
    "    # 确保行列索引一致\n",
    "    if not adj.index.equals(adj.columns):\n",
    "        raise ValueError(\"邻接矩阵的行索引和列索引必须相同。\")\n",
    "    \n",
    "    # 将所有非零元素转换为1，零元素保持为0\n",
    "    return adj.ne(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183dea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"离散化统计指标的工具函数\"\"\"\n",
    "def discretize_sequence(x, ffactor=10):\n",
    "    \"\"\"\n",
    "    将连续序列离散化。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 输入的连续变量，范围在 [-1, 1] 之间。\n",
    "    ffactor (int): 离散化因子，用于缩放和离散化。\n",
    "\n",
    "    返回:\n",
    "    np.ndarray: 离散化后的序列（整数类型）。\n",
    "    \"\"\"\n",
    "    # 确保输入在 [-1, 1] 范围内\n",
    "    x = np.clip(x, -1, 1)\n",
    "    # 缩放并四舍五入\n",
    "    x = np.round(x * ffactor).astype(int)\n",
    "    return x\n",
    "\n",
    "def compute_discrete_probability(x):\n",
    "    \"\"\"\n",
    "    计算离散概率分布。\n",
    "\n",
    "    参数:\n",
    "    x (iterable): 输入的离散化后的序列，可以是单变量或联合变量。\n",
    "\n",
    "    返回:\n",
    "    Counter: 元素及其计数。\n",
    "    \"\"\"\n",
    "    return Counter(x)\n",
    "\n",
    "def discrete_entropy(x, bias_factor=0.7):\n",
    "    \"\"\"\n",
    "    计算离散熵。\n",
    "\n",
    "    参数:\n",
    "    x (iterable): 输入的离散化后的序列，可以是单变量或联合变量。\n",
    "    bias_factor (float): 偏差因子，用于修正有限样本的熵估计。\n",
    "\n",
    "    返回:\n",
    "    float: 计算得到的熵值。\n",
    "    \"\"\"\n",
    "    c = compute_discrete_probability(x)\n",
    "    pk = np.array(list(c.values()), dtype=float)\n",
    "    pk_sum = pk.sum()\n",
    "    if pk_sum == 0:\n",
    "        return 0.0\n",
    "    pk /= pk_sum\n",
    "    # 避免 log(0) 问题，添加一个很小的常数\n",
    "    vec = pk * np.log(pk + 1e-12)\n",
    "    S = -np.sum(vec)\n",
    "    # 添加偏差项\n",
    "    bias = bias_factor * (len(pk) - 1) / (2.0 * len(x))\n",
    "    return S + bias\n",
    "\n",
    "def discrete_joint_entropy(x, y):\n",
    "    \"\"\"\n",
    "    计算两个离散序列的联合熵 H(X, Y)。\n",
    "\n",
    "    参数:\n",
    "    x, y (array-like): 输入的离散化后的序列。\n",
    "\n",
    "    返回:\n",
    "    float: 联合熵 H(X, Y)。\n",
    "    \"\"\"\n",
    "    joint = list(zip(x, y))  # 将 x 和 y 配对\n",
    "    return discrete_entropy(joint)\n",
    "\n",
    "def normalized_error_probability(x, y):\n",
    "    \"\"\"\n",
    "    计算归一化的错误概率。\n",
    "\n",
    "    该函数通过构建联合概率矩阵，计算分类错误的概率，并将其归一化。\n",
    "\n",
    "    参数:\n",
    "    x, y (array-like): 输入的离散化后的序列。\n",
    "\n",
    "    返回:\n",
    "    float: 归一化的错误概率。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    cy = Counter(y)\n",
    "\n",
    "    sorted_cx = sorted(cx.keys())\n",
    "    sorted_cy = sorted(cy.keys())\n",
    "\n",
    "    # 统计联合频数\n",
    "    pxy = defaultdict(int)\n",
    "    for a, b in zip(x, y):\n",
    "        pxy[(a, b)] += 1\n",
    "\n",
    "    total = sum(pxy.values())\n",
    "    if total == 0:\n",
    "        return 0.0  # 或者根据需求返回其他值\n",
    "\n",
    "    # 构建联合概率矩阵\n",
    "    pxy_matrix = np.array([\n",
    "        [pxy.get((a, b), 0) for b in sorted_cy]\n",
    "        for a in sorted_cx\n",
    "    ], dtype=float)\n",
    "\n",
    "    # 归一化为概率\n",
    "    pxy_matrix /= total\n",
    "\n",
    "    # 计算每行的最大概率\n",
    "    max_per_row = pxy_matrix.max(axis=1)\n",
    "    perr = 1 - np.sum(max_per_row)\n",
    "\n",
    "    # 计算每列的概率和的最大值\n",
    "    sum_per_column = pxy_matrix.sum(axis=0)\n",
    "    max_perr = 1 - np.max(sum_per_column)\n",
    "\n",
    "    # 归一化错误概率\n",
    "    pnorm = perr / max_perr if max_perr > 0 else perr\n",
    "    return pnorm\n",
    "\n",
    "def discrete_divergence(cx, cy):\n",
    "    \"\"\"\n",
    "    计算两个离散分布之间的KL散度（Kullback-Leibler Divergence）。\n",
    "\n",
    "    KL散度衡量了分布 cx 相对于分布 cy 的差异，是信息论中的一个重要概念。\n",
    "\n",
    "    参数:\n",
    "    cx (Counter): 第一个离散分布的元素计数。\n",
    "    cy (Counter): 第二个离散分布的元素计数。\n",
    "\n",
    "    返回:\n",
    "    float: KL散度 D_KL(cx || cy)。\n",
    "    \"\"\"\n",
    "    # 创建 cy 的副本，避免修改原始对象\n",
    "    cy = cy.copy()\n",
    "\n",
    "    # 为了避免 cy 中某些元素的概率为零，将它们的计数设为 1\n",
    "    for a in cx:\n",
    "        if cy[a] == 0:\n",
    "            cy[a] = 1\n",
    "\n",
    "    # 计算概率\n",
    "    nx = float(sum(cx.values()))\n",
    "    ny = float(sum(cy.values()))\n",
    "\n",
    "    kl_div = 0.0\n",
    "    for a, v in cx.items():\n",
    "        px = v / nx\n",
    "        py = cy[a] / ny\n",
    "        kl_div += px * np.log(px / py)\n",
    "    return kl_div\n",
    "\n",
    "def discrete_conditional_entropy(x, y):\n",
    "    \"\"\"\n",
    "    计算两个离散序列的条件熵 H(X|Y)。\n",
    "\n",
    "    条件熵衡量了在已知 Y 的情况下，X 的不确定性。\n",
    "\n",
    "    参数:\n",
    "    x, y (array-like): 输入的离散化后的序列。\n",
    "\n",
    "    返回:\n",
    "    float: 条件熵 H(X|Y)。\n",
    "    \"\"\"\n",
    "    joint_entropy = discrete_joint_entropy(x, y)\n",
    "    entropy_y = discrete_entropy(y)\n",
    "    return joint_entropy - entropy_y\n",
    "\n",
    "def adjusted_mutual_information_score(x, y):\n",
    "    \"\"\"\n",
    "    计算两个离散序列的调整互信息（Adjusted Mutual Information, AMI）。\n",
    "\n",
    "    AMI 是互信息的一种调整版本，考虑了随机期望的互信息，通常用于聚类评估。\n",
    "\n",
    "    参数:\n",
    "    x, y (array-like): 输入的离散化后的序列。\n",
    "\n",
    "    返回:\n",
    "    float: 调整后的互信息。\n",
    "    \"\"\"\n",
    "    return adjusted_mutual_info_score(x, y)\n",
    "\n",
    "def discrete_mutual_information(x, y):\n",
    "    \"\"\"\n",
    "    计算两个离散序列的互信息 I(X; Y)。\n",
    "\n",
    "    互信息衡量了两个变量之间共享的信息量，是信息论中的一个基本概念。\n",
    "\n",
    "    参数:\n",
    "    x, y (array-like): 输入的离散化后的序列。\n",
    "\n",
    "    返回:\n",
    "    float: 互信息 I(X; Y)。\n",
    "    \"\"\"\n",
    "    entropy_x = discrete_entropy(x)\n",
    "    entropy_y = discrete_entropy(y)\n",
    "    joint_entropy = discrete_joint_entropy(x, y)\n",
    "    mutual_info = entropy_x + entropy_y - joint_entropy\n",
    "    # 避免由于数值误差导致的负值\n",
    "    mutual_info = max(mutual_info, 0)\n",
    "    return mutual_info\n",
    "\n",
    "# -------------------\n",
    "# Helper Functions\n",
    "# -------------------\n",
    "\n",
    "def normalize_discrete(x):\n",
    "    \"\"\"\n",
    "    对离散化后的序列进行标准化处理。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散化后的x序列。\n",
    "\n",
    "    返回:\n",
    "    np.ndarray: 标准化后的x序列。\n",
    "    \"\"\"\n",
    "    if len(set(x)) < 2:\n",
    "        return np.zeros_like(x, dtype=float)\n",
    "    x_mean = np.mean(x)\n",
    "    x_std = np.std(x)\n",
    "    if x_std > 0:\n",
    "        return (x - x_mean) / x_std\n",
    "    else:\n",
    "        return x - x_mean\n",
    "\n",
    "def to_numerical(x_discrete, y_continuous):\n",
    "    \"\"\"\n",
    "    将类别型的离散x转换为数值型，通过将每个唯一的x值替换为对应y的平均值。\n",
    "\n",
    "    参数:\n",
    "    x_discrete (array-like): 离散化后的x数组。\n",
    "    y_continuous (array-like): 与x对应的连续y数组。\n",
    "\n",
    "    返回:\n",
    "    np.ndarray: 数值型的x数组，每个x值被替换为对应的y平均值。\n",
    "    \"\"\"\n",
    "    dx = defaultdict(lambda: [0.0, 0])\n",
    "    for a, b in zip(x_discrete, y_continuous):\n",
    "        dx[a][0] += b\n",
    "        dx[a][1] += 1\n",
    "    for a in dx:\n",
    "        dx[a][0] /= dx[a][1] if dx[a][1] > 0 else 1e-12\n",
    "    x_numerical = np.array([dx[a][0] for a in x_discrete], dtype=float)\n",
    "    x_numerical = (x_numerical - np.mean(x_numerical)) / np.std(x_numerical) if np.std(x_numerical) > 0 else x_numerical\n",
    "    return x_numerical\n",
    "\n",
    "def count_unique(x):\n",
    "    \"\"\"\n",
    "    计算数组中唯一元素的数量。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 输入数组。\n",
    "\n",
    "    返回:\n",
    "    int: 唯一元素的数量。\n",
    "    \"\"\"\n",
    "    return len(set(x))\n",
    "\n",
    "# -------------------\n",
    "# Feature Engineering Functions\n",
    "# -------------------\n",
    "\n",
    "def normalized_entropy_baseline(x):\n",
    "    \"\"\"\n",
    "    计算给定归一化x的标准化熵基线。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "\n",
    "    返回:\n",
    "    float: 标准化熵基线值。\n",
    "    \"\"\"\n",
    "    if len(set(x)) < 2:\n",
    "        return 0.0\n",
    "    xs = np.sort(x)\n",
    "    delta = xs[1:] - xs[:-1]\n",
    "    delta = delta[delta != 0]\n",
    "    if len(delta) == 0:\n",
    "        return 0.0\n",
    "    hx = np.mean(np.log(delta))\n",
    "    hx += special.psi(len(delta))\n",
    "    hx -= special.psi(1)\n",
    "    return hx\n",
    "\n",
    "def normalized_entropy(x, m=2):\n",
    "    \"\"\"\n",
    "    计算标准化熵。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    m (int): delta计算的参数。\n",
    "\n",
    "    返回:\n",
    "    float: 标准化熵值。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    if len(cx) < 2:\n",
    "        return 0.0\n",
    "    xk = np.array(list(cx.keys()), dtype=float)\n",
    "    xk.sort()\n",
    "    if len(xk) < 2:\n",
    "        return 0.0\n",
    "    delta = (xk[1:] - xk[:-1]) / m\n",
    "    counter = np.array([cx[i] for i in xk], dtype=float)\n",
    "    hx = np.sum(counter[1:] * np.log(delta / counter[1:])) / len(x)\n",
    "    hx += (special.psi(len(delta)) - np.log(len(delta)))\n",
    "    hx += np.log(len(x))\n",
    "    hx -= (special.psi(m) - np.log(m))\n",
    "    return hx\n",
    "\n",
    "def igci(x, y):\n",
    "    \"\"\"\n",
    "    计算IGCI（信息几何因果推断）度量。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "\n",
    "    返回:\n",
    "    float: IGCI度量值。\n",
    "    \"\"\"\n",
    "    # 检查是否有足够的唯一值\n",
    "    if len(set(x)) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # 判断是否有重复的x值\n",
    "    if len(x) != len(set(x)):\n",
    "        dx = defaultdict(lambda: [0.0, 0])\n",
    "        for a, b in zip(x, y):\n",
    "            dx[a][0] += b\n",
    "            dx[a][1] += 1\n",
    "        for a in dx:\n",
    "            dx[a][0] /= dx[a][1] if dx[a][1] > 0 else 1e-12\n",
    "        # 构建联合序列\n",
    "        xy = np.array([[a, dx[a][0]] for a in dx.keys()], dtype=float)\n",
    "        # 获取每个x的计数\n",
    "        counter = np.array([dx[a][1] for a in xy[:, 0]], dtype=float)\n",
    "    else:\n",
    "        # 如果x没有重复，直接排序\n",
    "        xy = np.array(sorted(zip(x, y)), dtype=float)\n",
    "        counter = np.ones(len(x))\n",
    "    \n",
    "    # 计算相邻差值\n",
    "    delta = xy[1:] - xy[:-1]\n",
    "    # 选择y差值不为0的样本\n",
    "    selec = delta[:, 1] != 0\n",
    "    delta = delta[selec]\n",
    "    counter = np.minimum(counter[1:], counter[:-1])[selec]\n",
    "    \n",
    "    if len(delta) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 添加一个极小值epsilon，避免log(0)\n",
    "    epsilon = 1e-12\n",
    "    ratio = (delta[:, 0] + epsilon) / np.abs(delta[:, 1])\n",
    "    ratio = np.where(ratio > 0, ratio, epsilon)\n",
    "    \n",
    "    # 计算 hxy，避免返回 NaN\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        hxy = np.sum(counter * np.log(ratio)) / len(x)\n",
    "    \n",
    "    # 检查 hxy 是否为有效数值\n",
    "    if np.isnan(hxy):\n",
    "        return 0.0\n",
    "    \n",
    "    return hxy\n",
    "\n",
    "def uniform_divergence(x, m=2):\n",
    "    \"\"\"\n",
    "    计算统一散度。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    m (int): delta计算的参数。\n",
    "\n",
    "    返回:\n",
    "    float: 统一散度值。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    xk = np.array(list(cx.keys()), dtype=float)\n",
    "    xk.sort()\n",
    "    delta = np.zeros(len(xk))\n",
    "    if len(xk) > 1:\n",
    "        delta[0] = xk[1] - xk[0]\n",
    "        if len(xk) > m:\n",
    "            delta[1:-1] = (xk[m:] - xk[:-m]) / m\n",
    "        else:\n",
    "            delta[1:-1] = (xk[-1] - xk[0]) / (len(xk) - 1)\n",
    "        delta[-1] = xk[-1] - xk[-2]\n",
    "    else:\n",
    "        delta = np.array([np.sqrt(12)], dtype=float)  # 假设均匀分布在[-1,1]\n",
    "\n",
    "    counter = np.array([cx[i] for i in xk], dtype=float)\n",
    "    delta_sum = np.sum(delta)\n",
    "    if delta_sum > 0:\n",
    "        delta = delta / delta_sum\n",
    "    else:\n",
    "        delta = delta\n",
    "    if len(xk) > 1:\n",
    "        hx = np.sum(counter * np.log(counter / delta)) / len(x)\n",
    "    else:\n",
    "        hx = 0.0\n",
    "    hx -= np.log(len(x))\n",
    "    hx += (special.psi(m) - np.log(m))\n",
    "    return hx\n",
    "\n",
    "def normalized_skewness(x):\n",
    "    \"\"\"\n",
    "    计算x的标准化偏度。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "\n",
    "    返回:\n",
    "    float: 标准化偏度值。\n",
    "    \"\"\"\n",
    "    return stats.skew(x)\n",
    "\n",
    "def normalized_kurtosis(x):\n",
    "    \"\"\"\n",
    "    计算x的标准化峰度。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "\n",
    "    返回:\n",
    "    float: 标准化峰度值。\n",
    "    \"\"\"\n",
    "    return stats.kurtosis(x)\n",
    "\n",
    "def normalized_moment(x, y, n, m):\n",
    "    \"\"\"\n",
    "    计算x和y的标准化联合矩。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "    n (int): x的幂次。\n",
    "    m (int): y的幂次。\n",
    "\n",
    "    返回:\n",
    "    float: 标准化的联合矩值。\n",
    "    \"\"\"\n",
    "    return np.mean((x ** n) * (y ** m))\n",
    "\n",
    "def moment21(x, y):\n",
    "    \"\"\"\n",
    "    计算标准化联合矩 I(X^2 * Y)。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "\n",
    "    返回:\n",
    "    float: 联合矩 I(X^2 * Y)。\n",
    "    \"\"\"\n",
    "    return normalized_moment(x, y, 2, 1)\n",
    "\n",
    "def moment22(x, y):\n",
    "    \"\"\"\n",
    "    计算标准化联合矩 I(X^2 * Y^2)。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "\n",
    "    返回:\n",
    "    float: 联合矩 I(X^2 * Y^2)。\n",
    "    \"\"\"\n",
    "    return normalized_moment(x, y, 2, 2)\n",
    "\n",
    "def moment31(x, y):\n",
    "    \"\"\"\n",
    "    计算标准化联合矩 I(X^3 * Y)。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "\n",
    "    返回:\n",
    "    float: 联合矩 I(X^3 * Y)。\n",
    "    \"\"\"\n",
    "    return normalized_moment(x, y, 3, 1)\n",
    "\n",
    "def fit_pairwise(x, y):\n",
    "    \"\"\"\n",
    "    拟合多项式到x和y，并基于系数计算一个复杂的度量值。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列（数值型）。\n",
    "    y (array-like): 离散且归一化的y序列（数值型）。\n",
    "\n",
    "    返回:\n",
    "    float: 拟合度量值。\n",
    "    \"\"\"\n",
    "    if count_unique(x) <= 2 or count_unique(y) <= 2:\n",
    "        return 0.0\n",
    "    x_std = x if np.std(x) == 1 else (x - np.mean(x)) / np.std(x) if np.std(x) > 0 else x\n",
    "    y_std = y if np.std(y) == 1 else (y - np.mean(y)) / np.std(y) if np.std(y) > 0 else y\n",
    "    try:\n",
    "        xy1 = np.polyfit(x_std, y_std, 1)\n",
    "        xy2 = np.polyfit(x_std, y_std, 2)\n",
    "        return abs(2 * xy2[0]) + abs(xy2[1] - xy1[0])\n",
    "    except np.RankWarning:\n",
    "        return 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def fit_error(x, y, m=2):\n",
    "    \"\"\"\n",
    "    计算x和y之间的拟合误差。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "    m (int): 拟合时使用的多项式的阶数。\n",
    "\n",
    "    返回:\n",
    "    float: 拟合误差。\n",
    "    \"\"\"\n",
    "    if count_unique(x) <= m or count_unique(y) <= m:\n",
    "        poly_degree = min(count_unique(x), count_unique(y)) - 1\n",
    "    else:\n",
    "        poly_degree = m\n",
    "\n",
    "    if poly_degree < 1:\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        poly = np.polyfit(x, y, poly_degree)\n",
    "        y_pred = np.polyval(poly, x)\n",
    "        return np.std(y - y_pred)\n",
    "    except np.RankWarning:\n",
    "        return 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def fit_noise_entropy(x, y, minc=10):\n",
    "    \"\"\"\n",
    "    计算拟合噪声熵。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "    minc (int): 计算熵的最小计数阈值。\n",
    "\n",
    "    返回:\n",
    "    float: 拟合噪声熵。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    entyx = []\n",
    "    for a in cx:\n",
    "        if cx[a] > minc:\n",
    "            y_subset = y[x == a]\n",
    "            entyx.append(discrete_entropy(y_subset))\n",
    "    if len(entyx) == 0:\n",
    "        return 0.0\n",
    "    n = count_unique(y)\n",
    "    return np.std(entyx) / np.log(n) if n > 0 else 0.0\n",
    "\n",
    "def fit_noise_skewness(x, y, minc=8):\n",
    "    \"\"\"\n",
    "    计算拟合噪声偏度的标准差。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "    minc (int): 计算偏度的最小计数阈值。\n",
    "\n",
    "    返回:\n",
    "    float: 拟合噪声偏度的标准差。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    skewyx = []\n",
    "    for a in cx:\n",
    "        if cx[a] >= minc:\n",
    "            y_subset = y[x == a]\n",
    "            skewyx.append(normalized_skewness(y_subset))\n",
    "    if len(skewyx) == 0:\n",
    "        return 0.0\n",
    "    return np.std(skewyx)\n",
    "\n",
    "def fit_noise_kurtosis(x, y, minc=8):\n",
    "    \"\"\"\n",
    "    计算拟合噪声峰度的标准差。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "    minc (int): 计算峰度的最小计数阈值。\n",
    "\n",
    "    返回:\n",
    "    float: 拟合噪声峰度的标准差。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    kurtyx = []\n",
    "    for a in cx:\n",
    "        if cx[a] >= minc:\n",
    "            y_subset = y[x == a]\n",
    "            kurtyx.append(normalized_kurtosis(y_subset))\n",
    "    if len(kurtyx) == 0:\n",
    "        return 0.0\n",
    "    return np.std(kurtyx)\n",
    "\n",
    "def conditional_distribution_similarity(x, y, minc=12):\n",
    "    \"\"\"\n",
    "    计算条件分布相似性。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "    minc (int): 计算条件分布的最小计数阈值。\n",
    "\n",
    "    返回:\n",
    "    float: 条件分布相似性度量。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    cy = Counter(y)\n",
    "    yrange = sorted(cy.keys())\n",
    "    ny = len(yrange)\n",
    "\n",
    "    py = np.array([cy[i] for i in yrange], dtype=float)\n",
    "    py = py / py.sum() if py.sum() > 0 else py\n",
    "\n",
    "    pyx = []\n",
    "    for a in cx:\n",
    "        if cx[a] > minc:\n",
    "            yx = y[x == a]\n",
    "            cyx = Counter(yx)\n",
    "            pyxa = np.array([cyx.get(i, 0.0) for i in yrange], dtype=float)\n",
    "            if pyxa.sum() == 0:\n",
    "                continue\n",
    "            pyxa = pyxa / pyxa.sum()\n",
    "            pyx.append(py * pyxa)  # 修正这里，将 pyx * pyxa 改为 py * pyxa\n",
    "\n",
    "    if len(pyx) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    pyx = np.array(pyx)\n",
    "    pyx = pyx - pyx.mean(axis=0)\n",
    "    return np.std(pyx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d193daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"特征工程\"\"\"\n",
    "###########################################################################\n",
    "################################相关系数特征################################\n",
    "###########################################################################\n",
    "\"\"\"皮尔逊相关系数\"\"\"\n",
    "def pearson_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the correlation-based features for each\n",
    "    varibale, which are the correlation between that variable with X and Y,\n",
    "    as well as summary statistics (max, min, mean, std) of all pairs\n",
    "    of correlations.\n",
    "    \"\"\"\n",
    "\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = dataset.corr().drop([variable], axis=\"columns\").loc[variable].abs()\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"corr(v,X)\": dataset[[variable, \"X\"]].corr().loc[variable, \"X\"],\n",
    "            \"corr(v,Y)\": dataset[[variable, \"Y\"]].corr().loc[variable, \"Y\"],\n",
    "            \"max(corr(v, others))\": tmp.max(),\n",
    "            \"min(corr(v, others))\": tmp.min(),\n",
    "            \"mean(corr(v, others))\": tmp.mean(),\n",
    "            \"std(corr(v, others))\": tmp.std(),\n",
    "            \"25%(corr(v, others))\": tmp.quantile(0.25), \n",
    "            \"75%(corr(v, others))\": tmp.quantile(0.75), \n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr().loc[\"X\", \"Y\"]\n",
    "\n",
    "    # pearsonr is NaN when the variance is 0, so we fill with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"滚动皮尔逊相关系数绝对值\"\"\"\n",
    "def rolling_abs_pearson_correlation(dataset):\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = dataset[[variable, \"X\", \"Y\"]].copy()\n",
    "        tmp = tmp.sort_values(by=variable, ascending=True).reset_index(drop=True)\n",
    "        # 计算窗口内v-\"X\"的皮尔逊相关系数绝对值\n",
    "        abs_rolling_corr_x = tmp[variable].rolling(window=300).corr(tmp['X']).abs()\n",
    "        # 计算窗口内v-\"Y\"的皮尔逊相关系数绝对值\n",
    "        abs_rolling_corr_y = tmp[variable].rolling(window=300).corr(tmp['Y']).abs()\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"max(abs_rolling_corr(v, X))\": abs_rolling_corr_x.max(),\n",
    "            \"min(abs_rolling_corr(v, X))\": abs_rolling_corr_x.min(),\n",
    "            \"mean(abs_rolling_corr(v, X))\": abs_rolling_corr_x.mean(),\n",
    "            # \"std(abs_rolling_corr(v, X))\": abs_rolling_corr_x.std(),  # 掉分\n",
    "            # \"25%(abs_rolling_corr(v, X))\": abs_rolling_corr_x.quantile(0.25),\n",
    "            # \"75%(abs_rolling_corr(v, X))\": abs_rolling_corr_x.quantile(0.75),\n",
    "            \"max(abs_rolling_corr(v, Y))\": abs_rolling_corr_y.max(),\n",
    "            \"min(abs_rolling_corr(v, Y))\": abs_rolling_corr_y.min(),\n",
    "            \"mean(abs_rolling_corr(v, Y))\": abs_rolling_corr_y.mean(),\n",
    "            # \"std(abs_rolling_corr(v, Y))\": abs_rolling_corr_y.std(),\n",
    "            # \"25%(abs_rolling_corr(v, Y))\": abs_rolling_corr_y.quantile(0.25),\n",
    "            # \"75%(abs_rolling_corr(v, Y))\": abs_rolling_corr_y.quantile(0.75)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"斯皮尔曼相关系数\"\"\"\n",
    "def spearman_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the Spearman rank correlation-based features for each\n",
    "    variable, which are the Spearman correlation between that variable with X and Y,\n",
    "    as well as summary statistics (max, min, mean, std) of all pairs of Spearman correlations.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = dataset.corr(method='spearman').drop([variable], axis=\"columns\").loc[variable].abs()\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"spearman_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='spearman').loc[variable, \"X\"],\n",
    "            \"spearman_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='spearman').loc[variable, \"Y\"],\n",
    "            \"max(spearman_corr(v, others))\": tmp.max(),\n",
    "            \"min(spearman_corr(v, others))\": tmp.min(),\n",
    "            \"mean(spearman_corr(v, others))\": tmp.mean(),\n",
    "            \"std(spearman_corr(v, others))\": tmp.std(),\n",
    "            #TODO 分位数\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"spearman_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='spearman').loc[\"X\", \"Y\"]\n",
    "\n",
    "    # Spearman correlation is NaN when there are ties in rank, so we fill with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"肯德尔相关系数\"\"\"\n",
    "def kendall_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the Kendall's tau correlation-based features for each\n",
    "    variable, which are the Kendall's tau correlation between that variable with X and Y,\n",
    "    as well as summary statistics (max, min, mean, std) of all pairs of Kendall's tau correlations.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = dataset.corr(method='kendall').drop([variable], axis=\"columns\").loc[variable].abs()\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"kendall_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='kendall').loc[variable, \"X\"],\n",
    "            \"kendall_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='kendall').loc[variable, \"Y\"],\n",
    "            \"max(kendall_corr(v, others))\": tmp.max(),\n",
    "            \"min(kendall_corr(v, others))\": tmp.min(),\n",
    "            \"mean(kendall_corr(v, others))\": tmp.mean(),\n",
    "            \"std(kendall_corr(v, others))\": tmp.std(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"kendall_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='kendall').loc[\"X\", \"Y\"]\n",
    "\n",
    "    # Kendall's tau correlation can be NaN in some cases, so we fill with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"互信息\"\"\"\n",
    "def mutual_information(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the mutual-information-based features\n",
    "    for each variable, which are the MI between that variable\n",
    "    and X and Y, as well as summary statistics (max, min, mean, std) of\n",
    "    all pairs of MI.\n",
    "    \"\"\"\n",
    "\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = mutual_info_regression(dataset.drop(columns=[variable]), dataset[variable])\n",
    "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"MI(v,X)\": mutual_info_regression(dataset[[variable]], dataset[\"X\"], discrete_features=False)[0],\n",
    "            \"MI(v,Y)\": mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0],\n",
    "            \"max(MI(v, others))\": tmp.max(),\n",
    "            \"min(MI(v, others))\": tmp.min(),\n",
    "            \"mean(MI(v, others))\": tmp.mean(),\n",
    "            \"std(MI(v, others))\": tmp.std(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"MI(X,Y)\"] = mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"条件互信息\"\"\"\n",
    "def conditional_mutual_information(dataset):\n",
    "    \"\"\"\n",
    "    Calculate conditional mutual information for each variable with X and Y.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "    \n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # Calculate conditional MI(v, X | Y)\n",
    "        mi_vx_given_y = mutual_info_regression(dataset[[variable, \"Y\"]], dataset[\"X\"], discrete_features=False)[0] - \\\n",
    "                        mutual_info_regression(dataset[[\"Y\"]], dataset[\"X\"], discrete_features=False)[0]\n",
    "        \n",
    "        # Calculate conditional MI(v, Y | X)\n",
    "        mi_vy_given_x = mutual_info_regression(dataset[[variable, \"X\"]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
    "                        mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
    "        \n",
    "        # Calculate conditional MI(X, Y | v)\n",
    "        mi_xy_given_v = mutual_info_regression(dataset[[\"X\", variable]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
    "                        mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0]\n",
    "        \n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"conditional_MI(v,X|Y)\": mi_vx_given_y,\n",
    "            \"conditional_MI(v,Y|X)\": mi_vy_given_x,\n",
    "            \"conditional_MI(X,Y|v)\": mi_xy_given_v,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\"\"\"距离相关系数\"\"\"\n",
    "def distance_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the distance correlation-based features for each\n",
    "    variable, which are the distance correlation between that variable with X and Y,\n",
    "    as well as summary statistics (max, min, mean, std) of all pairs of distance correlations.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = []\n",
    "        # Compute distance correlation between 'variable' and all other variables (excluding itself)\n",
    "        other_variables = dataset.columns.drop([variable])\n",
    "        for other_var in other_variables:\n",
    "            corr = dcor.distance_correlation(dataset[variable], dataset[other_var])\n",
    "            tmp.append(corr)\n",
    "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
    "\n",
    "        distance_correlation_v_X = dcor.distance_correlation(dataset[variable], dataset[\"X\"])\n",
    "        distance_correlation_v_Y = dcor.distance_correlation(dataset[variable], dataset[\"Y\"])\n",
    "        distance_correlation_X_Y = dcor.distance_correlation(dataset[\"X\"], dataset[\"Y\"])\n",
    "        distance_correlation_v_X_square = distance_correlation_v_X ** 2\n",
    "        distance_correlation_v_Y_square = distance_correlation_v_Y ** 2\n",
    "        distance_correlation_X_Y_square = distance_correlation_X_Y ** 2\n",
    "        \n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"dcor(v,X)\": distance_correlation_v_X,\n",
    "            \"dcor(v,Y)\": distance_correlation_v_Y,\n",
    "            \"dcor(v,X)^2\": distance_correlation_v_X_square,\n",
    "            \"dcor(v,Y)^2\": distance_correlation_v_Y_square,\n",
    "            \"max(dcor(v, others))\": tmp.max(),\n",
    "            \"min(dcor(v, others))\": tmp.min(),\n",
    "            \"mean(dcor(v, others))\": tmp.mean(),\n",
    "            \"std(dcor(v, others))\": tmp.std(),\n",
    "            \"25%(dcor(v, others))\": tmp.quantile(0.25),\n",
    "            \"75%(dcor(v, others))\": tmp.quantile(0.75),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"dcor(X,Y)\"] = distance_correlation_X_Y\n",
    "    df[\"dcor(X,Y)^2\"] = distance_correlation_X_Y_square\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"距离相关系数-尝试不同的欧几里得距离指数\"\"\"\n",
    "def distance_correlation_different_exponent(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the distance correlation-based features for each\n",
    "    variable, which are the distance correlation between that variable with X and Y,\n",
    "    with different exponent, as well as summary statistics (max, min, mean, std) of all pairs of distance correlations.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = []\n",
    "        # Compute distance correlation between 'variable' and all other variables (excluding itself)\n",
    "        other_variables = dataset.columns.drop([variable])\n",
    "        for other_var in other_variables:\n",
    "            corr = dcor.distance_correlation(dataset[variable], dataset[other_var])\n",
    "            tmp.append(corr)\n",
    "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
    "\n",
    "        distance_correlation_v_X = dcor.distance_correlation(dataset[variable], dataset[\"X\"], exponent=0.5)\n",
    "        distance_correlation_v_Y = dcor.distance_correlation(dataset[variable], dataset[\"Y\"], exponent=0.5)\n",
    "        distance_correlation_X_Y = dcor.distance_correlation(dataset[\"X\"], dataset[\"Y\"], exponent=0.5)\n",
    "        distance_correlation_v_X_square = distance_correlation_v_X ** 2\n",
    "        distance_correlation_v_Y_square = distance_correlation_v_Y ** 2\n",
    "        distance_correlation_X_Y_square = distance_correlation_X_Y ** 2\n",
    "        \n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"dcor_0.5exp(v,X)\": distance_correlation_v_X,\n",
    "            \"dcor_0.5exp(v,Y)\": distance_correlation_v_Y,\n",
    "            \"dcor_0.5exp(v,X)^2\": distance_correlation_v_X_square,\n",
    "            \"dcor_0.5exp(v,Y)^2\": distance_correlation_v_Y_square,\n",
    "            \"max(dcor_0.5exp(v, others))\": tmp.max(),\n",
    "            \"min(dcor_0.5exp(v, others))\": tmp.min(),\n",
    "            \"mean(dcor_0.5exp(v, others))\": tmp.mean(),\n",
    "            \"std(dcor_0.5exp(v, others))\": tmp.std(),\n",
    "            \"25%(dcor_0.5exp(v, others))\": tmp.quantile(0.25),\n",
    "            \"75%(dcor_0.5exp(v, others))\": tmp.quantile(0.75),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"dcor_0.5exp(X,Y)\"] = distance_correlation_X_Y\n",
    "    df[\"dcor_0.5exp(X,Y)^2\"] = distance_correlation_X_Y_square\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"能量距离\"\"\"\n",
    "def energy_distance_features(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the energy distance-based features for each\n",
    "    variable, which are the energy distance between that variable with X and Y,\n",
    "    as well as summary statistics (max, min, mean, std) of all pairs of energy distances.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # tmp = []\n",
    "        # # Compute energy distance between 'variable' and all other variables (excluding itself)\n",
    "        # other_variables = dataset.columns.drop([variable])\n",
    "        # for other_var in other_variables:\n",
    "        #     energy_dist = dcor.energy_distance(dataset[variable], dataset[other_var])\n",
    "        #     tmp.append(energy_dist)\n",
    "        # tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
    "\n",
    "        energy_distance_v_X = dcor.energy_distance(dataset[variable], dataset[\"X\"])\n",
    "        energy_distance_v_Y = dcor.energy_distance(dataset[variable], dataset[\"Y\"])\n",
    "        energy_distance_X_Y = dcor.energy_distance(dataset[\"X\"], dataset[\"Y\"])\n",
    "        \n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"energy_dist(v,X)\": energy_distance_v_X,\n",
    "            \"energy_dist(v,Y)\": energy_distance_v_Y,\n",
    "            # \"max(energy_dist(v, others))\": tmp.max(),\n",
    "            # \"min(energy_dist(v, others))\": tmp.min(),\n",
    "            # \"mean(energy_dist(v, others))\": tmp.mean(),\n",
    "            # \"std(energy_dist(v, others))\": tmp.std(),\n",
    "            # \"25%(energy_dist(v, others))\": tmp.quantile(0.25),\n",
    "            # \"75%(energy_dist(v, others))\": tmp.quantile(0.75),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"energy_dist(X,Y)\"] = energy_distance_X_Y\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"偏相关系数\"\"\"\n",
    "def partial_correlation(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \"\"\"\n",
    "    Compute partial correlation coefficients for each variable with X and Y,\n",
    "    controlling for the other variable, as well as the partial correlation\n",
    "    between X and Y controlling for each variable.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "    \n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # Compute partial correlations\n",
    "        pcorr_vX_Y = pg.partial_corr(data=dataset, x=variable, y='X', covar='Y')['r'].iloc[0]\n",
    "        pcorr_vY_X = pg.partial_corr(data=dataset, x=variable, y='Y', covar='X')['r'].iloc[0]\n",
    "        pcorr_XY_v = pg.partial_corr(data=dataset, x='X', y='Y', covar=variable)['r'].iloc[0]\n",
    "        \n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"partial_corr(v,X|Y)\": pcorr_vX_Y,\n",
    "            \"partial_corr(v,Y|X)\": pcorr_vY_X,\n",
    "            \"partial_corr(X,Y|v)\": pcorr_XY_v,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\"\"\"Copula Entropy\"\"\"\n",
    "def copula_entropy(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the Copula entropy-based features for each\n",
    "    variable, which are the Copula entropy between that variable with X and Y,\n",
    "    as well as summary statistics of all pairs of Copula entropies.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # Compute copula entropy between v and X\n",
    "        ce_v_X = copent(dataset[[variable, \"X\"]].values)\n",
    "        \n",
    "        # Compute copula entropy between v and Y\n",
    "        ce_v_Y = copent(dataset[[variable, \"Y\"]].values)\n",
    "        \n",
    "        # # Compute transfer entropy from v to X and X to v\n",
    "        # te_v_X = transent(dataset[variable].values, dataset[\"X\"].values)\n",
    "        # te_X_v = transent(dataset[\"X\"].values, dataset[variable].values)\n",
    "        \n",
    "        # # Compute transfer entropy from v to Y and Y to v\n",
    "        # te_v_Y = transent(dataset[variable].values, dataset[\"Y\"].values)\n",
    "        # te_Y_v = transent(dataset[\"Y\"].values, dataset[variable].values)\n",
    "        \n",
    "        \n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"copula_entropy(v,X)\": ce_v_X,\n",
    "            \"copula_entropy(v,Y)\": ce_v_Y,\n",
    "            # \"transfer_entropy(v->X)\": te_v_X,\n",
    "            # \"transfer_entropy(X->v)\": te_X_v,\n",
    "            # \"transfer_entropy(v->Y)\": te_v_Y,\n",
    "            # \"transfer_entropy(Y->v)\": te_Y_v,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # Compute copula entropy between X and Y\n",
    "    df[\"copula_entropy(X,Y)\"] = copent(dataset[[\"X\", \"Y\"]].values)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"Predictive Power Score\"\"\"\n",
    "def PPS_feature(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "\n",
    "    matrix_df = pps_matrix(dataset)\n",
    "    pivot_df = pd.pivot_table(matrix_df, index='x', columns='y', values='ppscore')\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"PPS(v,X)\": pivot_df.loc[variable, 'X'],\n",
    "            \"PPS(X,v)\": pivot_df.loc['X', variable],\n",
    "            \"PPS(v,Y)\": pivot_df.loc[variable, 'Y'],\n",
    "            \"PPS(Y,v)\": pivot_df.loc['Y', variable],\n",
    "            \"PPS(X,Y)\": pivot_df.loc['X', 'Y'],\n",
    "            \"max(PPS(v,others))\": pivot_df.loc[variable, variables].max(),\n",
    "            \"mean(PPS(v,others))\": pivot_df.loc[variable, variables].mean(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "###########################################################################\n",
    "################################其他类型特征################################\n",
    "###########################################################################\n",
    "\"\"\"维度信息\"\"\"\n",
    "def add_dimension_feature(dataset):\n",
    "    \"\"\"\n",
    "    Add a dimension feature to the dataset.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "    dimension = len(variables)\n",
    "    square_dimension = dimension * dimension\n",
    "    df = pd.DataFrame({\n",
    "        \"variable\": variables,\n",
    "        \"dimension\": dimension,\n",
    "        \"square_dimension\": square_dimension\n",
    "    })\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\"\"\"网格化信息\"\"\"\n",
    "def grid_describe(df, var_col, dir_col, grid=5, xlim=[-1.0, 1.0], ylim=[-1.0, 1.0]):\n",
    "    \"\"\"\n",
    "    将指定的空间根据 xlim 和 ylim 划分为 grid x grid 的小窗格，并统计每个窗格内的数据点数量。\n",
    "    确保所有划分出的区域都被记录，即使某些区域内没有数据点，计数为0。\n",
    "\n",
    "    参数:\n",
    "    - df: pandas.DataFrame 包含 var_col 和 dir_col 两列\n",
    "    - var_col: 用于 X 轴的列名（例如 'X'）\n",
    "    - dir_col: 用于 Y 轴的列名（例如 'Y'）\n",
    "    - grid: 网格的数量，默认为 5\n",
    "    - xlim: X 轴的范围，默认为 [-1.0, 1.0]\n",
    "    - ylim: Y 轴的范围，默认为 [-1.0, 1.0]\n",
    "\n",
    "    返回:\n",
    "    - 一个字典，键为 (x_bin, y_bin) 的元组，值为对应窗格内的数据点数量\n",
    "    \"\"\"\n",
    "    # 定义网格边界\n",
    "    x_bins = np.linspace(xlim[0], xlim[1], grid + 1)\n",
    "    y_bins = np.linspace(ylim[0], ylim[1], grid + 1)\n",
    "\n",
    "    # 使用 numpy.histogram2d 计算2D直方图\n",
    "    counts, _, _ = np.histogram2d(df[var_col], df[dir_col], bins=[x_bins, y_bins])\n",
    "\n",
    "    # 将计数结果转换为字典，键为 (x_bin, y_bin)，值为计数\n",
    "    grid_counts = {}\n",
    "    for x in range(grid):\n",
    "        for y in range(grid):\n",
    "            grid_counts[(x, y)] = int(counts[x, y])  # 转换为整数\n",
    "\n",
    "    return grid_counts\n",
    "\n",
    "def grid_feature(dataset):\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "\n",
    "    grid = 5\n",
    "    xlim = [-1.0, 1.0]\n",
    "    ylim = [-1.0, 1.0]\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        temp = dataset[[variable, \"X\"]].sort_values(by=variable, ascending=True)\n",
    "        grid_features_v_to_X = grid_describe(temp, variable, 'X', grid, xlim, ylim)\n",
    "        temp = dataset[[variable, \"Y\"]].sort_values(by=variable, ascending=True)\n",
    "        grid_features_v_to_Y = grid_describe(temp, variable, 'Y', grid, xlim, ylim)\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"grid(v,X)(0,0)\": grid_features_v_to_X[(0, 0)],\n",
    "            \"grid(v,X)(0,1)\": grid_features_v_to_X[(0, 1)],\n",
    "            \"grid(v,X)(0,2)\": grid_features_v_to_X[(0, 2)],\n",
    "            \"grid(v,X)(0,3)\": grid_features_v_to_X[(0, 3)],\n",
    "            \"grid(v,X)(0,4)\": grid_features_v_to_X[(0, 4)],\n",
    "            \"grid(v,X)(1,0)\": grid_features_v_to_X[(1, 0)],\n",
    "            \"grid(v,X)(1,1)\": grid_features_v_to_X[(1, 1)],\n",
    "            \"grid(v,X)(1,2)\": grid_features_v_to_X[(1, 2)],\n",
    "            \"grid(v,X)(1,3)\": grid_features_v_to_X[(1, 3)],\n",
    "            \"grid(v,X)(1,4)\": grid_features_v_to_X[(1, 4)],\n",
    "            \"grid(v,X)(2,0)\": grid_features_v_to_X[(2, 0)],\n",
    "            \"grid(v,X)(2,1)\": grid_features_v_to_X[(2, 1)],\n",
    "            \"grid(v,X)(2,2)\": grid_features_v_to_X[(2, 2)],\n",
    "            \"grid(v,X)(2,3)\": grid_features_v_to_X[(2, 3)],\n",
    "            \"grid(v,X)(2,4)\": grid_features_v_to_X[(2, 4)],\n",
    "            \"grid(v,X)(3,0)\": grid_features_v_to_X[(3, 0)],\n",
    "            \"grid(v,X)(3,1)\": grid_features_v_to_X[(3, 1)],\n",
    "            \"grid(v,X)(3,2)\": grid_features_v_to_X[(3, 2)],\n",
    "            \"grid(v,X)(3,3)\": grid_features_v_to_X[(3, 3)],\n",
    "            \"grid(v,X)(3,4)\": grid_features_v_to_X[(3, 4)],\n",
    "            \"grid(v,X)(4,0)\": grid_features_v_to_X[(4, 0)],\n",
    "            \"grid(v,X)(4,1)\": grid_features_v_to_X[(4, 1)],\n",
    "            \"grid(v,X)(4,2)\": grid_features_v_to_X[(4, 2)],\n",
    "            \"grid(v,X)(4,3)\": grid_features_v_to_X[(4, 3)],\n",
    "            \"grid(v,X)(4,4)\": grid_features_v_to_X[(4, 4)],\n",
    "            \"grid(v,Y)(0,0)\": grid_features_v_to_Y[(0, 0)],\n",
    "            \"grid(v,Y)(0,1)\": grid_features_v_to_Y[(0, 1)],\n",
    "            \"grid(v,Y)(0,2)\": grid_features_v_to_Y[(0, 2)],\n",
    "            \"grid(v,Y)(0,3)\": grid_features_v_to_Y[(0, 3)],\n",
    "            \"grid(v,Y)(0,4)\": grid_features_v_to_Y[(0, 4)],\n",
    "            \"grid(v,Y)(1,0)\": grid_features_v_to_Y[(1, 0)],\n",
    "            \"grid(v,Y)(1,1)\": grid_features_v_to_Y[(1, 1)],\n",
    "            \"grid(v,Y)(1,2)\": grid_features_v_to_Y[(1, 2)],\n",
    "            \"grid(v,Y)(1,3)\": grid_features_v_to_Y[(1, 3)],\n",
    "            \"grid(v,Y)(1,4)\": grid_features_v_to_Y[(1, 4)],\n",
    "            \"grid(v,Y)(2,0)\": grid_features_v_to_Y[(2, 0)],\n",
    "            \"grid(v,Y)(2,1)\": grid_features_v_to_Y[(2, 1)],\n",
    "            \"grid(v,Y)(2,2)\": grid_features_v_to_Y[(2, 2)],\n",
    "            \"grid(v,Y)(2,3)\": grid_features_v_to_Y[(2, 3)],\n",
    "            \"grid(v,Y)(2,4)\": grid_features_v_to_Y[(2, 4)],\n",
    "            \"grid(v,Y)(3,0)\": grid_features_v_to_Y[(3, 0)],\n",
    "            \"grid(v,Y)(3,1)\": grid_features_v_to_Y[(3, 1)],\n",
    "            \"grid(v,Y)(3,2)\": grid_features_v_to_Y[(3, 2)],\n",
    "            \"grid(v,Y)(3,3)\": grid_features_v_to_Y[(3, 3)],\n",
    "            \"grid(v,Y)(3,4)\": grid_features_v_to_Y[(3, 4)],\n",
    "            \"grid(v,Y)(4,0)\": grid_features_v_to_Y[(4, 0)],\n",
    "            \"grid(v,Y)(4,1)\": grid_features_v_to_Y[(4, 1)],\n",
    "            \"grid(v,Y)(4,2)\": grid_features_v_to_Y[(4, 2)],\n",
    "            \"grid(v,Y)(4,3)\": grid_features_v_to_Y[(4, 3)],\n",
    "            \"grid(v,Y)(4,4)\": grid_features_v_to_Y[(4, 4)],\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"离散化统计指标\"\"\"\n",
    "def discrete_statistic_features(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "\n",
    "    ffactor = 10\n",
    "    x = dataset[\"X\"].values\n",
    "    y = dataset[\"Y\"].values\n",
    "    x_discrete = discretize_sequence(x, ffactor=ffactor)\n",
    "    y_discrete = discretize_sequence(y, ffactor=ffactor)\n",
    "    x_normalized = normalize_discrete(x_discrete)\n",
    "    y_normalized = normalize_discrete(y_discrete)\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        v = dataset[variable].values\n",
    "        v_discrete = discretize_sequence(v, ffactor=ffactor)\n",
    "        v_normalized = normalize_discrete(v_discrete)   \n",
    "        \n",
    "        # 计算联合熵 H(v,x) 和 H(v,y)\n",
    "        H_vx = discrete_joint_entropy(v_discrete, x_discrete)\n",
    "        H_vy = discrete_joint_entropy(v_discrete, y_discrete)\n",
    "        # 计算条件熵 H(v|x) 和 H(v|y)\n",
    "        H_v_given_x = discrete_conditional_entropy(v_discrete, x_discrete)\n",
    "        H_v_given_y = discrete_conditional_entropy(v_discrete, y_discrete)\n",
    "        # 计算互信息 I(v,x) 和 I(v,y)\n",
    "        I_vx = discrete_mutual_information(v_discrete, x_discrete)\n",
    "        I_vy = discrete_mutual_information(v_discrete, y_discrete)\n",
    "        # 计算调整互信息 AMI(v,x) 和 AMI(v,y)\n",
    "        AMI_vx = adjusted_mutual_information_score(v_discrete, x_discrete)\n",
    "        AMI_vy = adjusted_mutual_information_score(v_discrete, y_discrete)\n",
    "        # 计算归一化的错误概率 NPE(v,x) 和 NPE(v,y)\n",
    "        error_prob_vx = normalized_error_probability(v_discrete, x_discrete)\n",
    "        error_prob_vy = normalized_error_probability(v_discrete, y_discrete)\n",
    "\n",
    "        # 计算归一化熵基线 H_baseline 和归一化熵 H_entropy 和均匀散度 uniform_div\n",
    "        # H_baseline = normalized_entropy_baseline(v_normalized)\n",
    "        # H_entropy = normalized_entropy(v_normalized)\n",
    "        # uniform_div = uniform_divergence(v_normalized)\n",
    "        # 计算IGCI (v,x) 和 IGCI(v,y) \n",
    "        igci_vx = igci(v_normalized, x_normalized)\n",
    "        igci_vy = igci(v_normalized, y_normalized)\n",
    "        # 计算IGCI (x,v) 和 IGCI(y,v) \n",
    "        igci_xv = igci(x_normalized, v_normalized)\n",
    "        igci_yv = igci(y_normalized, v_normalized)\n",
    "        # 计算矩特征 MM(v^2 * x) 和 MM(v^2 * y) \n",
    "        moment_21_vx = moment21(v_normalized, x_normalized)\n",
    "        moment_21_vy = moment21(v_normalized, y_normalized)\n",
    "        # 计算矩特征 MM(v^2 * x^2) 和 MM(v^2 * y^2) \n",
    "        moment_22_vx = moment22(v_normalized, x_normalized)\n",
    "        moment_22_vy = moment22(v_normalized, y_normalized)\n",
    "        # 计算矩特征 MM(v^3 * x) 和 MM(v^3 * y) \n",
    "        moment_31_vx = moment31(v_normalized, x_normalized)\n",
    "        moment_31_vy = moment31(v_normalized, y_normalized)\n",
    "        # 计算拟合度量值 fit 和拟合误差 fit_error 和拟合噪声熵 fit_noise_entropy 和拟合噪声偏度标准差 fit_noise_skewness 和拟合噪声峰度标准差 fit_noise_kurtosis\n",
    "        fit_val_vx = fit_pairwise(v_normalized, x_normalized)\n",
    "        fit_err_vx = fit_error(v_normalized, x_normalized, m=2)\n",
    "        fit_noise_ent_vx = fit_noise_entropy(v_normalized, x_normalized, minc=8)\n",
    "        fit_noise_skew_vx = fit_noise_skewness(v_normalized, x_normalized, minc=8)\n",
    "        fit_noise_kurt_vx = fit_noise_kurtosis(v_normalized, x_normalized, minc=8)\n",
    "        fit_val_vy = fit_pairwise(v_normalized, y_normalized)\n",
    "        fit_err_vy = fit_error(v_normalized, y_normalized, m=2)\n",
    "        fit_noise_ent_vy = fit_noise_entropy(v_normalized, y_normalized, minc=8)\n",
    "        fit_noise_skew_vy = fit_noise_skewness(v_normalized, y_normalized, minc=8)\n",
    "        fit_noise_kurt_vy = fit_noise_kurtosis(v_normalized, y_normalized, minc=8)\n",
    "        # 计算条件分布相似度 cond_dist_sim\n",
    "        cond_dist_sim_vx = conditional_distribution_similarity(v_normalized, x_normalized, minc=8)\n",
    "        cond_dist_sim_vy = conditional_distribution_similarity(v_normalized, y_normalized, minc=8)\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"JH(v,x)\": H_vx,\n",
    "            \"JH(v,y)\": H_vy,\n",
    "            \"CH(v|x)\": H_v_given_x,\n",
    "            \"CH(v|y)\": H_v_given_y,\n",
    "            \"I(v,x)\": I_vx,\n",
    "            \"I(v,y)\": I_vy,\n",
    "            \"AMI(v,x)\": AMI_vx,\n",
    "            \"AMI(v,y)\": AMI_vy,\n",
    "            \"NPE(v,x)\": error_prob_vx,\n",
    "            \"NPE(v,y)\": error_prob_vy,\n",
    "            # \"H_baseline\": H_baseline,\n",
    "            # \"H_entropy\": H_entropy,\n",
    "            # \"uniform_div\": uniform_div,\n",
    "            \"IGCI(v,x)\": igci_vx,\n",
    "            \"IGCI(v,y)\": igci_vy,\n",
    "            \"IGCI(x,v)\": igci_xv,\n",
    "            \"IGCI(y,v)\": igci_yv,\n",
    "            \"MM(v^2 * x)\": moment_21_vx,\n",
    "            \"MM(v^2 * y)\": moment_21_vy,\n",
    "            \"MM(v^2 * x^2)\": moment_22_vx,\n",
    "            \"MM(v^2 * y^2)\": moment_22_vy,\n",
    "            \"MM(v^3 * x)\": moment_31_vx,\n",
    "            \"MM(v^3 * y)\": moment_31_vy,\n",
    "            \"fit(v,x)\": fit_val_vx,\n",
    "            \"fit(v,y)\": fit_val_vy,\n",
    "            \"fit_error(v,x)\": fit_err_vx,\n",
    "            \"fit_error(v,y)\": fit_err_vy,\n",
    "            \"fit_noise_entropy(v,x)\": fit_noise_ent_vx,\n",
    "            \"fit_noise_entropy(v,y)\": fit_noise_ent_vy,\n",
    "            \"fit_noise_skewness(v,x)\": fit_noise_skew_vx,\n",
    "            \"fit_noise_skewness(v,y)\": fit_noise_skew_vy,\n",
    "            \"fit_noise_kurtosis(v,x)\": fit_noise_kurt_vx,\n",
    "            \"fit_noise_kurtosis(v,y)\": fit_noise_kurt_vy,\n",
    "            \"cond_dist_sim(v,x)\": cond_dist_sim_vx,\n",
    "            \"cond_dist_sim(v,y)\": cond_dist_sim_vy,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "###########################################################################\n",
    "################################回归系数特征################################\n",
    "###########################################################################\n",
    "\"\"\"线性回归\"\"\"\n",
    "def linear_regression_feature(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # model1: Fit X, v, v^2, v_i*v_j, v_i*X, cos(v), sin(v) ~ Y\n",
    "    model1_features = [\"X\"] + variables\n",
    "    d1 = Squared_term(dataset[model1_features], model1_features)\n",
    "    d1 = Interaction_term(d1, model1_features)\n",
    "    d1 = Cos_Sin_term(d1, variables)\n",
    "    model1_features = d1.columns.tolist()\n",
    "    d1_scaled = scaler.fit_transform(d1)\n",
    "    model1 = LinearRegression().fit(d1_scaled, dataset[[\"Y\"]])\n",
    "    model1_coefs = model1.coef_[0].tolist()\n",
    "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
    "    \n",
    "    # model2: Fit v, v^2, v_i*v_j, cos(v), sin(v) ~ X\n",
    "    model2_features = variables\n",
    "    d2 = Squared_term(dataset[model2_features], model2_features)\n",
    "    d2 = Interaction_term(d2, model2_features)\n",
    "    d2 = Cos_Sin_term(d2, variables)\n",
    "    model2_features = d2.columns.tolist()\n",
    "    d2_scaled = scaler.fit_transform(d2)\n",
    "    model2 = LinearRegression().fit(d2_scaled, dataset[[\"X\"]])\n",
    "    model2_coefs = model2.coef_[0].tolist()\n",
    "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
    "    \n",
    "    df = []\n",
    "    for i, variable in enumerate(variables):\n",
    "        # model3: Fit other v, X, Y ~ v\n",
    "        model3_features = [\"X\", \"Y\"] + dataset.columns.drop([\"X\", \"Y\", variable]).tolist()\n",
    "        d3 = Squared_term(dataset[model3_features], model3_features)\n",
    "        d3 = Interaction_term(d3, model3_features)\n",
    "        d3 = Cos_Sin_term(d3, model3_features)\n",
    "        model3_features = d3.columns.tolist()\n",
    "        d3_scaled = scaler.fit_transform(d3)\n",
    "        model3 = LinearRegression().fit(d3_scaled, dataset[[variable]])\n",
    "        model3_coefs = model3.coef_[0].tolist()\n",
    "        model3_dict = {name: coef for name, coef in zip(model3_features, model3_coefs)}\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"v~Y_coefficient\": model1_dict[variable],     # <--- model1\n",
    "            \"v_squared~Y_coefficient\": model1_dict[f\"{variable}_squared_term\"],\n",
    "            \"v*X~Y_coefficient\": model1_dict[f\"X_{variable}\"],\n",
    "            \"v_cos~Y_coefficient\": model1_dict[f\"{variable}_cos_term\"],\n",
    "            \"v_sin~Y_coefficient\": model1_dict[f\"{variable}_sin_term\"],\n",
    "            \"v~X_coefficient\": model2_dict[variable],     # <--- model2\n",
    "            \"v_squared~X_coefficient\": model2_dict[f\"{variable}_squared_term\"],\n",
    "            \"v_cos~X_coefficient\": model2_dict[f\"{variable}_cos_term\"], \n",
    "            \"v_sin~X_coefficient\": model2_dict[f\"{variable}_sin_term\"],  \n",
    "            \"X~v_coefficient\": model3_dict[\"X\"],          # <--- model3\n",
    "            \"X_squared~v_coefficient\": model3_dict[\"X_squared_term\"],\n",
    "            \"X_cos~v_coefficient\": model3_dict[\"X_cos_term\"],\n",
    "            \"X_sin~v_coefficient\": model3_dict[\"X_sin_term\"],\n",
    "            \"Y~v_coefficient\": model3_dict[\"Y\"],\n",
    "            \"Y_squared~v_coefficient\": model3_dict[\"Y_squared_term\"],\n",
    "            \"Y_cos~v_coefficient\": model3_dict[\"Y_cos_term\"],\n",
    "            \"Y_sin~v_coefficient\": model3_dict[\"Y_sin_term\"],\n",
    "            \"X*Y~v_coefficient\": model3_dict[\"X_Y\"]\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    df[\"X~Y_coefficient\"] = model1_dict[\"X\"]\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"分段线性回归\"\"\"\n",
    "def piecewise_linear_regression_estimate(dataset, X_col, y_col):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    X = dataset[X_col].values.reshape(-1, 1)\n",
    "    y = dataset[y_col].values\n",
    "\n",
    "    break_point = np.median(X)\n",
    "    X_piecewise = np.column_stack((X, np.maximum(X - break_point, 0)))\n",
    "    model = LinearRegression().fit(X_piecewise, y)\n",
    "    return model.coef_\n",
    "\n",
    "def piecewise_linear_regression_feature(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute piecewise linear regression features for each\n",
    "    variable with X and Y, using a single breakpoint at the median.\n",
    "    We also include interaction terms between v and X for predicting Y.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # For v ~ X\n",
    "        v2X_coef = piecewise_linear_regression_estimate(dataset, variable, \"X\")\n",
    "        # For v ~ Y\n",
    "        v2Y_coef = piecewise_linear_regression_estimate(dataset, variable, \"Y\")\n",
    "        # For X ~ v\n",
    "        X2v_coef = piecewise_linear_regression_estimate(dataset, \"X\", variable)\n",
    "        # For Y ~ v\n",
    "        Y2v_coef = piecewise_linear_regression_estimate(dataset, \"Y\", variable)\n",
    "        \n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"v~X_piecewise_coef1\": v2X_coef[0],\n",
    "            \"v~X_piecewise_coef2\": v2X_coef[1],\n",
    "            \"v~Y_piecewise_coef1\": v2Y_coef[0],\n",
    "            \"v~Y_piecewise_coef2\": v2Y_coef[1],\n",
    "            \"X~v_piecewise_coef1\": X2v_coef[0],\n",
    "            \"X~v_piecewise_coef2\": X2v_coef[1],\n",
    "            \"Y~v_piecewise_coef1\": Y2v_coef[0],\n",
    "            \"Y~v_piecewise_coef2\": Y2v_coef[1],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def piecewise_quadratic_regression_feature(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \"\"\"\n",
    "    Compute piecewise quadratic regression features for each variable with X and Y,\n",
    "    using a single breakpoint at the median.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # Prepare data\n",
    "        v = dataset[variable].values.reshape(-1, 1)\n",
    "        X = dataset[\"X\"].values.reshape(-1, 1)\n",
    "        Y = dataset[\"Y\"].values.reshape(-1, 1)\n",
    "\n",
    "        # Create piecewise quadratic features\n",
    "        v_breakpoint = np.median(v)\n",
    "        X_breakpoint = np.median(X)\n",
    "        Y_breakpoint = np.median(Y)\n",
    "\n",
    "        v_piecewise = np.column_stack((v, v**2, np.maximum(v - v_breakpoint, 0), np.maximum(v - v_breakpoint, 0)**2))\n",
    "        X_piecewise = np.column_stack((X, X**2, np.maximum(X - X_breakpoint, 0), np.maximum(X - X_breakpoint, 0)**2))\n",
    "        Y_piecewise = np.column_stack((Y, Y**2, np.maximum(Y - Y_breakpoint, 0), np.maximum(Y - Y_breakpoint, 0)**2))\n",
    "\n",
    "        # Fit models\n",
    "        model_v_X = LinearRegression().fit(v_piecewise, X)\n",
    "        model_v_Y = LinearRegression().fit(v_piecewise, Y)\n",
    "        model_X_v = LinearRegression().fit(X_piecewise, v)\n",
    "        model_Y_v = LinearRegression().fit(Y_piecewise, v)\n",
    "\n",
    "        # Store coefficients\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"v~X_piecewise_coef1\": model_v_X.coef_[0][0],\n",
    "            \"v~X_piecewise_coef2\": model_v_X.coef_[0][1],\n",
    "            \"v~X_piecewise_coef3\": model_v_X.coef_[0][2],\n",
    "            \"v~X_piecewise_coef4\": model_v_X.coef_[0][3],\n",
    "            \"v~Y_piecewise_coef1\": model_v_Y.coef_[0][0],\n",
    "            \"v~Y_piecewise_coef2\": model_v_Y.coef_[0][1],\n",
    "            \"v~Y_piecewise_coef3\": model_v_Y.coef_[0][2],\n",
    "            \"v~Y_piecewise_coef4\": model_v_Y.coef_[0][3],\n",
    "            \"X~v_piecewise_coef1\": model_X_v.coef_[0][0],\n",
    "            \"X~v_piecewise_coef2\": model_X_v.coef_[0][1],\n",
    "            \"X~v_piecewise_coef3\": model_X_v.coef_[0][2],\n",
    "            \"X~v_piecewise_coef4\": model_X_v.coef_[0][3],\n",
    "            \"Y~v_piecewise_coef1\": model_Y_v.coef_[0][0],\n",
    "            \"Y~v_piecewise_coef2\": model_Y_v.coef_[0][1],\n",
    "            \"Y~v_piecewise_coef3\": model_Y_v.coef_[0][2],\n",
    "            \"Y~v_piecewise_coef4\": model_Y_v.coef_[0][3],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # Reorder columns\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def piecewise_quadratic_regression_feature_4_improved(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \"\"\"\n",
    "    Compute piecewise quadratic regression features for each variable with X and Y,\n",
    "    using three breakpoints to create four segments, and only using quadratic terms.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # Prepare data\n",
    "        v = dataset[variable].values.reshape(-1, 1)\n",
    "        X = dataset[\"X\"].values.reshape(-1, 1)\n",
    "        Y = dataset[\"Y\"].values.reshape(-1, 1)\n",
    "\n",
    "        # Create piecewise quadratic features with three breakpoints\n",
    "        v_breakpoints = np.percentile(v, [25, 50, 75])\n",
    "        X_breakpoints = np.percentile(X, [25, 50, 75])\n",
    "        Y_breakpoints = np.percentile(Y, [25, 50, 75])\n",
    "\n",
    "        v_piecewise = np.column_stack((\n",
    "            v**2,\n",
    "            np.maximum(v - v_breakpoints[0], 0)**2,\n",
    "            np.maximum(v - v_breakpoints[1], 0)**2,\n",
    "            np.maximum(v - v_breakpoints[2], 0)**2\n",
    "        ))\n",
    "        X_piecewise = np.column_stack((\n",
    "            X**2,\n",
    "            np.maximum(X - X_breakpoints[0], 0)**2,\n",
    "            np.maximum(X - X_breakpoints[1], 0)**2,\n",
    "            np.maximum(X - X_breakpoints[2], 0)**2\n",
    "        ))\n",
    "        Y_piecewise = np.column_stack((\n",
    "            Y**2,\n",
    "            np.maximum(Y - Y_breakpoints[0], 0)**2,\n",
    "            np.maximum(Y - Y_breakpoints[1], 0)**2,\n",
    "            np.maximum(Y - Y_breakpoints[2], 0)**2\n",
    "        ))\n",
    "\n",
    "        # Fit models\n",
    "        model_v_X = LinearRegression(fit_intercept=False).fit(v_piecewise, X)\n",
    "        model_v_Y = LinearRegression(fit_intercept=False).fit(v_piecewise, Y)\n",
    "        model_X_v = LinearRegression(fit_intercept=False).fit(X_piecewise, v)\n",
    "        model_Y_v = LinearRegression(fit_intercept=False).fit(Y_piecewise, v)\n",
    "\n",
    "        # Store coefficients\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"v~X_quadratic_coef1\": model_v_X.coef_[0][0],\n",
    "            \"v~X_quadratic_coef2\": model_v_X.coef_[0][1],\n",
    "            \"v~X_quadratic_coef3\": model_v_X.coef_[0][2],\n",
    "            \"v~X_quadratic_coef4\": model_v_X.coef_[0][3],\n",
    "            \"v~Y_quadratic_coef1\": model_v_Y.coef_[0][0],\n",
    "            \"v~Y_quadratic_coef2\": model_v_Y.coef_[0][1],\n",
    "            \"v~Y_quadratic_coef3\": model_v_Y.coef_[0][2],\n",
    "            \"v~Y_quadratic_coef4\": model_v_Y.coef_[0][3],\n",
    "            \"X~v_quadratic_coef1\": model_X_v.coef_[0][0],\n",
    "            \"X~v_quadratic_coef2\": model_X_v.coef_[0][1],\n",
    "            \"X~v_quadratic_coef3\": model_X_v.coef_[0][2],\n",
    "            \"X~v_quadratic_coef4\": model_X_v.coef_[0][3],\n",
    "            \"Y~v_quadratic_coef1\": model_Y_v.coef_[0][0],\n",
    "            \"Y~v_quadratic_coef2\": model_Y_v.coef_[0][1],\n",
    "            \"Y~v_quadratic_coef3\": model_Y_v.coef_[0][2],\n",
    "            \"Y~v_quadratic_coef4\": model_Y_v.coef_[0][3],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # Reorder columns\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def sliding_window_linear_regression_estimate(dataset, X_col, y_col, window_size=None, step_size=None):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    if window_size is None:\n",
    "        window_size = len(dataset) // 3\n",
    "    if step_size is None:\n",
    "        step_size = len(dataset) // 6\n",
    "\n",
    "    X = dataset[X_col].values\n",
    "    y = dataset[y_col].values\n",
    "\n",
    "    coefficients = []\n",
    "    for start in range(0, len(dataset) - window_size + 1, step_size):\n",
    "        end = start + window_size\n",
    "        X_window = X[start:end].reshape(-1, 1)\n",
    "        y_window = y[start:end]\n",
    "        \n",
    "        model = LinearRegression().fit(X_window, y_window)\n",
    "        coefficients.append(model.coef_[0])\n",
    "\n",
    "    return coefficients\n",
    "\n",
    "def sliding_window_linear_regression_feature(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute sliding window linear regression features for each\n",
    "    variable with X and Y, using a window size of 1/3 of the data and step size of 1/6.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # For v ~ X\n",
    "        v2X_coef = sliding_window_linear_regression_estimate(dataset, variable, \"X\")\n",
    "        # For v ~ Y\n",
    "        v2Y_coef = sliding_window_linear_regression_estimate(dataset, variable, \"Y\")\n",
    "        # For X ~ v\n",
    "        X2v_coef = sliding_window_linear_regression_estimate(dataset, \"X\", variable)\n",
    "        # For Y ~ v\n",
    "        Y2v_coef = sliding_window_linear_regression_estimate(dataset, \"Y\", variable)\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"v~X_sliding_coef1\": v2X_coef[0],\n",
    "            \"v~X_sliding_coef2\": v2X_coef[1],\n",
    "            \"v~X_sliding_coef3\": v2X_coef[2],\n",
    "            \"v~X_sliding_coef4\": v2X_coef[3],\n",
    "            \"v~X_sliding_coef5\": v2X_coef[4],\n",
    "            \"v~Y_sliding_coef1\": v2Y_coef[0],\n",
    "            \"v~Y_sliding_coef2\": v2Y_coef[1],\n",
    "            \"v~Y_sliding_coef3\": v2Y_coef[2],\n",
    "            \"v~Y_sliding_coef4\": v2Y_coef[3],\n",
    "            \"v~Y_sliding_coef5\": v2Y_coef[4],\n",
    "            \"X~v_sliding_coef1\": X2v_coef[0],\n",
    "            \"X~v_sliding_coef2\": X2v_coef[1],\n",
    "            \"X~v_sliding_coef3\": X2v_coef[2],\n",
    "            \"X~v_sliding_coef4\": X2v_coef[3],\n",
    "            \"X~v_sliding_coef5\": X2v_coef[4],\n",
    "            \"Y~v_sliding_coef1\": Y2v_coef[0],\n",
    "            \"Y~v_sliding_coef2\": Y2v_coef[1],\n",
    "            \"Y~v_sliding_coef3\": Y2v_coef[2],\n",
    "            \"Y~v_sliding_coef4\": Y2v_coef[3],\n",
    "            \"Y~v_sliding_coef5\": Y2v_coef[4],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"岭回归\"\"\"\n",
    "def ridge_regression_feature(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "    # 使用GridSearchCV来选择最佳的alpha值\n",
    "    param_grid = {'alpha': np.logspace(-6, 6, 13)}\n",
    "    \n",
    "    # model1: Fit X, v ~ Y\n",
    "    model1_features = [\"X\"] + variables\n",
    "    d1 = Squared_term(dataset[model1_features], model1_features)\n",
    "    # d1 = Interaction_term(d1, model1_features) # 掉分\n",
    "    model1_features = d1.columns.tolist()\n",
    "    scaler1 = StandardScaler()\n",
    "    d1_scaled = scaler1.fit_transform(d1)\n",
    "    model1 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
    "    model1.fit(d1_scaled, dataset[\"Y\"])\n",
    "    model1_coefs = model1.best_estimator_.coef_.tolist()\n",
    "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
    "    \n",
    "    # model2: Fit v ~ X\n",
    "    model2_features = variables\n",
    "    d2 = Squared_term(dataset[model2_features], model2_features)\n",
    "    d2 = Interaction_term(d2, model2_features)\n",
    "    # d2 = Cos_Sin_term(d2, model2_features)  # 掉分\n",
    "    model2_features = d2.columns.tolist()\n",
    "    scaler2 = StandardScaler()\n",
    "    d2_scaled = scaler2.fit_transform(d2)\n",
    "    model2 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
    "    model2.fit(d2_scaled, dataset[\"X\"])\n",
    "    model2_coefs = model2.best_estimator_.coef_.tolist()\n",
    "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
    "\n",
    "    # # 获取最优的 alpha 值\n",
    "    # best_alpha_model1 = model1.best_params_['alpha']   # 0.4730-0.4727\n",
    "    # best_alpha_model2 = model2.best_params_['alpha']\n",
    "    \n",
    "    df = []\n",
    "    for i, variable in enumerate(variables):\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"v~Y_ridge_coefficient\": model1_dict[variable],\n",
    "            # \"v_squared~Y_ridge_coefficient\": model1_dict[f\"{variable}_squared_term\"],  # 掉分\n",
    "            \"v~X_ridge_coefficient\": model2_dict[variable],\n",
    "            # \"v_squared~X_ridge_coefficient\": model2_dict[f\"{variable}_squared_term\"],  # 掉分\n",
    "            # \"v_cos~X_ridge_coefficient\": model2_dict[f\"{variable}_cos_term\"],  # 掉分\n",
    "            # \"v_sin~X_ridge_coefficient\": model2_dict[f\"{variable}_sin_term\"],  # 掉分\n",
    "            # \"v~Y_ridge_alpha\": best_alpha_model1,\n",
    "            # \"v~X_ridge_alpha\": best_alpha_model2\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    df[\"X~Y_ridge_coefficient\"] = model1_dict[\"X\"]\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "###########################################################################\n",
    "################################因果发现特征################################\n",
    "###########################################################################\n",
    "\"\"\"因果发现算法\"\"\"\n",
    "def ExactSearch_estimate(dataset, search_method='astar', use_path_extension=True, \n",
    "                        use_k_cycle_heuristic=False, k=3, max_parents=None):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \"\"\"\n",
    "    使用Exact Search算法进行因果发现。\n",
    "    \n",
    "    参数:\n",
    "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
    "    - search_method (str): Exact Search方法，'astar'或'dp'。默认值为'astar'。\n",
    "    - use_path_extension (bool): 是否使用路径扩展。默认值为True。\n",
    "    - use_k_cycle_heuristic (bool): 是否使用k-cycle冲突启发式。仅适用于'astar'方法。默认值为False。\n",
    "    - k (int): k-cycle启发式的参数。默认值为3。\n",
    "    - max_parents (int or None): 节点的最大父节点数量。默认值为None。\n",
    "    \n",
    "    返回:\n",
    "    - adj_df (pd.DataFrame): 因果图的邻接矩阵，格式为DataFrame。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 将数据框转换为numpy.ndarray\n",
    "    data = dataset.values\n",
    "\n",
    "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
    "    data = handle_multicollinearity(data)\n",
    "\n",
    "    # 2. 定义X→Y的超级图\n",
    "    d = dataset.shape[1]   \n",
    "    super_graph = np.ones((d, d), dtype=int)  # 初始化为全1，表示所有边默认允许\n",
    "    columns = dataset.columns.tolist()\n",
    "    x_idx = columns.index('X')\n",
    "    y_idx = columns.index('Y')\n",
    "    super_graph[y_idx][x_idx] = 0     # 禁止'Y'→'X'\n",
    "    np.fill_diagonal(super_graph, 0)  # 禁止自环：确保对角线为0\n",
    "    \n",
    "    # 3. 运行Exact Search算法，使用指定的参数\n",
    "    dag_est, search_stats = bic_exact_search(X=data, super_graph=super_graph, search_method=search_method,\n",
    "        use_path_extension=use_path_extension, use_k_cycle_heuristic=use_k_cycle_heuristic,\n",
    "        k=k, verbose=False, max_parents=max_parents)\n",
    "    \n",
    "    # 4. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
    "    adj_df = pd.DataFrame(dag_est, index=dataset.columns, columns=dataset.columns)\n",
    "    \n",
    "    return adj_df\n",
    "\n",
    "def ExactSearch_feature(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "\n",
    "    estimate_adj_df = ExactSearch_estimate(dataset)\n",
    "    estimate_adj_df_dag = estimate_adj_df.astype(int)  # 转换为整型\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # 检查变量与'X'和'Y'之间的边\n",
    "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
    "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
    "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
    "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
    "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
    "\n",
    "        # # 检查是否存在中介路径\n",
    "        # v_to_others_to_X = int(any(\n",
    "        #     estimate_adj_df_dag.loc[variable, other] and estimate_adj_df_dag.loc[other, 'X']\n",
    "        #     for other in variables if other != variable\n",
    "        # ))\n",
    "        # X_to_others_to_v = int(any(\n",
    "        #     estimate_adj_df_dag.loc['X', other] and estimate_adj_df_dag.loc[other, variable]\n",
    "        #     for other in variables if other != variable\n",
    "        # ))\n",
    "        # v_to_others_to_Y = int(any(\n",
    "        #     estimate_adj_df_dag.loc[variable, other] and estimate_adj_df_dag.loc[other, 'Y']\n",
    "        #     for other in variables if other != variable\n",
    "        # ))\n",
    "        # Y_to_others_to_v = int(any(\n",
    "        #     estimate_adj_df_dag.loc['Y', other] and estimate_adj_df_dag.loc[other, variable]\n",
    "        #     for other in variables if other != variable\n",
    "        # ))\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"ExactSearch(v,X)\": v_to_X,\n",
    "            \"ExactSearch(X,v)\": X_to_v,\n",
    "            \"ExactSearch(v,Y)\": v_to_Y,\n",
    "            \"ExactSearch(Y,v)\": Y_to_v,\n",
    "            \"ExactSearch(X,Y)\": X_to_Y,\n",
    "            # \"ExactSearch(v,others,X)\": v_to_others_to_X,\n",
    "            # \"ExactSearch(X,others,v)\": X_to_others_to_v,\n",
    "            # \"ExactSearch(v,others,Y)\": v_to_others_to_Y,\n",
    "            # \"ExactSearch(Y,others,v)\": Y_to_others_to_v\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def PC_estimate(dataset, alpha=0.05, indep_test='fisherz', kernel=None, stable=True, \n",
    "               uc_rule=0, uc_priority=2, verbose=False, show_progress=False):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \"\"\"\n",
    "    使用PC算法进行因果发现，并应用先验知识。\n",
    "\n",
    "    参数:\n",
    "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
    "    - alpha (float): 显著性水平，默认值为0.05。\n",
    "    - indep_test (str): 独立性检验方法，默认值为'fisherz'。\n",
    "    - kernel (str): 核函数类型，默认值为'linear'。\n",
    "    - stable (bool): 是否运行稳定的骨架发现，默认值为True。\n",
    "    - uc_rule (int): 未屏蔽碰撞点的定向规则，默认值为0。\n",
    "    - uc_priority (int): 解决未屏蔽碰撞点冲突的优先规则，默认值为2。\n",
    "    - verbose (bool): 是否打印详细输出，默认值为False。\n",
    "    - show_progress (bool): 是否显示算法进度，默认值为False。\n",
    "\n",
    "    返回:\n",
    "    - adj_df (pd.DataFrame): 因果图的邻接矩阵，格式为pivot_table。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 将数据框转换为numpy.ndarray\n",
    "    data = dataset.values\n",
    "\n",
    "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
    "    data = handle_multicollinearity(data)\n",
    "    \n",
    "    # 2. 定义先验知识：'X' → 'Y'\n",
    "    # 创建GraphNode对象\n",
    "    try:\n",
    "        node_X = GraphNode('X')\n",
    "        node_Y = GraphNode('Y')\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"确保数据框中包含名为'X'和'Y'的列。\") from e\n",
    "    \n",
    "    # 初始化BackgroundKnowledge对象并添加先验知识\n",
    "    bk = BackgroundKnowledge().add_required_by_node(node_X, node_Y)\n",
    "\n",
    "    # 3. 配置核参数\n",
    "    if indep_test == 'kci':\n",
    "        if kernel is None:\n",
    "            kernel = 'linear'\n",
    "        if kernel == 'linear':\n",
    "            kernel_kwargs = {\n",
    "                'kernelX': 'Linear', \n",
    "                'kernelY': 'Linear', \n",
    "                'kernelZ': 'Linear', \n",
    "                'approx': True,           # 使用伽玛近似\n",
    "                'nullss': 1000,          # 原假设下模拟的样本量\n",
    "            }\n",
    "        elif kernel == 'polynomial':\n",
    "            kernel_kwargs = {\n",
    "                'kernelX': 'Polynomial', \n",
    "                'kernelY': 'Polynomial', \n",
    "                'kernelZ': 'Polynomial', \n",
    "                'polyd': 3,               # 多项式次数设置为3\n",
    "                'approx': True,           # 使用伽玛近似\n",
    "                'nullss': 1000,          # 原假设下模拟的样本量\n",
    "            }\n",
    "        elif kernel == 'gaussian':\n",
    "            kernel_kwargs = {\n",
    "                'kernelX': 'Gaussian', \n",
    "                'kernelY': 'Gaussian', \n",
    "                'kernelZ': 'Gaussian', \n",
    "                'est_width': 'empirical', # 使用经验宽度\n",
    "                'approx': True,           # 使用伽玛近似\n",
    "                'nullss': 1000,          # 原假设下模拟的样本量\n",
    "            }\n",
    "        elif kernel == 'mix':\n",
    "            kernel_kwargs = {\n",
    "                'kernelX': 'Polynomial', \n",
    "                'kernelY': 'Polynomial', \n",
    "                'kernelZ': 'Gaussian',     # Z使用高斯核\n",
    "                'polyd': 3,                # 多项式次数设置为3\n",
    "                'est_width': 'median',     # Z的高斯核带宽使用中位数技巧\n",
    "                'approx': True,            # 使用伽玛近似\n",
    "                'nullss': 1000,           # 原假设下模拟的样本量\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f'Unknown kernel: {kernel}')\n",
    "    else:\n",
    "        kernel_kwargs = {}\n",
    "\n",
    "    # 4. 运行PC算法，传入先验知识\n",
    "    cg = pc(data, alpha=alpha, indep_test=indep_test, stable=stable, uc_rule=uc_rule, uc_priority=uc_priority, \n",
    "            background_knowledge=bk, verbose=verbose, show_progress=show_progress, **kernel_kwargs)\n",
    "    \n",
    "    # 5. 提取邻接矩阵\n",
    "    adj_matrix = cg.G.graph\n",
    "    # 6. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
    "    adj_df = pd.DataFrame(adj_matrix, index=dataset.columns, columns=dataset.columns)\n",
    "    \n",
    "    return adj_df\n",
    "\n",
    "def PC_feature(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "\n",
    "    estimate_adj_df_bidirectional = PC_estimate(dataset)  # 双向的估计因果图\n",
    "    estimate_adj_df_dag = convert_bidirectional_to_dag(estimate_adj_df_bidirectional)  # 将双向图转换为有向图\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # 检查变量与'X'和'Y'之间的边\n",
    "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
    "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
    "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
    "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
    "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"PC(v,X)\": v_to_X,\n",
    "            \"PC(X,v)\": X_to_v,\n",
    "            \"PC(v,Y)\": v_to_Y,\n",
    "            \"PC(Y,v)\": Y_to_v,\n",
    "            \"PC(X,Y)\": X_to_Y\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def FCI_estimate(dataset, alpha=0.05, indep_test='fisherz', kernel=None, \n",
    "               depth=-1, max_path_length=-1, verbose=False, show_progress=False):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \"\"\"\n",
    "    使用FCI算法进行因果发现，并应用先验知识。\n",
    "\n",
    "    参数:\n",
    "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
    "    - alpha (float): 显著性水平，默认值为0.05。\n",
    "    - indep_test (str): 独立性检验方法，默认值为'fisherz'。\n",
    "    - kernel (str): 核函数类型，默认值为'linear'。\n",
    "    - verbose (bool): 是否打印详细输出，默认值为False。\n",
    "    - show_progress (bool): 是否显示算法进度，默认值为False。\n",
    "\n",
    "    返回:\n",
    "    - adj_df (pd.DataFrame): 因果图的邻接矩阵，格式为pivot_table。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 将数据框转换为numpy.ndarray\n",
    "    data = dataset.values\n",
    "\n",
    "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
    "    data = handle_multicollinearity(data)\n",
    "    \n",
    "    # 2. 定义先验知识：'X' → 'Y'\n",
    "    # 创建GraphNode对象\n",
    "    try:\n",
    "        node_X = GraphNode('X')\n",
    "        node_Y = GraphNode('Y')\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"确保数据框中包含名为'X'和'Y'的列。\") from e\n",
    "    \n",
    "    # 初始化BackgroundKnowledge对象并添加先验知识\n",
    "    bk = BackgroundKnowledge().add_required_by_node(node_X, node_Y)\n",
    "\n",
    "    # 3. 配置核参数\n",
    "    if indep_test == 'kci':\n",
    "        if kernel is None:\n",
    "            kernel = 'linear'\n",
    "        if kernel == 'linear':\n",
    "            kernel_kwargs = {\n",
    "                'kernelX': 'Linear', \n",
    "                'kernelY': 'Linear', \n",
    "                'kernelZ': 'Linear', \n",
    "                'approx': True,           # 使用伽玛近似\n",
    "                'nullss': 1000,          # 原假设下模拟的样本量\n",
    "            }\n",
    "        elif kernel == 'polynomial':\n",
    "            kernel_kwargs = {\n",
    "                'kernelX': 'Polynomial', \n",
    "                'kernelY': 'Polynomial', \n",
    "                'kernelZ': 'Polynomial', \n",
    "                'polyd': 3,               # 多项式次数设置为3\n",
    "                'approx': True,           # 使用伽玛近似\n",
    "                'nullss': 1000,          # 原假设下模拟的样本量\n",
    "            }\n",
    "        elif kernel == 'gaussian':\n",
    "            kernel_kwargs = {\n",
    "                'kernelX': 'Gaussian', \n",
    "                'kernelY': 'Gaussian', \n",
    "                'kernelZ': 'Gaussian', \n",
    "                'est_width': 'empirical', # 使用经验宽度\n",
    "                'approx': True,           # 使用伽玛近似\n",
    "                'nullss': 1000,          # 原假设下模拟的样本量\n",
    "            }\n",
    "        elif kernel == 'mix':\n",
    "            kernel_kwargs = {\n",
    "                'kernelX': 'Polynomial', \n",
    "                'kernelY': 'Polynomial', \n",
    "                'kernelZ': 'Gaussian',     # Z使用高斯核\n",
    "                'polyd': 3,                # 多项式次数设置为3\n",
    "                'est_width': 'median',     # Z的高斯核带宽使用中位数技巧\n",
    "                'approx': True,            # 使用伽玛近似\n",
    "                'nullss': 1000,           # 原假设下模拟的样本量\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f'Unknown kernel: {kernel}')\n",
    "    else:\n",
    "        kernel_kwargs = {}\n",
    "\n",
    "    # 4. 运行FCI算法，传入先验知识\n",
    "    try:\n",
    "        g, edges = fci(data, \n",
    "                alpha=alpha, \n",
    "                independence_test_method=indep_test, \n",
    "                depth=depth,\n",
    "                max_path_length=max_path_length,\n",
    "                background_knowledge=bk, \n",
    "                verbose=verbose, \n",
    "                show_progress=show_progress,\n",
    "                **kernel_kwargs\n",
    "        )\n",
    "\n",
    "        # 5. 提取邻接矩阵\n",
    "        adj_matrix = g.graph\n",
    "    except Exception as e:\n",
    "        adj_matrix = np.zeros((data.shape[1], data.shape[1]))\n",
    "    \n",
    "    # 6. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
    "    adj_df = pd.DataFrame(adj_matrix, index=dataset.columns, columns=dataset.columns)\n",
    "    \n",
    "    return adj_df\n",
    "\n",
    "def FCI_feature(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "\n",
    "    estimate_adj_df_bidirectional = FCI_estimate(dataset)  # PAG\n",
    "    estimate_adj_df_dag = estimate_adj_df_bidirectional.astype('int')\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # 检查变量与'X'和'Y'之间的边\n",
    "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
    "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
    "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
    "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
    "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"FCI(v,X)\": v_to_X,\n",
    "            \"FCI(X,v)\": X_to_v,\n",
    "            \"FCI(v,Y)\": v_to_Y,\n",
    "            \"FCI(Y,v)\": Y_to_v,\n",
    "            \"FCI(X,Y)\": X_to_Y\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # # One-hot encode the edge types (-1, 0, 1, 2)\n",
    "    # one_hot_columns = [\"FCI(v,X)\", \"FCI(X,v)\", \"FCI(v,Y)\", \"FCI(Y,v)\"]\n",
    "    \n",
    "    # for col in one_hot_columns:\n",
    "    #     one_hot = pd.get_dummies(df[col], prefix=col)\n",
    "    #     df = pd.concat([df, one_hot], axis=1)\n",
    "\n",
    "    # # Remove original edge type columns after one-hot encoding\n",
    "    # df = df.drop(columns=one_hot_columns)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\", \"variable\"] + [col for col in df.columns if col not in [\"dataset\", \"variable\"]]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def GRaSP_estimate(dataset, score_func='local_score_BIC', maxP=None):\n",
    "    warnings.filterwarnings(\"ignore\")  \n",
    "    # 1. 将数据框转换为numpy.ndarray\n",
    "    data = dataset.values\n",
    "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
    "    data = handle_multicollinearity(data)\n",
    "    \n",
    "    parameters = {\n",
    "        'kfold': 2,         # 2 折交叉验证\n",
    "        'lambda': 0.01      # 正则化参数\n",
    "    }\n",
    "\n",
    "    # 2. 运行PC算法，传入先验知识\n",
    "    G = grasp(\n",
    "        data, \n",
    "        score_func=score_func, \n",
    "        maxP=maxP, \n",
    "        parameters=parameters\n",
    "    )\n",
    "\n",
    "    # 3. 获取邻接矩阵\n",
    "    adj_matrix = G.graph\n",
    "    \n",
    "    # 4. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
    "    adj_df = pd.DataFrame(adj_matrix, index=dataset.columns, columns=dataset.columns)\n",
    "    \n",
    "    return adj_df\n",
    "\n",
    "def GRaSP_feature(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "\n",
    "    estimate_adj_df = GRaSP_estimate(dataset)\n",
    "    estimate_adj_df_dag = estimate_adj_df.astype(int)  # 将邻接矩阵转换为整数类型\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # 检查变量与'X'和'Y'之间的边\n",
    "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
    "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
    "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
    "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
    "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"GRaSP(v,X)\": v_to_X,\n",
    "            \"GRaSP(X,v)\": X_to_v,\n",
    "            \"GRaSP(v,Y)\": v_to_Y,\n",
    "            \"GRaSP(Y,v)\": Y_to_v,\n",
    "            \"GRaSP(X,Y)\": X_to_Y,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"DML\"\"\"\n",
    "def DML_estimate(T_, Y_, X_, data):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # 设置处理变量、结果变量和控制变量\n",
    "    T = data[T_].values\n",
    "    Y = data[Y_].values\n",
    "    X = data[X_].values\n",
    "\n",
    "    # 定义 LassoCV 作为第一阶段模型\n",
    "    model_t = LassoCV(random_state=42, n_jobs=None)\n",
    "    model_y = LassoCV(random_state=42, n_jobs=None)\n",
    "\n",
    "    # # 定义 \"forest\" 作为第二阶段模型\n",
    "    # model_t = \"forest\"\n",
    "    # model_y = \"forest\"\n",
    "\n",
    "    # 初始化 CausalForestDML 使用自定义的估计器\n",
    "    model = CausalForestDML(model_t=model_t, model_y=model_y,\n",
    "        cv=4, n_estimators=36, n_jobs=None, random_state=42, inference=True)\n",
    "    \n",
    "    # # 初始化 LinearDML 使用自定义的估计器\n",
    "    # model = LinearDML(model_t=model_t, model_y=model_y, \n",
    "    #     cv=4, n_jobs=None, random_state=42, inference=True)\n",
    "    \n",
    "    model.fit(Y, T, X=X)\n",
    "\n",
    "    # 平均边际处理效应的推断结果\n",
    "    amte_inference = model.ate_inference(X=X)\n",
    "\n",
    "    return amte_inference\n",
    "\n",
    "def DML_feature(dataset):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # 判断v-X的因果效应，设置variables中的其他v和Y为控制变量\n",
    "        amte_inference1 = DML_estimate(variable, \"X\", [\"Y\"] + list(variables.drop(variable)), dataset)\n",
    "        # 判断v-Y的因果效应，设置variables中的其他v和X为控制变量\n",
    "        amte_inference2 = DML_estimate(variable, \"Y\", [\"X\"] + list(variables.drop(variable)), dataset)\n",
    "        # 判断X-v的因果效应，设置variables中的其他v和Y为控制变量\n",
    "        amte_inference3 = DML_estimate(\"X\", variable, [\"Y\"] + list(variables.drop(variable)), dataset)\n",
    "        # 判断Y-v的因果效应，设置variables中的其他v和X为控制变量\n",
    "        amte_inference4 = DML_estimate(\"Y\", variable, [\"X\"] + list(variables.drop(variable)), dataset)\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"v~X_DML_AMTE\": amte_inference1.mean_point,  \n",
    "            \"v~X_DML_AMTE_zstat\": amte_inference1.zstat(),\n",
    "            \"v~X_DML_AMTE_pvalue\": amte_inference1.pvalue(),\n",
    "            \"v~X_DML_std_point\": amte_inference1.std_point,\n",
    "            \"v~X_DML_stderr_point\": amte_inference1.stderr_point,\n",
    "\n",
    "            \"v~Y_DML_AMTE\": amte_inference2.mean_point,\n",
    "            \"v~Y_DML_AMTE_zstat\": amte_inference2.zstat(),\n",
    "            \"v~Y_DML_AMTE_pvalue\": amte_inference2.pvalue(),\n",
    "            \"v~Y_DML_std_point\": amte_inference2.std_point,\n",
    "            \"v~Y_DML_stderr_point\": amte_inference2.stderr_point,\n",
    "\n",
    "            \"X~v_DML_AMTE\": amte_inference3.mean_point,\n",
    "            \"X~v_DML_AMTE_zstat\": amte_inference3.zstat(),\n",
    "            \"X~v_DML_AMTE_pvalue\": amte_inference3.pvalue(),\n",
    "            \"X~v_DML_std_point\": amte_inference3.std_point,\n",
    "            \"X~v_DML_stderr_point\": amte_inference3.stderr_point,\n",
    "\n",
    "            \"Y~v_DML_AMTE\": amte_inference4.mean_point,\n",
    "            \"Y~v_DML_AMTE_zstat\": amte_inference4.zstat(),\n",
    "            \"Y~v_DML_AMTE_pvalue\": amte_inference4.pvalue(),\n",
    "            \"Y~v_DML_std_point\": amte_inference4.std_point,\n",
    "            \"Y~v_DML_stderr_point\": amte_inference4.stderr_point,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "###########################################################################\n",
    "##################################结构方程特征##############################\n",
    "###########################################################################\n",
    "def sem_features(dataset):\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Sample covariance matrix is not PD\")\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \"\"\"\n",
    "    针对每个变量 v，构建八种 SEM 模型，计算模型拟合指标，生成特征。\n",
    "\n",
    "    参数：\n",
    "    - dataset: 包含 X, Y, v1, v2, ... 的 pandas DataFrame\n",
    "\n",
    "    返回：\n",
    "    - 包含 SEM 特征的 pandas DataFrame\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop(['X', 'Y'])\n",
    "    df = []\n",
    "\n",
    "    # 定义八种关系的 SEM 模型描述\n",
    "    model_templates = {\n",
    "        \"Confounder\": \"\"\"\n",
    "            X ~ a1*v\n",
    "            Y ~ b1*X + b2*v\n",
    "            X ~~ X\n",
    "            v ~~ v\n",
    "            Y ~~ Y\n",
    "        \"\"\",\n",
    "        \"Collider\": \"\"\"\n",
    "            v ~ a1*X + a2*Y\n",
    "            Y ~ b1*X\n",
    "            X ~~ X\n",
    "            v ~~ v\n",
    "            Y ~~ Y\n",
    "        \"\"\",\n",
    "        \"Mediator\": \"\"\"\n",
    "            v ~ a1*X\n",
    "            Y ~ b1*v\n",
    "            X ~~ X\n",
    "            v ~~ v\n",
    "            Y ~~ Y\n",
    "        \"\"\",\n",
    "        \"Cause of X\": \"\"\"\n",
    "            X ~ a1*v\n",
    "            Y ~ b1*X\n",
    "            X ~~ X\n",
    "            v ~~ v\n",
    "            Y ~~ Y\n",
    "        \"\"\",\n",
    "        \"Cause of Y\": \"\"\"\n",
    "            Y ~ a1*X + a2*v\n",
    "            X ~~ X\n",
    "            v ~~ v\n",
    "            Y ~~ Y\n",
    "        \"\"\",\n",
    "        \"Consequence of X\": \"\"\"\n",
    "            v ~ a1*X\n",
    "            Y ~ b1*X\n",
    "            X ~~ X\n",
    "            v ~~ v\n",
    "            Y ~~ Y\n",
    "        \"\"\",\n",
    "        \"Consequence of Y\": \"\"\"\n",
    "            v ~ a1*Y\n",
    "            Y ~ b1*X\n",
    "            X ~~ X\n",
    "            v ~~ v\n",
    "            Y ~~ Y\n",
    "        \"\"\",\n",
    "        # \"Independent\": \"\"\"\n",
    "        #     Y ~ a1*X\n",
    "        #     X ~~ X\n",
    "        #     v ~~ v\n",
    "        #     Y ~~ Y\n",
    "        # \"\"\"\n",
    "    }\n",
    "\n",
    "    # 对于每个变量 v，构建并拟合八种模型\n",
    "    for variable in variables:\n",
    "        # 存储每种模型的拟合指标\n",
    "        fit_indices_list = []\n",
    "        for label, model_desc_template in model_templates.items():\n",
    "            # 替换模型描述中的变量名\n",
    "            model_desc = model_desc_template.replace('v', variable)\n",
    "            try:\n",
    "                # 创建并拟合模型\n",
    "                model = Model(model_desc)\n",
    "                # 使用全局优化器，以提高模型拟合的稳定性, 关闭所有警告\n",
    "                model.fit(dataset, solver='SLSQP')\n",
    "                # 获取模型拟合指标\n",
    "                # fit_indices = inspect(model)\n",
    "                stats = semopy.calc_stats(model)\n",
    "                # 提取常用的拟合指标\n",
    "                \n",
    "                fit_metrics = {\n",
    "                    'AIC': stats['AIC'].loc['Value'],\n",
    "                    'BIC': stats['BIC'].loc['Value'],\n",
    "                    'CFI': stats['CFI'].loc['Value'],\n",
    "                    'TLI': stats['TLI'].loc['Value'],\n",
    "                    'RMSEA': stats['RMSEA'].loc['Value'],\n",
    "                    # 'chi2': stats['chi2'].loc['Value'],\n",
    "                    # 'chi2_baseline': stats['chi2 Baseline'].loc['Value'],\n",
    "                    # 'NFI': stats['NFI'].loc['Value'],\n",
    "                    # 'LogLik': stats['LogLik'].loc['Value'],\n",
    "                }\n",
    "            except Exception as e:\n",
    "                # 如果模型无法收敛，设置拟合指标为缺失值\n",
    "                # print(e)\n",
    "                # print(model_desc_template)\n",
    "                fit_metrics = {\n",
    "                    'AIC': None,\n",
    "                    'BIC': None,\n",
    "                    'CFI': None,\n",
    "                    'TLI': None,\n",
    "                    'RMSEA': None,\n",
    "                    # 'chi2': None,\n",
    "                    # 'chi2_baseline': None,\n",
    "                    # 'NFI': None,\n",
    "                    # \"logLik\": None,\n",
    "                }\n",
    "            fit_metrics['Model'] = label\n",
    "            fit_indices_list.append(fit_metrics)\n",
    "        # 将拟合指标列表转换为 DataFrame\n",
    "        fit_df = pd.DataFrame(fit_indices_list)\n",
    "        fit_df['variable'] = variable\n",
    "        df.append(fit_df)\n",
    "\n",
    "    # 合并所有变量的结果\n",
    "    result_df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "    # 将模型名称和变量名称组合，展开为列\n",
    "    pivot_df = result_df.pivot(index='variable', columns='Model')\n",
    "    pivot_df.columns = ['_'.join(col).strip() for col in pivot_df.columns.values]\n",
    "    pivot_df.reset_index(inplace=True)\n",
    "    pivot_df['dataset'] = dataset.name\n",
    "\n",
    "    # 返回结果 DataFrame\n",
    "    return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "086680b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Given a graph as adjacency_matrix, create the class labels of each variable.\n",
    "    \"\"\"\n",
    "\n",
    "    adjacency_graph, adjacency_label = create_graph_label()\n",
    "    labels = get_labels(adjacency_matrix, adjacency_label)\n",
    "    variables = adjacency_matrix.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"variable\": variables,\n",
    "        \"label\": [labels[variable] for variable in variables],\n",
    "    })\n",
    "    df[\"dataset\"] = adjacency_matrix.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "174d66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_some_columns(names_datasets, function):\n",
    "    \"\"\"\n",
    "    Apply an embedding function to a list of datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    df = []\n",
    "    for name, dataset in tqdm(names_datasets.items()):\n",
    "        dataset = names_datasets[name]\n",
    "        dataset.name = name\n",
    "    \n",
    "        try:\n",
    "            df_dataset = function(dataset)\n",
    "        except ValueError as e:\n",
    "            print(name, e)\n",
    "            raise NotImplementedError\n",
    "\n",
    "        df_dataset[\"dataset\"] = name\n",
    "        df.append(df_dataset)\n",
    "\n",
    "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5d95f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_some_columns_parallel(names_datasets, function, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Apply an embedding function to a list of datasets.\n",
    "\n",
    "    Parallel version.\n",
    "    \"\"\"\n",
    "\n",
    "    def f(name, dataset, function):\n",
    "        dataset.name = name\n",
    "        df_dataset = function(dataset)\n",
    "        df_dataset[\"dataset\"] = name\n",
    "        return df_dataset\n",
    "\n",
    "    df = joblib.Parallel(n_jobs=n_jobs)(\n",
    "        joblib.delayed(f)(name, dataset, function)\n",
    "        for name, dataset in tqdm(names_datasets.items())\n",
    "    )\n",
    "\n",
    "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f344d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_columns(functions_names_datasets, n_jobs=-1, create_dimension_feature = False):\n",
    "    \"\"\"\n",
    "    given a dictionary of {function1:names, function2:names,...} apply\n",
    "    the desired functions to the list of datasets and merge all of them\n",
    "    in a single X_y_group dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    columns = []\n",
    "    if create_dimension_feature:\n",
    "        dimension_feature = create_some_columns(functions_names_datasets[list(functions_names_datasets.keys())[0]], add_dimension_feature)\n",
    "        columns.append(dimension_feature)\n",
    "    \n",
    "    for function, names_datasets in functions_names_datasets.items():\n",
    "        print(f\"set: {function.__name__}\")\n",
    "\n",
    "        if n_jobs != 1:\n",
    "            feature_set = create_some_columns_parallel(names_datasets, function, n_jobs=n_jobs)\n",
    "        else:\n",
    "            feature_set = create_some_columns(names_datasets, function)\n",
    "\n",
    "        columns.append(feature_set)\n",
    "\n",
    "    # Merge all feature sets into a single dataframe:\n",
    "    columns = functools.reduce(\n",
    "        lambda left, right: pd.merge(left, right, on=[\"dataset\", \"variable\"]),\n",
    "        columns,\n",
    "    )\n",
    "\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbe294b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import catboost\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import random\n",
    "import typing\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7359723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"训练的工具函数\"\"\"\n",
    "def remove_columns_with_keywords(df, keywords):\n",
    "    columns_to_drop = [col for col in df.columns if any(keyword in col for keyword in keywords)]\n",
    "    return df.drop(columns=columns_to_drop)\n",
    "\n",
    "def get_columns_with_keywords(df, keywords):\n",
    "    columns_to_get = [col for col in df.columns if any(keyword in col for keyword in keywords)]\n",
    "    return columns_to_get\n",
    "\n",
    "def recovery_dataset_id(X_y_group_train):\n",
    "    # 假设'dataset'列是需要转换的列\n",
    "    X_y_group_train['dataset'] = X_y_group_train['dataset'].apply(lambda x: f'{int(x):05}')\n",
    "    return X_y_group_train\n",
    "\n",
    "def clean_feature_names(X):\n",
    "    def clean_name(name):\n",
    "        # 将空格替换为下划线\n",
    "        name = name.replace(' ', '_')\n",
    "        # 将逗号替换为下划线\n",
    "        name = name.replace(',', '_')\n",
    "        # 移除或替换其他特殊字符\n",
    "        name = re.sub(r'[^\\w\\-]', '_', name)\n",
    "        # 确保名称不以数字开头\n",
    "        if name and name[0].isdigit():\n",
    "            name = 'f_' + name\n",
    "        # 移除连续的下划线\n",
    "        name = re.sub(r'_+', '_', name)\n",
    "        # 移除开头和结尾的下划线\n",
    "        name = name.strip('_')\n",
    "        return name\n",
    "\n",
    "    X.columns = [clean_name(col) for col in X.columns]\n",
    "    return X\n",
    "\n",
    "def process_categorical_features(df, max_unique=10):\n",
    "    \"\"\"\n",
    "    检测和处理数据框中的类别变量。\n",
    "\n",
    "    参数：\n",
    "    - df (pd.DataFrame): 输入的数据框。\n",
    "    - max_unique (int): 判定为类别变量的最大唯一值数量。\n",
    "\n",
    "    返回：\n",
    "    - cat_idxs (list of int): 类别特征的索引。\n",
    "    - cat_dims (list of int): 每个类别特征的模态数。\n",
    "    - df (pd.DataFrame): 经过编码后的数据框。\n",
    "    \"\"\"\n",
    "    cat_cols = [col for col in df.columns if df[col].nunique() <= max_unique]\n",
    "    cat_dims = []\n",
    "    cat_idxs = []\n",
    "\n",
    "    for col in cat_cols:\n",
    "        print(f\"     ->->->处理类别特征: {col}，唯一值数量: {df[col].nunique()}\")\n",
    "        # 使用 LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str).fillna('NaN'))\n",
    "        cat_dims.append(len(le.classes_))\n",
    "        cat_idxs.append(df.columns.get_loc(col))\n",
    "\n",
    "    return cat_idxs, cat_dims, df\n",
    "\n",
    "def filter_features(X_test, model):\n",
    "    \"\"\"\n",
    "    过滤测试数据集，只保留模型训练时使用的特征。\n",
    "\n",
    "    参数:\n",
    "    X_test : pandas.DataFrame 或 numpy.array\n",
    "        需要进行预测的测试数据\n",
    "    model : 已训练的模型\n",
    "        包含 feature_names_in_ 属性的模型（如sklearn的大多数模型）\n",
    "\n",
    "    返回:\n",
    "    pandas.DataFrame 或 numpy.array\n",
    "        只包含模型训练时使用的特征的测试数据\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_name_'):\n",
    "        # 获取模型训练时使用的特征名称\n",
    "        model_features = model.feature_name_\n",
    "        \n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            print(\"测试数据类型为 DataFrame\")\n",
    "            # 对于DataFrame，我们可以直接使用列名\n",
    "            common_features = list(set(X_test.columns) & set(model_features))\n",
    "            missing_features = set(model_features) - set(X_test.columns)\n",
    "            if missing_features:\n",
    "                print(f\"警告: 测试数据缺少 {len(missing_features)} 个训练时使用的特征: {missing_features}\")\n",
    "            extra_features = set(X_test.columns) - set(model_features)\n",
    "            if extra_features:\n",
    "                print(f\"警告: 移除了 {len(extra_features)} 个在训练时未使用的特征: {extra_features}\")\n",
    "            return X_test[common_features]\n",
    "        elif isinstance(X_test, np.ndarray):\n",
    "            print(\"测试数据类型为 numpy array\")\n",
    "            # 对于numpy数组，我们假设特征的顺序与训练时相同\n",
    "            if X_test.shape[1] > len(model_features):\n",
    "                print(f\"警告: 测试数据包含额外的特征。只使用前 {len(model_features)} 个特征。\")\n",
    "                return X_test[:, :len(model_features)]\n",
    "            elif X_test.shape[1] < len(model_features):\n",
    "                missing_count = len(model_features) - X_test.shape[1]\n",
    "                print(f\"错误: 测试数据的特征数 ({X_test.shape[1]}) 少于模型训练时的特征数 ({len(model_features)})\")\n",
    "                print(f\"缺少的特征数量: {missing_count}\")\n",
    "                raise ValueError(\"特征数量不匹配\")\n",
    "            return X_test\n",
    "    else:\n",
    "        print(\"警告: 模型没有 feature_name_ 属性。无法验证特征。\")\n",
    "        return X_test\n",
    "\n",
    "def align_features(X, model):\n",
    "    \"\"\"\n",
    "    调整输入特征的顺序，使其与模型训练时的特征顺序一致。\n",
    "\n",
    "    参数:\n",
    "    X : pandas.DataFrame 或 numpy.ndarray\n",
    "        需要调整顺序的输入特征\n",
    "    model : 已训练的模型\n",
    "        包含 feature_names_in_ 属性的模型（如sklearn的大多数模型）\n",
    "\n",
    "    返回:\n",
    "    pandas.DataFrame 或 numpy.ndarray\n",
    "        特征顺序调整后的数据\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'feature_name_'):\n",
    "        print(\"警告: 模型没有 feature_name_ 属性。无法调整特征顺序。\")\n",
    "        return X\n",
    "\n",
    "    model_features = model.feature_name_\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        print(\"测试数据类型为 DataFrame\")\n",
    "        # 对于DataFrame，我们可以直接使用列名重新排序\n",
    "        if set(X.columns) != set(model_features):\n",
    "            raise ValueError(\"输入特征与模型特征不完全匹配。\")\n",
    "        return X.reindex(columns=model_features)\n",
    "\n",
    "    elif isinstance(X, np.ndarray):\n",
    "        print(\"测试数据类型为 numpy array\")\n",
    "        if X.shape[1] != len(model_features):\n",
    "            raise ValueError(\"输入特征数量与模型特征数量不匹配。\")\n",
    "        \n",
    "        # 对于numpy数组，我们需要创建一个映射来重新排序\n",
    "        current_features = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "        df = pd.DataFrame(X, columns=current_features)\n",
    "        feature_mapping = dict(zip(current_features, X.columns if isinstance(X, pd.DataFrame) else model_features))\n",
    "        df = df.rename(columns=feature_mapping)\n",
    "        return df.reindex(columns=model_features).values\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"输入X必须是pandas.DataFrame或numpy.ndarray。\")\n",
    "\n",
    "def filter_train_data(\n",
    "    X_train: typing.Dict[str, pd.DataFrame],\n",
    "    y_train: typing.Dict[str, pd.DataFrame],\n",
    "    n_samples: int = 50\n",
    ") -> typing.Tuple[typing.Dict[str, pd.DataFrame], typing.Dict[str, pd.DataFrame]]:\n",
    "    assert n_samples >= 8, \"n_samples must be at least 8 to ensure representation of all dimensions\"\n",
    "\n",
    "    all_keys = list(X_train.keys())\n",
    "    dimension_dict = defaultdict(list)\n",
    "\n",
    "    # Group keys by their dimension (number of columns)\n",
    "    for key in all_keys:\n",
    "        dim = X_train[key].shape[1]\n",
    "        dimension_dict[dim].append(key)\n",
    "\n",
    "    selected_keys = []\n",
    "\n",
    "    # Ensure at least one sample from each dimension\n",
    "    for dim in range(3, 11):  # dimensions from 3 to 10\n",
    "        if dimension_dict[dim]:\n",
    "            selected_keys.append(random.choice(dimension_dict[dim]))\n",
    "            dimension_dict[dim].remove(selected_keys[-1])\n",
    "\n",
    "    # Fill the rest randomly\n",
    "    remaining_keys = [key for keys in dimension_dict.values() for key in keys]\n",
    "    additional_keys = random.sample(remaining_keys, min(n_samples - len(selected_keys), len(remaining_keys)))\n",
    "    selected_keys.extend(additional_keys)\n",
    "\n",
    "    # Shuffle the selected keys to randomize the order\n",
    "    random.shuffle(selected_keys)\n",
    "\n",
    "    # Filter X_train and y_train\n",
    "    filtered_X_train = {k: X_train[k] for k in selected_keys}\n",
    "    filtered_y_train = {k: y_train[k] for k in selected_keys}\n",
    "\n",
    "    return filtered_X_train, filtered_y_train\n",
    "\n",
    "def filter_test_data(\n",
    "    X_test: typing.Dict[str, pd.DataFrame],\n",
    "    n_samples: int = 50\n",
    ") -> typing.Dict[str, pd.DataFrame]:\n",
    "    assert n_samples >= 8, \"n_samples must be at least 8 to ensure representation of all dimensions\"\n",
    "\n",
    "    all_keys = list(X_test.keys())\n",
    "    dimension_dict = defaultdict(list)\n",
    "\n",
    "    # Group keys by their dimension (number of columns)\n",
    "    for key in all_keys:\n",
    "        dim = X_test[key].shape[1]\n",
    "        dimension_dict[dim].append(key)\n",
    "\n",
    "    selected_keys = []\n",
    "\n",
    "    # Ensure at least one sample from each dimension\n",
    "    for dim in range(3, 11):  # dimensions from 3 to 10\n",
    "        if dimension_dict[dim]:\n",
    "            selected_keys.append(random.choice(dimension_dict[dim]))\n",
    "            dimension_dict[dim].remove(selected_keys[-1])\n",
    "\n",
    "    # Fill the rest randomly\n",
    "    remaining_keys = [key for keys in dimension_dict.values() for key in keys]\n",
    "    additional_keys = random.sample(remaining_keys, min(n_samples - len(selected_keys), len(remaining_keys)))\n",
    "    selected_keys.extend(additional_keys)\n",
    "\n",
    "    # Shuffle the selected keys to randomize the order\n",
    "    random.shuffle(selected_keys)\n",
    "\n",
    "    # Filter X_test\n",
    "    filtered_X_test = {k: X_test[k] for k in selected_keys}\n",
    "\n",
    "    return filtered_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8ecad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_online_data(X_train, y_train, local_datasets_describe):\n",
    "    online_add_X_train = {}\n",
    "    online_add_y_train = {}\n",
    "\n",
    "    # 取云端数据集的Key\n",
    "    dataset_keys = list(X_train.keys())\n",
    "    # Initialize\n",
    "    missing_keys_count = 0       # 云端数据集的Key不在本地数据集的Key中\n",
    "\n",
    "    for key in tqdm(dataset_keys, desc=\"Checking\", unit=\"dataset\"):\n",
    "        dataset = X_train[key]\n",
    "        DAG = y_train[key]\n",
    "        # 云端数据集的Key不在本地数据集的Key中\n",
    "        if key not in local_datasets_describe:\n",
    "            print(f\"     ->Key not found in local dataset: {key}\")\n",
    "            missing_keys_count += 1\n",
    "            online_add_X_train[key] = dataset\n",
    "            online_add_y_train[key] = DAG\n",
    "\n",
    "    # 打印统计信息\n",
    "    print(f\"     ->Statistics:\")\n",
    "    print(f\"     ->Number of keys not found in the local dataset: {missing_keys_count}\")\n",
    "    return online_add_X_train, online_add_y_train\n",
    "\n",
    "def create_submission(X_y_pred_test):\n",
    "    \"\"\"\n",
    "    From the predicted test set, for each dataset, take predicted\n",
    "    classes of all variables, create the adjacency matrix, then create\n",
    "    the submission in the requested format.\n",
    "    \"\"\"\n",
    "\n",
    "    submission = {}\n",
    "    for name, prediction in tqdm(X_y_pred_test.groupby(\"dataset\"), delay=10):\n",
    "        variables_labels = prediction[[\"variable\", \"label_predicted\"]].set_index(\"variable\")\n",
    "        variables = variables_labels.index.tolist()\n",
    "        variables_all = [\"X\", \"Y\"] + variables\n",
    "\n",
    "        adjacency_matrix = pd.DataFrame(index=variables_all, columns=variables_all)\n",
    "        adjacency_matrix.index.name = \"parent\"\n",
    "        adjacency_matrix[:] = 0\n",
    "        adjacency_matrix.loc[\"X\", \"Y\"] = 1\n",
    "\n",
    "        for v in variables:\n",
    "            l = variables_labels.loc[v].item()\n",
    "            if l == \"Cause of X\":\n",
    "                adjacency_matrix.loc[v, \"X\"] = 1\n",
    "            elif l == \"Cause of Y\":\n",
    "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
    "            elif l == \"Consequence of X\":\n",
    "                adjacency_matrix.loc[\"X\", v] = 1\n",
    "            elif l == \"Consequence of Y\":\n",
    "                adjacency_matrix.loc[\"Y\", v] = 1\n",
    "            elif l == \"Confounder\":\n",
    "                adjacency_matrix.loc[v, \"X\"] = 1\n",
    "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
    "            elif l == \"Collider\":\n",
    "                adjacency_matrix.loc[\"X\", v] = 1\n",
    "                adjacency_matrix.loc[\"Y\", v] = 1\n",
    "            elif l == \"Mediator\":\n",
    "                adjacency_matrix.loc[\"X\", v] = 1\n",
    "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
    "            elif l == \"Confounder\":\n",
    "                pass\n",
    "\n",
    "        for i in variables_all:\n",
    "            for j in variables_all:\n",
    "                submission[f'{name}_{i}_{j}'] = int(adjacency_matrix.loc[i, j])\n",
    "\n",
    "    return submission\n",
    "\n",
    "class PretrainedVotingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimators, voting='soft', weights=None):\n",
    "        self.estimators = estimators\n",
    "        self.voting = voting\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # 已经预训练，无需再训练\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.voting == 'soft':\n",
    "            # 对于概率投票\n",
    "            probas = np.asarray([clf.predict_proba(X) for clf in self.estimators])\n",
    "            avg_proba = np.average(probas, axis=0, weights=self.weights)\n",
    "            return np.argmax(avg_proba, axis=1)\n",
    "        else:\n",
    "            # 对于硬投票\n",
    "            predictions = np.asarray([clf.predict(X) for clf in self.estimators]).T\n",
    "            maj_vote = np.apply_along_axis(\n",
    "                lambda x: np.argmax(np.bincount(x, weights=self.weights)), axis=1, arr=predictions\n",
    "            )\n",
    "            return maj_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2a15476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    X_train: typing.Dict[str, pd.DataFrame],\n",
    "    y_train: typing.Dict[str, pd.DataFrame],\n",
    "    # number_of_features: int,\n",
    "    model_directory_path: str,\n",
    "    # id_column_name: str,\n",
    "    # prediction_column_name: str,\n",
    "    # has_gpu: bool,\n",
    ") -> None:\n",
    "    # 设置版本（模型、数据和结果保存名称）\n",
    "    version = 'local_test_localdata_earlystop_newaugment'\n",
    "    X_y_group_train_pathname = os.path.join(model_directory_path, f\"X_y_group_train_{version}.pkl\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"##### Loading {X_y_group_train_pathname}\")\n",
    "        # 读取本地数据\n",
    "        X_y_group_train = pd.read_pickle(X_y_group_train_pathname)\n",
    "        print('     ->已加载本地数据 X_y_group_train')\n",
    "        if 'Unnamed_0' in X_y_group_train.columns:\n",
    "            print(\"     ->'Unnamed_0' column found. It will be removed.\")\n",
    "            X_y_group_train.drop(columns=['Unnamed_0'], inplace=True)\n",
    "        shape = X_y_group_train.shape\n",
    "        columns = X_y_group_train.columns.tolist()\n",
    "        print(f'     ->X_y_group_train形状: {shape} \\n X_y_group_train列名: {columns}')\n",
    "\n",
    "\n",
    "        print(f\"##### Checking More Data in Online X_train\")\n",
    "        # 读取本地数据描述字典\n",
    "        local_datasets_describe_path = os.path.join(model_directory_path, \"local_datasets_describe_drop200.json\")\n",
    "        local_datasets_describe = json.load(open(local_datasets_describe_path, 'r', encoding='utf-8'))\n",
    "        online_add_X_train, online_add_y_train = check_online_data(X_train, y_train, local_datasets_describe)\n",
    "\n",
    "\n",
    "        print(\"##### Creating online_add_X_y_group_train\")\n",
    "        # 补充线上数据\n",
    "        # # 小批量测试\n",
    "        # online_add_X_train, online_add_y_train = filter_train_data(online_add_X_train, online_add_y_train, n_samples=20)\n",
    "        # # 打印X_train中每个数据集的形状\n",
    "        # for dataset_name, dataset in online_add_X_train.items():\n",
    "        #     print(f\"{dataset_name} 的形状: {dataset.shape}\")\n",
    "        \n",
    "        # 数据增强\n",
    "        online_add_X_train, online_add_y_train, failure_count = augment_data(online_add_X_train, online_add_y_train, augment_factor=6.0)\n",
    "        print(f'     ->线上数据增强失败次数: {failure_count}')\n",
    "        print(f'     ->线上数据增强后样本量: {len(online_add_X_train)}, {len(online_add_y_train)}')\n",
    "        # # 打印X_train中每个数据集的形状\n",
    "        # for dataset_name, dataset in online_add_X_train.items():\n",
    "        #     print(f\"{dataset_name} 的形状: {dataset.shape}\")\n",
    "        \n",
    "        online_add_names_datasets_train = online_add_X_train\n",
    "        online_add_names_graphs_train = online_add_y_train\n",
    "        online_add_X_y_group_train = create_all_columns(\n",
    "            {\n",
    "                label: online_add_names_graphs_train,\n",
    "                \n",
    "                pearson_correlation: online_add_names_datasets_train,\n",
    "                rolling_abs_pearson_correlation: online_add_names_datasets_train,\n",
    "                spearman_correlation: online_add_names_datasets_train,\n",
    "                kendall_correlation: online_add_names_datasets_train,\n",
    "                mutual_information: online_add_names_datasets_train, \n",
    "                conditional_mutual_information: online_add_names_datasets_train,\n",
    "                distance_correlation: online_add_names_datasets_train,\n",
    "                partial_correlation: online_add_names_datasets_train,\n",
    "                copula_entropy: online_add_names_datasets_train,\n",
    "                PPS_feature: online_add_names_datasets_train,\n",
    "\n",
    "                linear_regression_feature: online_add_names_datasets_train,\n",
    "                ridge_regression_feature: online_add_names_datasets_train,\n",
    "\n",
    "                ExactSearch_feature: online_add_names_datasets_train,\n",
    "                PC_feature: online_add_names_datasets_train,\n",
    "                FCI_feature: online_add_names_datasets_train,\n",
    "                GRaSP_feature: online_add_names_datasets_train,\n",
    "                DML_feature: online_add_names_datasets_train,\n",
    "                \n",
    "                piecewise_linear_regression_feature: online_add_names_datasets_train,\n",
    "                piecewise_quadratic_regression_feature: online_add_names_datasets_train,\n",
    "                piecewise_quadratic_regression_feature_4_improved: online_add_names_datasets_train,\n",
    "                sliding_window_linear_regression_feature: online_add_names_datasets_train,\n",
    "\n",
    "                sem_features: online_add_names_datasets_train,\n",
    "                grid_feature: online_add_names_datasets_train,\n",
    "                discrete_statistic_features: online_add_names_datasets_train\n",
    "            },\n",
    "            n_jobs=-1,\n",
    "            create_dimension_feature=True,\n",
    "        )\n",
    "        online_add_X_y_group_train['MI(v,X)^2'] = online_add_X_y_group_train['MI(v,X)'] ** 2\n",
    "        online_add_X_y_group_train['MI(v,Y)^2'] = online_add_X_y_group_train['MI(v,Y)'] ** 2\n",
    "        online_add_X_y_group_train['MI(X,Y)^2'] = online_add_X_y_group_train['MI(X,Y)'] ** 2\n",
    "        online_add_X_y_group_train['max(MI(v, others))^2'] = online_add_X_y_group_train['max(MI(v, others))'] ** 2\n",
    "        online_add_X_y_group_train['min(MI(v, others))^2'] = online_add_X_y_group_train['min(MI(v, others))'] ** 2\n",
    "        \n",
    "        online_add_X_y_group_train = online_add_X_y_group_train.loc[:,~online_add_X_y_group_train.columns.duplicated()]\n",
    "        print('     ->已生成线上补充数据 online_add_X_y_group_train')\n",
    "\n",
    "        print(\"##### Checking different columns between online_add_X_y_group_train and X_y_group_train\")\n",
    "        # 检查online_add_X_y_group_train的列名和X_y_group_train的列名是否相同\n",
    "        if online_add_X_y_group_train.columns.equals(X_y_group_train.columns):\n",
    "            print('     ->online_add_X_y_group_train的列名和X_y_group_train的列名相同')\n",
    "            print(f'     ->拼接前样本量: {X_y_group_train.shape}, {online_add_X_y_group_train.shape}')\n",
    "            # 拼接\n",
    "            X_y_group_train = pd.concat([X_y_group_train, online_add_X_y_group_train], ignore_index=True)\n",
    "            print(f'     ->拼接后样本量: {X_y_group_train.shape}')\n",
    "            shape = X_y_group_train.shape\n",
    "            columns = X_y_group_train.columns.tolist()\n",
    "            print('     ->线上训练数据 X_y_group_train 已生成')\n",
    "            print(f'     ->X_y_group_train形状: {shape} \\n X_y_group_train列名: {columns}')\n",
    "        else:\n",
    "            print('     ->online_add_X_y_group_train的列名和X_y_group_train的列名不同')\n",
    "            print('!!!!!!!无法拼接本地和线上数据集，跳过训练，使用已经训练好的模型进行推理!!!!!!!')\n",
    "            return\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"##### Creating X_y_group_train\")\n",
    "\n",
    "        # # 小批量测试\n",
    "        # X_train, y_train = filter_train_data(X_train, y_train, n_samples=20)\n",
    "        \n",
    "        # 数据增强\n",
    "        X_train, y_train, failure_count = augment_data(X_train, y_train, augment_factor=3.5)\n",
    "        print(f'     ->数据增强失败次数: {failure_count}')\n",
    "        print(f'     ->数据增强后样本量: {len(X_train)}, {len(y_train)}')\n",
    "        \n",
    "        names_datasets_train = X_train\n",
    "        names_graphs_train = y_train\n",
    "        X_y_group_train = create_all_columns(\n",
    "            {\n",
    "                label: names_graphs_train,\n",
    "                \n",
    "                pearson_correlation: names_datasets_train,\n",
    "                rolling_abs_pearson_correlation: names_datasets_train,\n",
    "                spearman_correlation: names_datasets_train,\n",
    "                kendall_correlation: names_datasets_train,\n",
    "                mutual_information: names_datasets_train, \n",
    "                conditional_mutual_information: names_datasets_train,\n",
    "                distance_correlation: names_datasets_train,\n",
    "                partial_correlation: names_datasets_train,\n",
    "                copula_entropy: names_datasets_train,\n",
    "                PPS_feature: names_datasets_train,\n",
    "\n",
    "                linear_regression_feature: names_datasets_train,\n",
    "                ridge_regression_feature: names_datasets_train,\n",
    "\n",
    "                ExactSearch_feature: names_datasets_train,\n",
    "                PC_feature: names_datasets_train,\n",
    "                FCI_feature: names_datasets_train,\n",
    "                GRaSP_feature: names_datasets_train,\n",
    "                DML_feature: names_datasets_train,\n",
    "                \n",
    "                piecewise_linear_regression_feature: names_datasets_train,\n",
    "                piecewise_quadratic_regression_feature: names_datasets_train,\n",
    "                piecewise_quadratic_regression_feature_4_improved: names_datasets_train,\n",
    "                sliding_window_linear_regression_feature: names_datasets_train,\n",
    "\n",
    "                sem_features: names_datasets_train,\n",
    "                grid_feature: names_datasets_train,\n",
    "                discrete_statistic_features: names_datasets_train\n",
    "            },\n",
    "            n_jobs=-1,\n",
    "            create_dimension_feature=True,\n",
    "        )\n",
    "        X_y_group_train['MI(v,X)^2'] = X_y_group_train['MI(v,X)'] ** 2\n",
    "        X_y_group_train['MI(v,Y)^2'] = X_y_group_train['MI(v,Y)'] ** 2\n",
    "        X_y_group_train['MI(X,Y)^2'] = X_y_group_train['MI(X,Y)'] ** 2\n",
    "        X_y_group_train['max(MI(v, others))^2'] = X_y_group_train['max(MI(v, others))'] ** 2\n",
    "        X_y_group_train['min(MI(v, others))^2'] = X_y_group_train['min(MI(v, others))'] ** 2\n",
    "        \n",
    "        X_y_group_train = X_y_group_train.loc[:,~X_y_group_train.columns.duplicated()]\n",
    "\n",
    "        shape = X_y_group_train.shape\n",
    "        columns = X_y_group_train.columns.tolist()\n",
    "        print('     ->线上训练数据 X_y_group_train 已生成')\n",
    "        print(f'     ->X_y_group_train形状: {shape} \\n X_y_group_train列名: {columns}')\n",
    "        \n",
    "\n",
    "    print(\"##### Adding numeric labels y\")\n",
    "    # 添加数值标签 y\n",
    "    le = LabelEncoder()\n",
    "    le.classes_ = np.array([\n",
    "        'Confounder', 'Collider',\n",
    "        'Mediator', 'Independent',\n",
    "        'Cause of X', 'Consequence of X', \n",
    "        'Cause of Y', 'Consequence of Y',\n",
    "    ])\n",
    "    X_y_group_train[\"y\"] = le.transform(X_y_group_train[\"label\"])\n",
    "    # 重新排列列\n",
    "    X_y_group_train = X_y_group_train[[\"dataset\", \"variable\"] + X_y_group_train.columns.drop([\"dataset\", \"variable\", \"label\", \"y\"]).tolist() + [\"label\", \"y\"]]\n",
    "\n",
    "\n",
    "    print(\"##### Data Preprocessing...\")\n",
    "    # 定义要删除的列\n",
    "    blacklist = [\n",
    "        \"ttest(v,X)\", \n",
    "        \"pvalue(ttest(v,X))<=0.05\", \n",
    "        \"ttest(v,Y)\", \n",
    "        \"pvalue(ttest(v,Y))<=0.05\", \n",
    "        \"ttest(X,Y)\", \n",
    "        \"pvalue(ttest(X,Y))<=0.05\",\n",
    "        \"square_dimension\", \n",
    "        \"max(PPS(v,others))\",\n",
    "        \"TLI_Collider\",\n",
    "        \"TLI_Confounder\",\n",
    "        \"RMSEA_Collider\",\n",
    "        \"RMSEA_Confounder\",\n",
    "    ]\n",
    "    columns_to_drop = [col for col in blacklist if col in X_y_group_train.columns]\n",
    "    X_y_group_train = X_y_group_train.drop(columns=columns_to_drop)\n",
    "    print(f'     1.删除多余列后样本量: {X_y_group_train.shape}')\n",
    "    # 填充缺失值\n",
    "    X_y_group_train = X_y_group_train.replace([np.inf, -np.inf], np.nan)\n",
    "    numeric_columns = X_y_group_train.select_dtypes(include=[np.number]).columns\n",
    "    X_y_group_train[numeric_columns] = X_y_group_train[numeric_columns].fillna(X_y_group_train[numeric_columns].mean())\n",
    "    print(f'     2.填充缺失值后样本量: {X_y_group_train.shape}')\n",
    "    # 清理特征名称\n",
    "    X_y_group_train = clean_feature_names(X_y_group_train)\n",
    "    print(f'     3.清理特征名称后样本量: {X_y_group_train.shape}')\n",
    "\n",
    "\n",
    "    print(\"##### Extracting X_train, y_train\")\n",
    "    X_y_group_train['raw_key'] = X_y_group_train['dataset'].str[-5:]\n",
    "    all_key_list = X_y_group_train['raw_key'].unique().tolist()\n",
    "    print(f'     ->all_key_list中key的个数: {len(all_key_list)}')\n",
    "    X = X_y_group_train.drop([\"variable\", \"dataset\", \"label\", \"y\"], axis=\"columns\")\n",
    "    y = X_y_group_train[\"y\"]\n",
    "\n",
    "    # 处理类别特征\n",
    "    cat_idxs, cat_dims, X = process_categorical_features(X)\n",
    "    print(f'     ->类别特征索引 (cat_idxs): {cat_idxs}')\n",
    "    print(f'     ->类别特征模态数 (cat_dims): {cat_dims}')\n",
    "    print(f'     4.处理类别特征后样本量: {X.shape}, {y.shape}')\n",
    "\n",
    "\n",
    "    print(\"##### Extracting X_train, y_train, and group\")\n",
    "    # 分割all_key_list为训练集和测试集key\n",
    "    train_keys, test_keys = train_test_split(all_key_list, test_size=0.1, random_state=42)\n",
    "    # 定义掩码：X的'dataset'列的后五个字符在train_keys中\n",
    "    mask = X.raw_key.str.endswith(tuple(train_keys))\n",
    "    X = X.drop(columns=['raw_key'])\n",
    "    # 使用掩码分割数据集为训练集和测试集\n",
    "    X_train = X[mask]\n",
    "    X_test = X[~mask]\n",
    "    y_train = y[mask]\n",
    "    y_test = y[~mask]\n",
    "    # # 分割数据集为训练集和测试集\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #     X, y, test_size=0.1, random_state=42, stratify=y\n",
    "    # )\n",
    "    print(f'     5.分割数据集后样本量: {X_train.shape}, {X_test.shape}, {y_train.shape}, {y_test.shape}')\n",
    "    print(f'     ->y_train 唯一值: {np.unique(y_train)}')\n",
    "    print(f'     ->y_test 唯一值: {np.unique(y_test)}')\n",
    "\n",
    "\n",
    "    print(\"##### Computing class weights\")\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "    # 类别权重列表\n",
    "    class_weights = list(class_weights) \n",
    "    print(f'     ->分类目标权重列表: {class_weights}')\n",
    "    # 类别权重字典\n",
    "    weight_dict = dict(zip(classes, class_weights))\n",
    "    print(f'     ->分类目标权重字典: {weight_dict}')\n",
    "    # 样本权重\n",
    "    sample_weights = y_train.map(weight_dict)\n",
    "\n",
    "\n",
    "    print(\"##### Start training\")\n",
    "    early_stop = xgb.callback.EarlyStopping(\n",
    "        rounds=20, save_best=True, maximize=False, metric_name='mlogloss'\n",
    "    )\n",
    "    xgb_model = XGBClassifier(\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.05,\n",
    "        objective='multi:softmax',\n",
    "        num_class=8,\n",
    "        eval_metric='mlogloss',\n",
    "\n",
    "        max_depth=6,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.3,\n",
    "        reg_lambda=18.0,\n",
    "        reg_alpha=18.0,\n",
    "        tree_method='gpu_hist',      # 如果没有GPU，可以使用 'hist'\n",
    "        max_delta_step=0.5,\n",
    "        \n",
    "        verbosity=0,\n",
    "        use_label_encoder=False,\n",
    "        random_state=42,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=sample_weights,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=100\n",
    "    )\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    params_str = pp.pformat(xgb_model.get_params())\n",
    "    print(\"##### Xgb的参数配置:\")\n",
    "    print(params_str)\n",
    "    print(\"############################\")\n",
    "    print(\"     Xgb的性能展示:\")\n",
    "    print(\"############################\")\n",
    "    # 预测\n",
    "    y_train_pred = xgb_model.predict(X_train)\n",
    "    y_test_pred = xgb_model.predict(X_test)\n",
    "    # 计算平衡准确率\n",
    "    train_score = balanced_accuracy_score(y_train, y_train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, y_test_pred)\n",
    "    print(f'     ->训练集平衡准确率: {train_score:.6f}')\n",
    "    print(f'     ->测试集平衡准确率: {test_score:.6f}')\n",
    "     # 打印分类报告\n",
    "    print('     ->测试集分类报告:')\n",
    "    report = classification_report(y_test, y_test_pred)\n",
    "    print(\"\\n\" + report)\n",
    "    # 保存\n",
    "    joblib.dump(\n",
    "        xgb_model,\n",
    "        os.path.join(model_directory_path, f\"xgb_model_{version}.joblib\")\n",
    "    )\n",
    "    print('     5.Xgboost训练完成')\n",
    "    print(\"############################\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    train_pool = catboost.Pool(data=X_train, label=y_train, cat_features=cat_idxs)\n",
    "    test_pool = catboost.Pool(data=X_test, label=y_test, cat_features=cat_idxs)\n",
    "    cat_model = CatBoostClassifier(\n",
    "        iterations=4000,\n",
    "        learning_rate=0.05,\n",
    "        classes_count=8,\n",
    "        class_weights=class_weights,\n",
    "        cat_features=cat_idxs,\n",
    "        loss_function='MultiClass',\n",
    "        eval_metric='Accuracy',\n",
    "\n",
    "        depth=12,\n",
    "        l2_leaf_reg=18.0,\n",
    "        \n",
    "        verbose=100,\n",
    "        random_seed=42,\n",
    "        early_stopping_rounds=20,\n",
    "        task_type='GPU',\n",
    "        devices='0:1',  # 如果使用GPU,指定GPU设备\n",
    "        save_snapshot=False,\n",
    "        train_dir=f\"/tmp/catboost_info_{version}\",\n",
    "        leaf_estimation_method='Newton',  # 默认方法\n",
    "    )\n",
    "    cat_model.fit(\n",
    "        train_pool,\n",
    "        eval_set=test_pool,\n",
    "        use_best_model=True\n",
    "    )\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    params_str = pp.pformat(cat_model.get_params())\n",
    "    print(\"##### Cat的参数配置:\")\n",
    "    print(params_str)\n",
    "    print(\"############################\")\n",
    "    print(\"     Cat的性能展示:\")\n",
    "    print(\"############################\")\n",
    "    # 预测\n",
    "    y_train_pred = cat_model.predict(X_train)\n",
    "    y_test_pred = cat_model.predict(X_test)\n",
    "    # 计算平衡准确率\n",
    "    train_score = balanced_accuracy_score(y_train, y_train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"     ->训练集平衡准确率: {train_score:.6f}\")\n",
    "    print(f\"     ->测试集平衡准确率: {test_score:.6f}\")\n",
    "    # 打印分类报告\n",
    "    print(\"     ->测试集分类报告:\")\n",
    "    report = classification_report(y_test, y_test_pred)\n",
    "    print(\"\\n\" + report)\n",
    "    # 保存\n",
    "    joblib.dump(\n",
    "        cat_model,\n",
    "        os.path.join(model_directory_path, f\"cat_model_{version}.joblib\")\n",
    "    )\n",
    "    print('     6.Catboost训练完成')\n",
    "    print(\"############################\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.log_evaluation(period=100), lgb.early_stopping(stopping_rounds=20)\n",
    "    ]\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.05,\n",
    "        class_weight='balanced',\n",
    "\n",
    "        num_leaves=21,\n",
    "        max_depth=6,\n",
    "        min_child_samples=50,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_alpha=18.0,\n",
    "        reg_lambda=18.0,\n",
    "\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        device='cpu',\n",
    "        verbosity=-1\n",
    "    )\n",
    "    lgb_model.fit(\n",
    "        X_train, y_train, \n",
    "        callbacks=callbacks, \n",
    "        eval_set=[(X_test, y_test)],\n",
    "        categorical_feature=cat_idxs\n",
    "    )\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    params_str = pp.pformat(lgb_model.get_params())\n",
    "    print(\"##### Lgb的参数配置:\")\n",
    "    print(params_str)\n",
    "    print(\"############################\")\n",
    "    print(\"     Lgb的性能展示:\")\n",
    "    print(\"############################\")\n",
    "    # 预测\n",
    "    y_train_pred = lgb_model.predict(X_train)\n",
    "    y_test_pred = lgb_model.predict(X_test)\n",
    "    # 计算平衡准确率\n",
    "    train_score = balanced_accuracy_score(y_train, y_train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"     ->训练集平衡准确率: {train_score:.6f}\")\n",
    "    print(f\"     ->测试集平衡准确率: {test_score:.6f}\")\n",
    "    # 打印分类报告\n",
    "    print(\"     ->测试集分类报告:\")\n",
    "    report = classification_report(y_test, y_test_pred)\n",
    "    print(\"\\n\" + report)\n",
    "    # 保存\n",
    "    joblib.dump(\n",
    "        lgb_model,\n",
    "        os.path.join(model_directory_path, f\"lgb_model_{version}.joblib\")\n",
    "    )\n",
    "    print('     7.LightGBM训练完成')\n",
    "    print(\"############################\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    voting_clf = PretrainedVotingClassifier(\n",
    "        estimators=[xgb_model, cat_model, lgb_model],\n",
    "        voting='soft'\n",
    "    )\n",
    "    print(\"     Voting的性能展示:\")\n",
    "    print(\"############################\")\n",
    "    # 预测\n",
    "    y_train_pred = voting_clf.predict(X_train)\n",
    "    y_test_pred = voting_clf.predict(X_test)\n",
    "    # 计算平衡准确率\n",
    "    train_score = balanced_accuracy_score(y_train, y_train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"     ->训练集平衡准确率: {train_score:.6f}\")\n",
    "    print(f\"     ->测试集平衡准确率: {test_score:.6f}\")\n",
    "    # 打印分类报告\n",
    "    print(\"     ->测试集分类报告:\")\n",
    "    report = classification_report(y_test, y_test_pred)\n",
    "    print(\"\\n\" + report)\n",
    "    print('     8.Voting检查完成')\n",
    "    print(\"############################\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "221c521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(\n",
    "    X_test: typing.Dict[str, pd.DataFrame],\n",
    "    # number_of_features: int,\n",
    "    model_directory_path: str,\n",
    "    id_column_name: str,\n",
    "    prediction_column_name: str,\n",
    "    # has_gpu: bool,\n",
    "    # has_trained: bool,\n",
    ") -> pd.DataFrame:\n",
    "    version = 'local_test_localdata_earlystop_newaugment'\n",
    "    X_group_test_pathname = f\"/tmp/X_group_test_{version}.pkl\"\n",
    "    try:\n",
    "        print(f\"##### Loading {X_group_test_pathname}\")\n",
    "        X_group_test = pd.read_pickle(X_group_test_pathname)\n",
    "        if 'Unnamed_0' in X_group_test.columns:\n",
    "            print(\"     ->'Unnamed_0' column found. It will be removed.\")\n",
    "            X_group_test.drop(columns=['Unnamed_0'], inplace=True)\n",
    "        shape = X_group_test.shape\n",
    "        columns = X_group_test.columns.tolist()\n",
    "        print(f'     ->X_group_test形状: {shape} \\n X_group_test列名: {columns}')\n",
    "    except FileNotFoundError:\n",
    "        print(\"##### Creating X_group_test\")\n",
    "\n",
    "        # # 小批量测试\n",
    "        # X_test = filter_test_data(X_test, n_samples=20)\n",
    "\n",
    "        names_datasets_test = X_test\n",
    "        X_group_test = create_all_columns(\n",
    "            {   \n",
    "                pearson_correlation: names_datasets_test,\n",
    "                rolling_abs_pearson_correlation: names_datasets_test,\n",
    "                spearman_correlation: names_datasets_test,\n",
    "                kendall_correlation: names_datasets_test,\n",
    "                mutual_information: names_datasets_test, \n",
    "                conditional_mutual_information: names_datasets_test,\n",
    "                distance_correlation: names_datasets_test,\n",
    "                partial_correlation: names_datasets_test,\n",
    "                copula_entropy: names_datasets_test,\n",
    "                PPS_feature: names_datasets_test,\n",
    "\n",
    "                linear_regression_feature: names_datasets_test,\n",
    "                ridge_regression_feature: names_datasets_test,\n",
    "\n",
    "                ExactSearch_feature: names_datasets_test,\n",
    "                PC_feature: names_datasets_test,\n",
    "                FCI_feature: names_datasets_test,\n",
    "                GRaSP_feature: names_datasets_test,\n",
    "                DML_feature: names_datasets_test,\n",
    "                \n",
    "                piecewise_linear_regression_feature: names_datasets_test,\n",
    "                piecewise_quadratic_regression_feature: names_datasets_test,\n",
    "                piecewise_quadratic_regression_feature_4_improved: names_datasets_test,\n",
    "                sliding_window_linear_regression_feature: names_datasets_test,\n",
    "\n",
    "                sem_features: names_datasets_test,\n",
    "                grid_feature: names_datasets_test,\n",
    "                discrete_statistic_features: names_datasets_test\n",
    "            },\n",
    "            n_jobs=-1,\n",
    "            create_dimension_feature=True,\n",
    "            )\n",
    "        X_group_test['MI(v,X)^2'] = X_group_test['MI(v,X)'] ** 2\n",
    "        X_group_test['MI(v,Y)^2'] = X_group_test['MI(v,Y)'] ** 2\n",
    "        X_group_test['MI(X,Y)^2'] = X_group_test['MI(X,Y)'] ** 2\n",
    "        X_group_test['max(MI(v, others))^2'] = X_group_test['max(MI(v, others))'] ** 2\n",
    "        X_group_test['min(MI(v, others))^2'] = X_group_test['min(MI(v, others))'] ** 2\n",
    "        \n",
    "        X_group_test = X_group_test.loc[:,~X_group_test.columns.duplicated()]\n",
    "\n",
    "        shape = X_group_test.shape\n",
    "        columns = X_group_test.columns.tolist()\n",
    "        print('     ->线上测试数据 X_group_test 已生成')\n",
    "        print(f'     ->X_group_test形状: {shape} \\n X_group_test列名: {columns}')\n",
    "        \n",
    "        \n",
    "    print('##### Loading Models...')\n",
    "    xgb_model = joblib.load(os.path.join(model_directory_path, f\"xgb_model_{version}.joblib\"))\n",
    "    cat_model = joblib.load(os.path.join(model_directory_path, f\"cat_model_{version}.joblib\"))\n",
    "    lgb_model = joblib.load(os.path.join(model_directory_path, f\"lgb_model_{version}.joblib\"))\n",
    "\n",
    "    print(\"##### Data Preprocessing...\")\n",
    "    # 定义要删除的列\n",
    "    blacklist = [\n",
    "        \"ttest(v,X)\", \n",
    "        \"pvalue(ttest(v,X))<=0.05\", \n",
    "        \"ttest(v,Y)\", \n",
    "        \"pvalue(ttest(v,Y))<=0.05\", \n",
    "        \"ttest(X,Y)\", \n",
    "        \"pvalue(ttest(X,Y))<=0.05\",\n",
    "        \"square_dimension\", \n",
    "        \"max(PPS(v,others))\",\n",
    "        \"TLI_Collider\",\n",
    "        \"TLI_Confounder\",\n",
    "        \"RMSEA_Collider\",\n",
    "        \"RMSEA_Confounder\",\n",
    "    ]\n",
    "    columns_to_drop = [col for col in blacklist if col in X_group_test.columns]\n",
    "    X_group_test = X_group_test.drop(columns=columns_to_drop)\n",
    "    print(f'     1.删除多余列后样本量: {X_group_test.shape}')\n",
    "    # 填充缺失值\n",
    "    X_group_test = X_group_test.replace([np.inf, -np.inf], np.nan)\n",
    "    numeric_columns = X_group_test.select_dtypes(include=[np.number]).columns\n",
    "    X_group_test[numeric_columns] = X_group_test[numeric_columns].fillna(X_group_test[numeric_columns].mean())\n",
    "    print(f'     2.填充缺失值后样本量: {X_group_test.shape}')\n",
    "    # 清理特征名称\n",
    "    X_group_test = clean_feature_names(X_group_test)\n",
    "    print(f'     3.清理特征名称后样本量: {X_group_test.shape}')\n",
    "\n",
    "    print(\"##### Extracting X_test\")\n",
    "    X = X_group_test.drop([\"variable\", \"dataset\"], axis=\"columns\")\n",
    "\n",
    "    # 处理类别特征\n",
    "    cat_idxs, cat_dims, X = process_categorical_features(X)\n",
    "    print(f'     ->类别特征索引 (cat_idxs): {cat_idxs}')\n",
    "    print(f'     ->类别特征模态数 (cat_dims): {cat_dims}')\n",
    "\n",
    "    X_test = X\n",
    "    print(f'     4.处理类别特征后样本量: {X_test.shape}')\n",
    "\n",
    "    # X = clean_feature_names(X)   # 前面已经有了\n",
    "    X_test = filter_features(X_test, lgb_model)\n",
    "    X_test = align_features(X_test, lgb_model)\n",
    "    print(f'     5.对齐特征后样本量: {X_test.shape}')\n",
    "    \n",
    "    print(\"##### Predicting on X_test\")\n",
    "    voting_clf = PretrainedVotingClassifier(\n",
    "        estimators=[xgb_model, cat_model, lgb_model],\n",
    "        voting='soft'\n",
    "    )\n",
    "    y_predicted = voting_clf.predict(X_test)\n",
    "    # y_predicted = lgb_model.predict(X_test)\n",
    "    X_y_pred_test = X_group_test\n",
    "    X_y_pred_test[\"y_predicted\"] = y_predicted\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.classes_ = np.array([\n",
    "        'Confounder', 'Collider',\n",
    "        'Mediator', 'Independent',\n",
    "        'Cause of X', 'Consequence of X', \n",
    "        'Cause of Y', 'Consequence of Y',\n",
    "    ])\n",
    "    X_y_pred_test[\"label_predicted\"] = le.inverse_transform(y_predicted)\n",
    "\n",
    "    submission = create_submission(X_y_pred_test)\n",
    "    print('预测完成')\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        submission.items(),\n",
    "        columns=[\n",
    "            id_column_name,\n",
    "            prediction_column_name\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3771091",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# \"\"\"单模\"\"\"\n",
    "# def create_submission(X_y_pred_test):\n",
    "#     \"\"\"\n",
    "#     From the predicted test set, for each dataset, take predicted\n",
    "#     classes of all variables, create the adjacency matrix, then create\n",
    "#     the submission in the requested format.\n",
    "#     \"\"\"\n",
    "\n",
    "#     submission = {}\n",
    "#     for name, prediction in tqdm(X_y_pred_test.groupby(\"dataset\"), delay=10):\n",
    "#         variables_labels = prediction[[\"variable\", \"label_predicted\"]].set_index(\"variable\")\n",
    "#         variables = variables_labels.index.tolist()\n",
    "#         variables_all = [\"X\", \"Y\"] + variables\n",
    "\n",
    "#         adjacency_matrix = pd.DataFrame(index=variables_all, columns=variables_all)\n",
    "#         adjacency_matrix.index.name = \"parent\"\n",
    "#         adjacency_matrix[:] = 0\n",
    "#         adjacency_matrix.loc[\"X\", \"Y\"] = 1\n",
    "\n",
    "#         for v in variables:\n",
    "#             l = variables_labels.loc[v].item()\n",
    "#             if l == \"Cause of X\":\n",
    "#                 adjacency_matrix.loc[v, \"X\"] = 1\n",
    "#             elif l == \"Cause of Y\":\n",
    "#                 adjacency_matrix.loc[v, \"Y\"] = 1\n",
    "#             elif l == \"Consequence of X\":\n",
    "#                 adjacency_matrix.loc[\"X\", v] = 1\n",
    "#             elif l == \"Consequence of Y\":\n",
    "#                 adjacency_matrix.loc[\"Y\", v] = 1\n",
    "#             elif l == \"Confounder\":\n",
    "#                 adjacency_matrix.loc[v, \"X\"] = 1\n",
    "#                 adjacency_matrix.loc[v, \"Y\"] = 1\n",
    "#             elif l == \"Collider\":\n",
    "#                 adjacency_matrix.loc[\"X\", v] = 1\n",
    "#                 adjacency_matrix.loc[\"Y\", v] = 1\n",
    "#             elif l == \"Mediator\":\n",
    "#                 adjacency_matrix.loc[\"X\", v] = 1\n",
    "#                 adjacency_matrix.loc[v, \"Y\"] = 1\n",
    "#             elif l == \"Confounder\":\n",
    "#                 pass\n",
    "\n",
    "#         for i in variables_all:\n",
    "#             for j in variables_all:\n",
    "#                 submission[f'{name}_{i}_{j}'] = int(adjacency_matrix.loc[i, j])\n",
    "\n",
    "#     return submission\n",
    "\n",
    "\n",
    "# def infer(\n",
    "#     X_test: typing.Dict[str, pd.DataFrame],\n",
    "#     # number_of_features: int,\n",
    "#     model_directory_path: str,\n",
    "#     id_column_name: str,\n",
    "#     prediction_column_name: str,\n",
    "#     # has_gpu: bool,\n",
    "#     # has_trained: bool,\n",
    "# ) -> pd.DataFrame:\n",
    "#     model = joblib.load(os.path.join(model_directory_path, \"model.joblib\"))\n",
    "\n",
    "#     names_datasets_test = X_test\n",
    "#     X_group_test = create_all_columns(\n",
    "#         {\n",
    "#             PPS_feature: names_datasets_test,\n",
    "#             pearson_correlation: names_datasets_test,\n",
    "#             mutual_information: names_datasets_test,  \n",
    "#             spearman_correlation: names_datasets_test,\n",
    "#             kendall_correlation: names_datasets_test,\n",
    "#             distance_correlation: names_datasets_test,\n",
    "#             conditional_mutual_information: names_datasets_test,\n",
    "#             partial_correlation: names_datasets_test,\n",
    "#             linear_regression_feature: names_datasets_test,\n",
    "#             ridge_regression_feature: names_datasets_test,\n",
    "#             PC_feature: names_datasets_test,\n",
    "#             ExactSearch_feature: names_datasets_test,\n",
    "#             FCI_feature: names_datasets_test,\n",
    "#             DML_feature: names_datasets_test,\n",
    "#             copula_entropy: names_datasets_test,\n",
    "#             piecewise_linear_regression_feature: names_datasets_test,\n",
    "#             },\n",
    "#             n_jobs=-1,\n",
    "#             create_dimension_feature=True,\n",
    "#         )\n",
    "#     X_group_test['MI(v,X)^2'] = X_group_test['MI(v,X)'] ** 2\n",
    "#     X_group_test['MI(v,Y)^2'] = X_group_test['MI(v,Y)'] ** 2\n",
    "#     X_group_test['MI(X,Y)^2'] = X_group_test['MI(X,Y)'] ** 2\n",
    "#     X_group_test['max(MI(v, others))^2'] = X_group_test['max(MI(v, others))'] ** 2\n",
    "#     X_group_test['min(MI(v, others))^2'] = X_group_test['min(MI(v, others))'] ** 2\n",
    "    \n",
    "#     blacklist = [\"ttest(v,X)\", \"pvalue(ttest(v,X))<=0.05\", \"ttest(v,Y)\", \"pvalue(ttest(v,Y))<=0.05\", \"ttest(X,Y)\", \"pvalue(ttest(X,Y))<=0.05\"]\n",
    "#     columns_to_drop = [col for col in blacklist if col in X_group_test.columns]\n",
    "    \n",
    "#     X_group_test = X_group_test.drop(columns=columns_to_drop)\n",
    "    \n",
    "#     numeric_columns = X_group_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "#     X_group_test[numeric_columns] = X_group_test[numeric_columns].fillna(X_group_test[numeric_columns].mean())\n",
    "\n",
    "#     X_test = X_group_test.drop(columns=[\"dataset\", \"variable\"])\n",
    "#     y_predicted = model.predict(X_test)\n",
    "#     X_y_pred_test = X_group_test\n",
    "#     X_y_pred_test[\"y_predicted\"] = y_predicted\n",
    "\n",
    "#     le = LabelEncoder()\n",
    "#     le.classes_ = np.array([\n",
    "#         'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
    "#         'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
    "#     ])\n",
    "\n",
    "#     X_y_pred_test[\"label_predicted\"] = le.inverse_transform(y_predicted)\n",
    "\n",
    "#     submission = create_submission(X_y_pred_test)\n",
    "\n",
    "#     return pd.DataFrame(\n",
    "#         submission.items(),\n",
    "#         columns=[\n",
    "#             id_column_name,\n",
    "#             prediction_column_name\n",
    "#         ]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fceff69-b855-4146-8c57-b32d0ecb666e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672,
     "referenced_widgets": [
      "5d4bf0130ac44e16a1d4370808370166",
      "da29edc9799d4e75aa295b2bd55a1b51",
      "a415eaf9f34a44668631676733ac54e0",
      "d78da2e4bc954c22bea79589fdf9dd48",
      "2d48ee12333d41dfa160bcc8e76d9f60",
      "3199aedf32034fe8b1af3314619b6adc",
      "3b80fc748a7a4e5ab2540c4aee162fbe",
      "f9007189f8be400883429ec748b879e7",
      "d9f86e16a9db4c78b7fa3d5bca8e033f",
      "ce75f0f9d92c4480b9a584319a8d76d8",
      "1b12c1b71ab64191a1ee6508179012e3",
      "477a34c495084301b7a1954ce704aebc",
      "d77bc032f7fb458b8acc60d26b8875b6",
      "26a1e0060aee4841921b8358153e6795",
      "cb0a36f6e0c048759f6f77a6db163e27",
      "08c97be01f87441580c9d6a97a199bc2",
      "af557519c67b413aa9f16a5665054cde",
      "248587de4d264e258321fcca56216cfb",
      "03270cd004de4c50bde33929d94babc4",
      "59200470439b44b1a55723c1786517fe",
      "cf4e1aa461404d2c8f63b25a50ae2513",
      "cfb1d1f914b9415284b39aeacbe632e3"
     ]
    },
    "id": "5fceff69-b855-4146-8c57-b32d0ecb666e",
    "outputId": "78d88eeb-80a6-4872-d674-6e63981f31e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:59:46\u001b[0m \u001b[33mno forbidden library found\u001b[0m\n",
      "\u001b[32m10:59:46\u001b[0m \u001b[33m\u001b[0m\n",
      "\u001b[32m10:59:46\u001b[0m started\n",
      "\u001b[32m10:59:46\u001b[0m running local test\n",
      "\u001b[32m10:59:46\u001b[0m \u001b[33minternet access isn't restricted, no check will be done\u001b[0m\n",
      "\u001b[32m10:59:46\u001b[0m \n",
      "\u001b[32m11:00:00\u001b[0m starting dag process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download data\\X_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_train.pickle (1523944532 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_train.pickle (7017735 bytes)\n",
      "already exists: file length match\n",
      "download data\\X_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_test_reduced.pickle (122341879 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_test_reduced.pickle (562930 bytes)\n",
      "already exists: file length match\n",
      "download data\\example_prediction.parquet from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/example_prediction_reduced.parquet (668981 bytes)\n",
      "already exists: file length match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m11:00:14\u001b[0m \u001b[33mcall: train\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Loading resources\\X_y_group_train_local_test_alldata_earlystop.csv\n",
      "     ->已加载本地数据 X_y_group_train\n",
      "     ->X_y_group_train形状: (501070, 281) \n",
      " X_y_group_train列名: ['variable', 'dimension', 'square_dimension', 'dataset', 'label', 'corr(v,X)', 'corr(v,Y)', 'max(corr(v, others))', 'min(corr(v, others))', 'mean(corr(v, others))', 'std(corr(v, others))', '25%(corr(v, others))', '75%(corr(v, others))', 'corr(X,Y)', 'max(abs_rolling_corr(v, X))', 'min(abs_rolling_corr(v, X))', 'mean(abs_rolling_corr(v, X))', 'max(abs_rolling_corr(v, Y))', 'min(abs_rolling_corr(v, Y))', 'mean(abs_rolling_corr(v, Y))', 'spearman_corr(v,X)', 'spearman_corr(v,Y)', 'max(spearman_corr(v, others))', 'min(spearman_corr(v, others))', 'mean(spearman_corr(v, others))', 'std(spearman_corr(v, others))', 'spearman_corr(X,Y)', 'kendall_corr(v,X)', 'kendall_corr(v,Y)', 'max(kendall_corr(v, others))', 'min(kendall_corr(v, others))', 'mean(kendall_corr(v, others))', 'std(kendall_corr(v, others))', 'kendall_corr(X,Y)', 'MI(v,X)', 'MI(v,Y)', 'max(MI(v, others))', 'min(MI(v, others))', 'mean(MI(v, others))', 'std(MI(v, others))', 'MI(X,Y)', 'conditional_MI(v,X|Y)', 'conditional_MI(v,Y|X)', 'conditional_MI(X,Y|v)', 'dcor(v,X)', 'dcor(v,Y)', 'dcor(v,X)^2', 'dcor(v,Y)^2', 'max(dcor(v, others))', 'min(dcor(v, others))', 'mean(dcor(v, others))', 'std(dcor(v, others))', '25%(dcor(v, others))', '75%(dcor(v, others))', 'dcor(X,Y)', 'dcor(X,Y)^2', 'partial_corr(v,X|Y)', 'partial_corr(v,Y|X)', 'partial_corr(X,Y|v)', 'copula_entropy(v,X)', 'copula_entropy(v,Y)', 'copula_entropy(X,Y)', 'PPS(v,X)', 'PPS(X,v)', 'PPS(v,Y)', 'PPS(Y,v)', 'PPS(X,Y)', 'max(PPS(v,others))', 'mean(PPS(v,others))', 'v~Y_coefficient', 'v_squared~Y_coefficient', 'v*X~Y_coefficient', 'v_cos~Y_coefficient', 'v_sin~Y_coefficient', 'v~X_coefficient', 'v_squared~X_coefficient', 'v_cos~X_coefficient', 'v_sin~X_coefficient', 'X~v_coefficient', 'X_squared~v_coefficient', 'X_cos~v_coefficient', 'X_sin~v_coefficient', 'Y~v_coefficient', 'Y_squared~v_coefficient', 'Y_cos~v_coefficient', 'Y_sin~v_coefficient', 'X*Y~v_coefficient', 'X~Y_coefficient', 'v~Y_ridge_coefficient', 'v~X_ridge_coefficient', 'X~Y_ridge_coefficient', 'ExactSearch(v,X)', 'ExactSearch(X,v)', 'ExactSearch(v,Y)', 'ExactSearch(Y,v)', 'ExactSearch(X,Y)', 'PC(v,X)', 'PC(X,v)', 'PC(v,Y)', 'PC(Y,v)', 'PC(X,Y)', 'FCI(v,X)', 'FCI(X,v)', 'FCI(v,Y)', 'FCI(Y,v)', 'FCI(X,Y)', 'GRaSP(v,X)', 'GRaSP(X,v)', 'GRaSP(v,Y)', 'GRaSP(Y,v)', 'GRaSP(X,Y)', 'v~X_DML_AMTE', 'v~X_DML_AMTE_zstat', 'v~X_DML_AMTE_pvalue', 'v~X_DML_std_point', 'v~X_DML_stderr_point', 'v~Y_DML_AMTE', 'v~Y_DML_AMTE_zstat', 'v~Y_DML_AMTE_pvalue', 'v~Y_DML_std_point', 'v~Y_DML_stderr_point', 'X~v_DML_AMTE', 'X~v_DML_AMTE_zstat', 'X~v_DML_AMTE_pvalue', 'X~v_DML_std_point', 'X~v_DML_stderr_point', 'Y~v_DML_AMTE', 'Y~v_DML_AMTE_zstat', 'Y~v_DML_AMTE_pvalue', 'Y~v_DML_std_point', 'Y~v_DML_stderr_point', 'v~X_piecewise_coef1_x', 'v~X_piecewise_coef2_x', 'v~Y_piecewise_coef1_x', 'v~Y_piecewise_coef2_x', 'X~v_piecewise_coef1_x', 'X~v_piecewise_coef2_x', 'Y~v_piecewise_coef1_x', 'Y~v_piecewise_coef2_x', 'v~X_piecewise_coef1_y', 'v~X_piecewise_coef2_y', 'v~X_piecewise_coef3', 'v~X_piecewise_coef4', 'v~Y_piecewise_coef1_y', 'v~Y_piecewise_coef2_y', 'v~Y_piecewise_coef3', 'v~Y_piecewise_coef4', 'X~v_piecewise_coef1_y', 'X~v_piecewise_coef2_y', 'X~v_piecewise_coef3', 'X~v_piecewise_coef4', 'Y~v_piecewise_coef1_y', 'Y~v_piecewise_coef2_y', 'Y~v_piecewise_coef3', 'Y~v_piecewise_coef4', 'v~X_quadratic_coef1', 'v~X_quadratic_coef2', 'v~X_quadratic_coef3', 'v~X_quadratic_coef4', 'v~Y_quadratic_coef1', 'v~Y_quadratic_coef2', 'v~Y_quadratic_coef3', 'v~Y_quadratic_coef4', 'X~v_quadratic_coef1', 'X~v_quadratic_coef2', 'X~v_quadratic_coef3', 'X~v_quadratic_coef4', 'Y~v_quadratic_coef1', 'Y~v_quadratic_coef2', 'Y~v_quadratic_coef3', 'Y~v_quadratic_coef4', 'v~X_sliding_coef1', 'v~X_sliding_coef2', 'v~X_sliding_coef3', 'v~X_sliding_coef4', 'v~X_sliding_coef5', 'v~Y_sliding_coef1', 'v~Y_sliding_coef2', 'v~Y_sliding_coef3', 'v~Y_sliding_coef4', 'v~Y_sliding_coef5', 'X~v_sliding_coef1', 'X~v_sliding_coef2', 'X~v_sliding_coef3', 'X~v_sliding_coef4', 'X~v_sliding_coef5', 'Y~v_sliding_coef1', 'Y~v_sliding_coef2', 'Y~v_sliding_coef3', 'Y~v_sliding_coef4', 'Y~v_sliding_coef5', 'AIC_Cause of X', 'AIC_Cause of Y', 'AIC_Collider', 'AIC_Confounder', 'AIC_Consequence of X', 'AIC_Consequence of Y', 'AIC_Mediator', 'BIC_Cause of X', 'BIC_Cause of Y', 'BIC_Collider', 'BIC_Confounder', 'BIC_Consequence of X', 'BIC_Consequence of Y', 'BIC_Mediator', 'CFI_Cause of X', 'CFI_Cause of Y', 'CFI_Collider', 'CFI_Confounder', 'CFI_Consequence of X', 'CFI_Consequence of Y', 'CFI_Mediator', 'TLI_Cause of X', 'TLI_Cause of Y', 'TLI_Collider', 'TLI_Confounder', 'TLI_Consequence of X', 'TLI_Consequence of Y', 'TLI_Mediator', 'RMSEA_Cause of X', 'RMSEA_Cause of Y', 'RMSEA_Collider', 'RMSEA_Confounder', 'RMSEA_Consequence of X', 'RMSEA_Consequence of Y', 'RMSEA_Mediator', 'grid(v,X)(0,0)', 'grid(v,X)(0,1)', 'grid(v,X)(0,2)', 'grid(v,X)(0,3)', 'grid(v,X)(0,4)', 'grid(v,X)(1,0)', 'grid(v,X)(1,1)', 'grid(v,X)(1,2)', 'grid(v,X)(1,3)', 'grid(v,X)(1,4)', 'grid(v,X)(2,0)', 'grid(v,X)(2,1)', 'grid(v,X)(2,2)', 'grid(v,X)(2,3)', 'grid(v,X)(2,4)', 'grid(v,X)(3,0)', 'grid(v,X)(3,1)', 'grid(v,X)(3,2)', 'grid(v,X)(3,3)', 'grid(v,X)(3,4)', 'grid(v,X)(4,0)', 'grid(v,X)(4,1)', 'grid(v,X)(4,2)', 'grid(v,X)(4,3)', 'grid(v,X)(4,4)', 'grid(v,Y)(0,0)', 'grid(v,Y)(0,1)', 'grid(v,Y)(0,2)', 'grid(v,Y)(0,3)', 'grid(v,Y)(0,4)', 'grid(v,Y)(1,0)', 'grid(v,Y)(1,1)', 'grid(v,Y)(1,2)', 'grid(v,Y)(1,3)', 'grid(v,Y)(1,4)', 'grid(v,Y)(2,0)', 'grid(v,Y)(2,1)', 'grid(v,Y)(2,2)', 'grid(v,Y)(2,3)', 'grid(v,Y)(2,4)', 'grid(v,Y)(3,0)', 'grid(v,Y)(3,1)', 'grid(v,Y)(3,2)', 'grid(v,Y)(3,3)', 'grid(v,Y)(3,4)', 'grid(v,Y)(4,0)', 'grid(v,Y)(4,1)', 'grid(v,Y)(4,2)', 'grid(v,Y)(4,3)', 'grid(v,Y)(4,4)', 'MI(v,X)^2', 'MI(v,Y)^2', 'MI(X,Y)^2', 'max(MI(v, others))^2', 'min(MI(v, others))^2']\n",
      "##### Checking More Data in Online X_train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da51965ede004dc5b98419ee0121463c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking:   0%|          | 0/23500 [00:00<?, ?dataset/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ->Key not found in local dataset: 00048\n",
      "     ->Key not found in local dataset: 00086\n",
      "     ->Key not found in local dataset: 00402\n",
      "     ->Key not found in local dataset: 01294\n",
      "     ->Key not found in local dataset: 01707\n",
      "     ->Key not found in local dataset: 02219\n",
      "     ->Key not found in local dataset: 02488\n",
      "     ->Key not found in local dataset: 02803\n",
      "     ->Key not found in local dataset: 03439\n",
      "     ->Key not found in local dataset: 04103\n",
      "     ->Key not found in local dataset: 04588\n",
      "     ->Key not found in local dataset: 05260\n",
      "     ->Key not found in local dataset: 05427\n",
      "     ->Key not found in local dataset: 05451\n",
      "     ->Key not found in local dataset: 06654\n",
      "     ->Key not found in local dataset: 06902\n",
      "     ->Key not found in local dataset: 07218\n",
      "     ->Key not found in local dataset: 07292\n",
      "     ->Key not found in local dataset: 07343\n",
      "     ->Key not found in local dataset: 07373\n",
      "     ->Key not found in local dataset: 07672\n",
      "     ->Key not found in local dataset: 08099\n",
      "     ->Key not found in local dataset: 08127\n",
      "     ->Key not found in local dataset: 08416\n",
      "     ->Key not found in local dataset: 08494\n",
      "     ->Key not found in local dataset: 08757\n",
      "     ->Key not found in local dataset: 09021\n",
      "     ->Key not found in local dataset: 09461\n",
      "     ->Key not found in local dataset: 09823\n",
      "     ->Key not found in local dataset: 09951\n",
      "     ->Key not found in local dataset: 09991\n",
      "     ->Key not found in local dataset: 10079\n",
      "     ->Key not found in local dataset: 10252\n",
      "     ->Key not found in local dataset: 10305\n",
      "     ->Key not found in local dataset: 10451\n",
      "     ->Key not found in local dataset: 10474\n",
      "     ->Key not found in local dataset: 10617\n",
      "     ->Key not found in local dataset: 11345\n",
      "     ->Key not found in local dataset: 11359\n",
      "     ->Key not found in local dataset: 11459\n",
      "     ->Key not found in local dataset: 11461\n",
      "     ->Key not found in local dataset: 12006\n",
      "     ->Key not found in local dataset: 12378\n",
      "     ->Key not found in local dataset: 12569\n",
      "     ->Key not found in local dataset: 12580\n",
      "     ->Key not found in local dataset: 12683\n",
      "     ->Key not found in local dataset: 12974\n",
      "     ->Key not found in local dataset: 13827\n",
      "     ->Key not found in local dataset: 13962\n",
      "     ->Key not found in local dataset: 14077\n",
      "     ->Key not found in local dataset: 14559\n",
      "     ->Key not found in local dataset: 14905\n",
      "     ->Key not found in local dataset: 14975\n",
      "     ->Key not found in local dataset: 15021\n",
      "     ->Key not found in local dataset: 15075\n",
      "     ->Key not found in local dataset: 15085\n",
      "     ->Key not found in local dataset: 15964\n",
      "     ->Key not found in local dataset: 16217\n",
      "     ->Key not found in local dataset: 16381\n",
      "     ->Key not found in local dataset: 16482\n",
      "     ->Key not found in local dataset: 16916\n",
      "     ->Key not found in local dataset: 16959\n",
      "     ->Key not found in local dataset: 17696\n",
      "     ->Key not found in local dataset: 17871\n",
      "     ->Key not found in local dataset: 17910\n",
      "     ->Key not found in local dataset: 17960\n",
      "     ->Key not found in local dataset: 18148\n",
      "     ->Key not found in local dataset: 18343\n",
      "     ->Key not found in local dataset: 18452\n",
      "     ->Key not found in local dataset: 18825\n",
      "     ->Key not found in local dataset: 18916\n",
      "     ->Key not found in local dataset: 19238\n",
      "     ->Key not found in local dataset: 19271\n",
      "     ->Key not found in local dataset: 19306\n",
      "     ->Key not found in local dataset: 19696\n",
      "     ->Key not found in local dataset: 20033\n",
      "     ->Key not found in local dataset: 20175\n",
      "     ->Key not found in local dataset: 21109\n",
      "     ->Key not found in local dataset: 21306\n",
      "     ->Key not found in local dataset: 21356\n",
      "     ->Key not found in local dataset: 22035\n",
      "     ->Key not found in local dataset: 22366\n",
      "     ->Key not found in local dataset: 22382\n",
      "     ->Key not found in local dataset: 22612\n",
      "     ->Key not found in local dataset: 22795\n",
      "     ->Key not found in local dataset: 22883\n",
      "     ->Key not found in local dataset: 22913\n",
      "     ->Key not found in local dataset: 23019\n",
      "     ->Key not found in local dataset: 23721\n",
      "     ->Key not found in local dataset: 23750\n",
      "     ->Key not found in local dataset: 24122\n",
      "     ->Key not found in local dataset: 24642\n",
      "     ->Key not found in local dataset: 24675\n",
      "     ->Key not found in local dataset: 24832\n",
      "     ->Key not found in local dataset: 24970\n",
      "     ->Key not found in local dataset: 25106\n",
      "     ->Key not found in local dataset: 25262\n",
      "     ->Key not found in local dataset: 26536\n",
      "     ->Key not found in local dataset: 27052\n",
      "     ->Key not found in local dataset: 27589\n",
      "     ->Key not found in local dataset: 27676\n",
      "     ->Key not found in local dataset: 28612\n",
      "     ->Key not found in local dataset: 28645\n",
      "     ->Key not found in local dataset: 28873\n",
      "     ->Key not found in local dataset: 28930\n",
      "     ->Key not found in local dataset: 29130\n",
      "     ->Key not found in local dataset: 29570\n",
      "     ->Key not found in local dataset: 29624\n",
      "     ->Key not found in local dataset: 29872\n",
      "     ->Key not found in local dataset: 29945\n",
      "     ->Key not found in local dataset: 30388\n",
      "     ->Key not found in local dataset: 30485\n",
      "     ->Key not found in local dataset: 30634\n",
      "     ->Key not found in local dataset: 30699\n",
      "     ->Key not found in local dataset: 30850\n",
      "     ->Key not found in local dataset: 31066\n",
      "     ->Key not found in local dataset: 31259\n",
      "     ->Key not found in local dataset: 31313\n",
      "     ->Key not found in local dataset: 31338\n",
      "     ->Key not found in local dataset: 31555\n",
      "     ->Key not found in local dataset: 31572\n",
      "     ->Key not found in local dataset: 31799\n",
      "     ->Key not found in local dataset: 31832\n",
      "     ->Key not found in local dataset: 31976\n",
      "     ->Key not found in local dataset: 32143\n",
      "     ->Key not found in local dataset: 32650\n",
      "     ->Key not found in local dataset: 32820\n",
      "     ->Key not found in local dataset: 33028\n",
      "     ->Key not found in local dataset: 33392\n",
      "     ->Key not found in local dataset: 33528\n",
      "     ->Key not found in local dataset: 33778\n",
      "     ->Key not found in local dataset: 33884\n",
      "     ->Key not found in local dataset: 34213\n",
      "     ->Key not found in local dataset: 34249\n",
      "     ->Key not found in local dataset: 35019\n",
      "     ->Key not found in local dataset: 35242\n",
      "     ->Key not found in local dataset: 35459\n",
      "     ->Key not found in local dataset: 35558\n",
      "     ->Key not found in local dataset: 35565\n",
      "     ->Key not found in local dataset: 35630\n",
      "     ->Key not found in local dataset: 35754\n",
      "     ->Key not found in local dataset: 35968\n",
      "     ->Key not found in local dataset: 36014\n",
      "     ->Key not found in local dataset: 36081\n",
      "     ->Key not found in local dataset: 36248\n",
      "     ->Key not found in local dataset: 36287\n",
      "     ->Key not found in local dataset: 36451\n",
      "     ->Key not found in local dataset: 36627\n",
      "     ->Key not found in local dataset: 36677\n",
      "     ->Key not found in local dataset: 37419\n",
      "     ->Key not found in local dataset: 37491\n",
      "     ->Key not found in local dataset: 37557\n",
      "     ->Key not found in local dataset: 37589\n",
      "     ->Key not found in local dataset: 37873\n",
      "     ->Key not found in local dataset: 37900\n",
      "     ->Key not found in local dataset: 37931\n",
      "     ->Key not found in local dataset: 38469\n",
      "     ->Key not found in local dataset: 38496\n",
      "     ->Key not found in local dataset: 38513\n",
      "     ->Key not found in local dataset: 38765\n",
      "     ->Key not found in local dataset: 38771\n",
      "     ->Key not found in local dataset: 38792\n",
      "     ->Key not found in local dataset: 38826\n",
      "     ->Key not found in local dataset: 39014\n",
      "     ->Key not found in local dataset: 39120\n",
      "     ->Key not found in local dataset: 39447\n",
      "     ->Key not found in local dataset: 39448\n",
      "     ->Key not found in local dataset: 39622\n",
      "     ->Key not found in local dataset: 39830\n",
      "     ->Key not found in local dataset: 39875\n",
      "     ->Key not found in local dataset: 40026\n",
      "     ->Key not found in local dataset: 40106\n",
      "     ->Key not found in local dataset: 40402\n",
      "     ->Key not found in local dataset: 40546\n",
      "     ->Key not found in local dataset: 40973\n",
      "     ->Key not found in local dataset: 41258\n",
      "     ->Key not found in local dataset: 41709\n",
      "     ->Key not found in local dataset: 41735\n",
      "     ->Key not found in local dataset: 42124\n",
      "     ->Key not found in local dataset: 42358\n",
      "     ->Key not found in local dataset: 42639\n",
      "     ->Key not found in local dataset: 43066\n",
      "     ->Key not found in local dataset: 43129\n",
      "     ->Key not found in local dataset: 43379\n",
      "     ->Key not found in local dataset: 43652\n",
      "     ->Key not found in local dataset: 43816\n",
      "     ->Key not found in local dataset: 44292\n",
      "     ->Key not found in local dataset: 45275\n",
      "     ->Key not found in local dataset: 45292\n",
      "     ->Key not found in local dataset: 45582\n",
      "     ->Key not found in local dataset: 45775\n",
      "     ->Key not found in local dataset: 46181\n",
      "     ->Key not found in local dataset: 46284\n",
      "     ->Key not found in local dataset: 46399\n",
      "     ->Key not found in local dataset: 46500\n",
      "     ->Key not found in local dataset: 46605\n",
      "     ->Key not found in local dataset: 46663\n",
      "     ->Key not found in local dataset: 46773\n",
      "     ->Key not found in local dataset: 46792\n",
      "     ->Key not found in local dataset: 46978\n",
      "     ->Statistics:\n",
      "     ->Number of keys not found in the local dataset: 200\n",
      "##### Creating online_add_X_y_group_train\n",
      "42639 的形状: (1000, 7)\n",
      "30699 的形状: (1000, 6)\n",
      "45275 的形状: (1000, 9)\n",
      "29570 的形状: (1000, 8)\n",
      "33392 的形状: (1000, 8)\n",
      "03439 的形状: (1000, 6)\n",
      "31066 的形状: (1000, 10)\n",
      "29872 的形状: (1000, 5)\n",
      "02219 的形状: (1000, 8)\n",
      "13827 的形状: (1000, 10)\n",
      "18825 的形状: (1000, 6)\n",
      "05451 的形状: (1000, 10)\n",
      "15085 的形状: (1000, 10)\n",
      "13962 的形状: (1000, 7)\n",
      "10474 的形状: (1000, 9)\n",
      "39014 的形状: (1000, 4)\n",
      "45775 的形状: (1000, 9)\n",
      "31832 的形状: (1000, 10)\n",
      "40026 的形状: (1000, 6)\n",
      "19271 的形状: (1000, 3)\n",
      "     ->数据增强失败次数: {3: 1, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0}\n",
      "42639 的形状: (1000, 7)\n",
      "30699 的形状: (1000, 6)\n",
      "45275 的形状: (1000, 9)\n",
      "29570 的形状: (1000, 8)\n",
      "33392 的形状: (1000, 8)\n",
      "03439 的形状: (1000, 6)\n",
      "31066 的形状: (1000, 10)\n",
      "29872 的形状: (1000, 5)\n",
      "02219 的形状: (1000, 8)\n",
      "13827 的形状: (1000, 10)\n",
      "18825 的形状: (1000, 6)\n",
      "05451 的形状: (1000, 10)\n",
      "15085 的形状: (1000, 10)\n",
      "13962 的形状: (1000, 7)\n",
      "10474 的形状: (1000, 9)\n",
      "39014 的形状: (1000, 4)\n",
      "45775 的形状: (1000, 9)\n",
      "31832 的形状: (1000, 10)\n",
      "40026 的形状: (1000, 6)\n",
      "19271 的形状: (1000, 3)\n",
      "042639 的形状: (1000, 7)\n",
      "142639 的形状: (1000, 7)\n",
      "242639 的形状: (1000, 7)\n",
      "342639 的形状: (1000, 7)\n",
      "030699 的形状: (1000, 6)\n",
      "130699 的形状: (1000, 6)\n",
      "230699 的形状: (1000, 6)\n",
      "045275 的形状: (1000, 9)\n",
      "145275 的形状: (1000, 9)\n",
      "245275 的形状: (1000, 9)\n",
      "345275 的形状: (1000, 9)\n",
      "029570 的形状: (1000, 8)\n",
      "129570 的形状: (1000, 8)\n",
      "229570 的形状: (1000, 8)\n",
      "329570 的形状: (1000, 8)\n",
      "033392 的形状: (1000, 8)\n",
      "133392 的形状: (1000, 8)\n",
      "233392 的形状: (1000, 8)\n",
      "333392 的形状: (1000, 8)\n",
      "003439 的形状: (1000, 6)\n",
      "103439 的形状: (1000, 6)\n",
      "203439 的形状: (1000, 6)\n",
      "031066 的形状: (1000, 10)\n",
      "131066 的形状: (1000, 10)\n",
      "231066 的形状: (1000, 10)\n",
      "331066 的形状: (1000, 10)\n",
      "029872 的形状: (1000, 5)\n",
      "129872 的形状: (1000, 5)\n",
      "229872 的形状: (1000, 5)\n",
      "002219 的形状: (1000, 8)\n",
      "102219 的形状: (1000, 8)\n",
      "202219 的形状: (1000, 8)\n",
      "302219 的形状: (1000, 8)\n",
      "013827 的形状: (1000, 10)\n",
      "113827 的形状: (1000, 10)\n",
      "213827 的形状: (1000, 10)\n",
      "313827 的形状: (1000, 10)\n",
      "018825 的形状: (1000, 6)\n",
      "118825 的形状: (1000, 6)\n",
      "218825 的形状: (1000, 6)\n",
      "318825 的形状: (1000, 6)\n",
      "005451 的形状: (1000, 10)\n",
      "105451 的形状: (1000, 10)\n",
      "205451 的形状: (1000, 10)\n",
      "015085 的形状: (1000, 10)\n",
      "115085 的形状: (1000, 10)\n",
      "215085 的形状: (1000, 10)\n",
      "013962 的形状: (1000, 7)\n",
      "113962 的形状: (1000, 7)\n",
      "213962 的形状: (1000, 7)\n",
      "313962 的形状: (1000, 7)\n",
      "010474 的形状: (1000, 9)\n",
      "110474 的形状: (1000, 9)\n",
      "210474 的形状: (1000, 9)\n",
      "039014 的形状: (1000, 4)\n",
      "139014 的形状: (1000, 4)\n",
      "239014 的形状: (1000, 4)\n",
      "045775 的形状: (1000, 9)\n",
      "145775 的形状: (1000, 9)\n",
      "245775 的形状: (1000, 9)\n",
      "345775 的形状: (1000, 9)\n",
      "031832 的形状: (1000, 10)\n",
      "131832 的形状: (1000, 10)\n",
      "231832 的形状: (1000, 10)\n",
      "331832 的形状: (1000, 10)\n",
      "040026 的形状: (1000, 6)\n",
      "140026 的形状: (1000, 6)\n",
      "240026 的形状: (1000, 6)\n",
      "340026 的形状: (1000, 6)\n",
      "019271 的形状: (1000, 3)\n",
      "119271 的形状: (1000, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7ae41b7c3a43b99c7cc8d7999233ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: label\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72efaeb71a094dbd87190b5069a987ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: pearson_correlation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb50346760c1430bbdd417bea5d9bca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: rolling_abs_pearson_correlation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7711b117bf114dbd8541acc923f76d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: spearman_correlation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9bbbf1e5db4da3bb50a08ab33f088a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: kendall_correlation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a76dfb45e1547bea04540c5c3816aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: mutual_information\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d198356a49704d049ab2a237364d413d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: conditional_mutual_information\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67813cb18664eb39a1c04834a34af75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: distance_correlation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564f44b9382f4c77aa471139cc337cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: partial_correlation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7532c4590f3e43b3b7d90386d02089d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: copula_entropy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b105c0ae9e124ba78501916f37643f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: PPS_feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbb76f9e5fc417f95193ae479cd76d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: linear_regression_feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73dda2b659cc4aa790648ec492b290dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: ridge_regression_feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ed09b7d3874709bb776126f063bbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: ExactSearch_feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bb656be11d453eb318501e1ecc9ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: PC_feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b836d43285f942ed93ff193a429bf2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: FCI_feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea3f2d8b37c44d4af464cf79e65244d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: GRaSP_feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b3b9bc85e345009e89e71583c40d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: DML_feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd74a2b9236f4a19b07725016ca3e2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: piecewise_linear_regression_feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b7b9211950484c811552ae45620c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: piecewise_quadratic_regression_feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ff829a196f461194a062f3b0b4f82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: piecewise_quadratic_regression_feature_4_improved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b34f61e4b2d4396bd41a6887756fd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: sliding_window_linear_regression_feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75e3b27ad0c4390919c129f24381da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: sem_features\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b694e88ec3444f49c5e9c267986e9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: grid_feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b961b65cf9644d894c903f9960b1c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ->已生成线上补充数据 online_add_X_y_group_train\n",
      "##### Checking different columns between online_add_X_y_group_train and X_y_group_train\n",
      "     ->online_add_X_y_group_train的列名和X_y_group_train的列名相同\n",
      "     ->拼接前样本量: (501070, 281), (517, 281)\n",
      "     ->拼接后样本量: (501587, 281)\n",
      "##### Adding numeric labels y\n",
      "##### Data Preprocessing...\n",
      "     1.删除多余列后样本量: (501587, 276)\n",
      "     2.填充缺失值后样本量: (501587, 276)\n",
      "     3.清理特征名称后样本量: (501587, 276)\n",
      "##### Extracting X_train, y_train\n",
      "     ->->->处理类别特征: dimension，唯一值数量: 8\n",
      "     ->->->处理类别特征: ExactSearch_v_X，唯一值数量: 2\n",
      "     ->->->处理类别特征: ExactSearch_X_v，唯一值数量: 2\n",
      "     ->->->处理类别特征: ExactSearch_v_Y，唯一值数量: 2\n",
      "     ->->->处理类别特征: ExactSearch_Y_v，唯一值数量: 2\n",
      "     ->->->处理类别特征: ExactSearch_X_Y，唯一值数量: 2\n",
      "     ->->->处理类别特征: PC_v_X，唯一值数量: 2\n",
      "     ->->->处理类别特征: PC_X_v，唯一值数量: 2\n",
      "     ->->->处理类别特征: PC_v_Y，唯一值数量: 2\n",
      "     ->->->处理类别特征: PC_Y_v，唯一值数量: 2\n",
      "     ->->->处理类别特征: PC_X_Y，唯一值数量: 2\n",
      "     ->->->处理类别特征: FCI_v_X，唯一值数量: 4\n",
      "     ->->->处理类别特征: FCI_X_v，唯一值数量: 4\n",
      "     ->->->处理类别特征: FCI_v_Y，唯一值数量: 4\n",
      "     ->->->处理类别特征: FCI_Y_v，唯一值数量: 4\n",
      "     ->->->处理类别特征: FCI_X_Y，唯一值数量: 4\n",
      "     ->->->处理类别特征: GRaSP_v_X，唯一值数量: 3\n",
      "     ->->->处理类别特征: GRaSP_X_v，唯一值数量: 3\n",
      "     ->->->处理类别特征: GRaSP_v_Y，唯一值数量: 3\n",
      "     ->->->处理类别特征: GRaSP_Y_v，唯一值数量: 3\n",
      "     ->->->处理类别特征: GRaSP_X_Y，唯一值数量: 3\n",
      "     ->类别特征索引 (cat_idxs): [0, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]\n",
      "     ->类别特征模态数 (cat_dims): [8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3]\n",
      "     4.处理类别特征后样本量: (501587, 272), (501587,)\n",
      "##### Extracting X_train, y_train, and group\n",
      "     5.分割数据集后样本量: (451428, 272), (50159, 272), (451428,), (50159,)\n",
      "     ->y_train 唯一值: [0 1 2 3 4 5 6 7]\n",
      "     ->y_test 唯一值: [0 1 2 3 4 5 6 7]\n",
      "##### Computing class weights\n",
      "     ->分类目标权重列表: [3.0195044948630136, 3.2795827037080088, 3.1425985742927156, 0.29604010261737257, 1.8058275729646698, 0.8411367498434845, 0.7286832216325108, 1.8089536449317176]\n",
      "     ->分类目标权重字典: {0: 3.0195044948630136, 1: 3.2795827037080088, 2: 3.1425985742927156, 3: 0.29604010261737257, 4: 1.8058275729646698, 5: 0.8411367498434845, 6: 0.7286832216325108, 7: 1.8089536449317176}\n",
      "##### Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m11:04:38\u001b[0m \u001b[33mduration - time=00:04:51\u001b[0m\n",
      "\u001b[32m11:04:38\u001b[0m \u001b[33mmemory - before=\"2.31 GB\" after=\"3.46 GB\" consumed=\"1.15 GB\"\u001b[0m\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[11:04:38] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:861: Exception in gpu_hist: [11:04:38] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:867: Check failed: ctx_->Ordinal() >= 0 (-1 vs. 0) : Must have at least one device\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcrunch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_determinism_check\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload this notebook and submit it to the platform: https://hub.crunchdao.com/competitions/causality-discovery/submit/via/notebook\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\crunch\\inline.py:69\u001b[0m, in \u001b[0;36m_Inline.test\u001b[1;34m(self, force_first_train, train_frequency, raise_abort, round_number, no_checks, no_determinism_check, read_kwargs, write_kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     library\u001b[38;5;241m.\u001b[39mscan(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule)\n\u001b[0;32m     67\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_first_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mround_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompetition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mno_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mno_determinism_check\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m click\u001b[38;5;241m.\u001b[39mAbort \u001b[38;5;28;01mas\u001b[39;00m abort:\n\u001b[0;32m     83\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAborted!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\crunch\\tester.py:52\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(module, model_directory_path, force_first_train, train_frequency, round_number, competition_format, has_gpu, checks, determinism_check_enabled, read_kwargs, write_kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocalRunner\n\u001b[0;32m     38\u001b[0m runner \u001b[38;5;241m=\u001b[39m LocalRunner(\n\u001b[0;32m     39\u001b[0m     module,\n\u001b[0;32m     40\u001b[0m     model_directory_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     write_kwargs,\n\u001b[0;32m     50\u001b[0m )\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\crunch\\runner\\local.py:47\u001b[0m, in \u001b[0;36mLocalRunner.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration - time=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     51\u001b[0m         time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mgmtime(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start))\n\u001b[0;32m     52\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\crunch\\runner\\runner.py:37\u001b[0m, in \u001b[0;36mRunner.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompetition_format \u001b[38;5;241m==\u001b[39m api\u001b[38;5;241m.\u001b[39mCompetitionFormat\u001b[38;5;241m.\u001b[39mDAG:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting dag process...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_dag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompetition_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\crunch\\runner\\runner.py:90\u001b[0m, in \u001b[0;36mRunner.start_dag\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_dag\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 90\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdag_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic:\n\u001b[0;32m     93\u001b[0m         prediction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdag_loop(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\crunch\\runner\\local.py:197\u001b[0m, in \u001b[0;36mLocalRunner.dag_loop\u001b[1;34m(self, train)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[0;32m    196\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall: train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 197\u001b[0m     \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mY_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall: infer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\crunch\\utils.py:231\u001b[0m, in \u001b[0;36msmart_call\u001b[1;34m(function, default_values, specific_values, log)\u001b[0m\n\u001b[0;32m    228\u001b[0m     debug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    229\u001b[0m     arguments[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 274\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X_train, y_train, model_directory_path)\u001b[0m\n\u001b[0;32m    249\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mcallback\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m    250\u001b[0m     rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, save_best\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, maximize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, metric_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    251\u001b[0m )\n\u001b[0;32m    252\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[0;32m    253\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,\n\u001b[0;32m    254\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stop]\n\u001b[0;32m    273\u001b[0m )\n\u001b[1;32m--> 274\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[0;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m pp \u001b[38;5;241m=\u001b[39m pprint\u001b[38;5;241m.\u001b[39mPrettyPrinter(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    281\u001b[0m params_str \u001b[38;5;241m=\u001b[39m pp\u001b[38;5;241m.\u001b[39mpformat(xgb_model\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\xgboost\\sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1529\u001b[0m )\n\u001b[1;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\xgboost\\core.py:2100\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2100\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\xgboost\\core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [11:04:38] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:861: Exception in gpu_hist: [11:04:38] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:867: Check failed: ctx_->Ordinal() >= 0 (-1 vs. 0) : Must have at least one device\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "crunch.test(\n",
    "    no_determinism_check=True\n",
    ")\n",
    "\n",
    "print(\"Download this notebook and submit it to the platform: https://hub.crunchdao.com/competitions/causality-discovery/submit/via/notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4016510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adia_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03270cd004de4c50bde33929d94babc4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08c97be01f87441580c9d6a97a199bc2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b12c1b71ab64191a1ee6508179012e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "248587de4d264e258321fcca56216cfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "26a1e0060aee4841921b8358153e6795": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03270cd004de4c50bde33929d94babc4",
      "max": 1880,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_59200470439b44b1a55723c1786517fe",
      "value": 1880
     }
    },
    "2d48ee12333d41dfa160bcc8e76d9f60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3199aedf32034fe8b1af3314619b6adc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b80fc748a7a4e5ab2540c4aee162fbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "477a34c495084301b7a1954ce704aebc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d77bc032f7fb458b8acc60d26b8875b6",
       "IPY_MODEL_26a1e0060aee4841921b8358153e6795",
       "IPY_MODEL_cb0a36f6e0c048759f6f77a6db163e27"
      ],
      "layout": "IPY_MODEL_08c97be01f87441580c9d6a97a199bc2"
     }
    },
    "59200470439b44b1a55723c1786517fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d4bf0130ac44e16a1d4370808370166": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da29edc9799d4e75aa295b2bd55a1b51",
       "IPY_MODEL_a415eaf9f34a44668631676733ac54e0",
       "IPY_MODEL_d78da2e4bc954c22bea79589fdf9dd48"
      ],
      "layout": "IPY_MODEL_2d48ee12333d41dfa160bcc8e76d9f60"
     }
    },
    "a415eaf9f34a44668631676733ac54e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9007189f8be400883429ec748b879e7",
      "max": 1880,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d9f86e16a9db4c78b7fa3d5bca8e033f",
      "value": 1880
     }
    },
    "af557519c67b413aa9f16a5665054cde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb0a36f6e0c048759f6f77a6db163e27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf4e1aa461404d2c8f63b25a50ae2513",
      "placeholder": "​",
      "style": "IPY_MODEL_cfb1d1f914b9415284b39aeacbe632e3",
      "value": " 1880/1880 [00:10&lt;00:00, 206.39it/s]"
     }
    },
    "ce75f0f9d92c4480b9a584319a8d76d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf4e1aa461404d2c8f63b25a50ae2513": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfb1d1f914b9415284b39aeacbe632e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d77bc032f7fb458b8acc60d26b8875b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af557519c67b413aa9f16a5665054cde",
      "placeholder": "​",
      "style": "IPY_MODEL_248587de4d264e258321fcca56216cfb",
      "value": "100%"
     }
    },
    "d78da2e4bc954c22bea79589fdf9dd48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce75f0f9d92c4480b9a584319a8d76d8",
      "placeholder": "​",
      "style": "IPY_MODEL_1b12c1b71ab64191a1ee6508179012e3",
      "value": " 1880/1880 [00:46&lt;00:00, 47.91it/s]"
     }
    },
    "d9f86e16a9db4c78b7fa3d5bca8e033f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "da29edc9799d4e75aa295b2bd55a1b51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3199aedf32034fe8b1af3314619b6adc",
      "placeholder": "​",
      "style": "IPY_MODEL_3b80fc748a7a4e5ab2540c4aee162fbe",
      "value": "100%"
     }
    },
    "f9007189f8be400883429ec748b879e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
