{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3f6e3dd2-888c-4541-b02f-d3caaae44ede",
      "metadata": {
        "id": "3f6e3dd2-888c-4541-b02f-d3caaae44ede"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import os\n",
        "import typing\n",
        "import random\n",
        "import joblib\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dcor\n",
        "from scipy.stats import pearsonr, ttest_rel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.metrics import balanced_accuracy_score, silhouette_score\n",
        "from sklearn.model_selection import GridSearchCV, GroupKFold, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, LinearRegression, LassoCV\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from scipy.special import softmax\n",
        "from scipy.stats import chi2_contingency, ks_2samp\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from tqdm.auto import tqdm\n",
        "from scipy import stats\n",
        "from scipy.stats import entropy\n",
        "import statsmodels.api as sm\n",
        "import pingouin as pg\n",
        "from pgmpy.estimators import CITests\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from econml.dml import CausalForestDML, DML\n",
        "import lightgbm as lgb\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.exceptions import NotFittedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "98414dd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98414dd1",
        "outputId": "d805a278-6422-4544-cce9-5060ad26d85a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded inline runner with module: <module '__main__'>\n"
          ]
        }
      ],
      "source": [
        "import crunch\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1cd05a2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def graph_nodes_representation(graph, nodelist):\n",
        "    \"\"\"\n",
        "    Create an alternative representation of a graph which is hashable\n",
        "    and equivalent graphs have the same hash.\n",
        "\n",
        "    Python cannot PROPERLY use nx.Graph/DiGraph as key for\n",
        "    dictionaries, because two equivalent graphs with just different\n",
        "    order of the nodes would result in different keys. This is\n",
        "    undesirable here.\n",
        "\n",
        "    So here we transform the graph into an equivalent form that is\n",
        "    based on a specific nodelist and that is hashable. In this way,\n",
        "    two equivalent graphs, once transformed, will result in identical\n",
        "    keys.\n",
        "\n",
        "    So we use the following trick: extract the adjacency matrix\n",
        "    (with nodes in a fixed order) and then make a hashable thing out\n",
        "    of it, through tuple(array.flatten()):\n",
        "    \"\"\"\n",
        "\n",
        "    # This get the adjacency matrix with nodes in a given order, as\n",
        "    # numpy array (which is not hashable):\n",
        "    adjacency_matrix = nx.adjacency_matrix(graph, nodelist=nodelist).todense()\n",
        "\n",
        "    # This transforms the numpy array into a hashable object:\n",
        "    hashable = tuple(adjacency_matrix.flatten())\n",
        "\n",
        "    return hashable\n",
        "\n",
        "def create_graph_label():\n",
        "    \"\"\"\n",
        "    Create a dictionary from graphs to labels, in two formats.\n",
        "    \"\"\"\n",
        "    graph_label = {\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]):             \"Cause of X\",\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]):             \"Cause of Y\",\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]):             \"Consequence of X\",\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]):             \"Consequence of Y\",\n",
        "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}):                \"Independent\",\n",
        "    }\n",
        "\n",
        "    nodelist = [\"v\", \"X\", \"Y\"]\n",
        "\n",
        "    # This is an equivalent alternative to graph_label but in a form\n",
        "    # for which two equivalent graphs have the same key:\n",
        "    adjacency_label = {\n",
        "        graph_nodes_representation(graph, nodelist): label\n",
        "        for graph, label in graph_label.items()\n",
        "    }\n",
        "\n",
        "    return graph_label, adjacency_label\n",
        "\n",
        "def get_labels(adjacency_matrix, adjacency_label):\n",
        "    \"\"\"\n",
        "    Transform an adjacency_matrix (as pd.DataFrame) into a dictionary of variable:label\n",
        "    \"\"\"\n",
        "\n",
        "    result = {}\n",
        "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
        "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]  # this is not hashable\n",
        "        key = tuple(submatrix.values.flatten())  # this is hashable and a compatible with adjacency_label\n",
        "    \n",
        "        result[variable] = adjacency_label[key]\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cf77b61a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_mapping(df, mapping):\n",
        "    df_new = df.copy()\n",
        "    # 创建临时映射以避免冲突\n",
        "    temp_mapping = {k: f'_temp_{k}' for k in mapping.keys()}\n",
        "    df_new.rename(columns=temp_mapping, inplace=True)\n",
        "    if df_new.shape[0] == df_new.shape[1]:  # 如果是方阵，如标签矩阵\n",
        "        df_new.rename(index=temp_mapping, inplace=True)\n",
        "    # 应用最终映射\n",
        "    final_mapping = {f'_temp_{k}': v for k, v in mapping.items()}\n",
        "    df_new.rename(columns=final_mapping, inplace=True)\n",
        "    if df_new.shape[0] == df_new.shape[1]:\n",
        "        df_new.rename(index=final_mapping, inplace=True)\n",
        "    return df_new\n",
        "\n",
        "def check_duplicate_columns(df):\n",
        "    \"\"\"检查是否存在重复的列名\"\"\"\n",
        "    return df.columns.duplicated().any()\n",
        "\n",
        "def augment_data(X_train, y_train):\n",
        "    new_X_train = X_train.copy()\n",
        "    new_y_train = y_train.copy()\n",
        "    for sample_id in X_train.keys():\n",
        "        X = X_train[sample_id]\n",
        "        y = y_train[sample_id]\n",
        "        variables = list(X.columns)\n",
        "        dim = len(variables)\n",
        "        # 提取因果关系对\n",
        "        edges = []\n",
        "        for u in y.index:\n",
        "            for v in y.columns:\n",
        "                if y.loc[u, v] == 1:\n",
        "                    edges.append((u, v))\n",
        "        # 排除涉及 X 和 Y 的边\n",
        "        edges_no_XY = [(u, v) for (u, v) in edges if u not in ['X', 'Y'] and v not in ['X', 'Y']]\n",
        "        if dim >= 4:\n",
        "            edges_to_use = edges_no_XY\n",
        "            attempts = 0\n",
        "            success = False\n",
        "            while attempts < 3 and not success:\n",
        "                if not edges_to_use:\n",
        "                    break  # 没有合适的边，跳出循环\n",
        "                u, v = random.choice(edges_to_use)\n",
        "                mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
        "                # 应用映射到特征矩阵和标签矩阵\n",
        "                X_new = apply_mapping(X, mapping)\n",
        "                y_new = apply_mapping(y, mapping)\n",
        "                # 检查特征矩阵是否有重复列\n",
        "                if check_duplicate_columns(X_new):\n",
        "                    attempts += 1\n",
        "                    continue  # 重试\n",
        "                else:\n",
        "                    # 没有重复列，存储新的数据\n",
        "                    new_sample_id = '0' + sample_id\n",
        "                    new_X_train[new_sample_id] = X_new\n",
        "                    new_y_train[new_sample_id] = y_new\n",
        "                    success = True\n",
        "            if not success:\n",
        "                # 没有找到合适的映射，复制原始数据\n",
        "                new_sample_id = '0' + sample_id\n",
        "                new_X_train[new_sample_id] = X.copy()\n",
        "                new_y_train[new_sample_id] = y.copy()\n",
        "        else:\n",
        "            # 对于维度较低的数据，允许涉及 X 和 Y 的边\n",
        "            edges_to_use = edges\n",
        "            if not edges_to_use:\n",
        "                # 没有边，复制原始数据\n",
        "                new_sample_id = '0' + sample_id\n",
        "                new_X_train[new_sample_id] = X.copy()\n",
        "                new_y_train[new_sample_id] = y.copy()\n",
        "                continue\n",
        "            u, v = random.choice(edges_to_use)\n",
        "            mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
        "            # 应用映射到特征矩阵和标签矩阵\n",
        "            X_new = apply_mapping(X, mapping)\n",
        "            y_new = apply_mapping(y, mapping)\n",
        "            # 检查特征矩阵是否有重复列\n",
        "            if check_duplicate_columns(X_new):\n",
        "                # 如果有重复列，复制原始数据\n",
        "                new_sample_id = '0' + sample_id\n",
        "                new_X_train[new_sample_id] = X.copy()\n",
        "                new_y_train[new_sample_id] = y.copy()\n",
        "            else:\n",
        "                # 没有重复列，存储新的数据\n",
        "                new_sample_id = '0' + sample_id\n",
        "                new_X_train[new_sample_id] = X_new\n",
        "                new_y_train[new_sample_id] = y_new\n",
        "    return new_X_train, new_y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "08d7d3e2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "download data\\X_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_train.pickle (1523944532 bytes)\n",
            "already exists: file length match\n",
            "download data\\y_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_train.pickle (7017735 bytes)\n",
            "already exists: file length match\n",
            "download data\\X_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_test_reduced.pickle (122341879 bytes)\n",
            "already exists: file length match\n",
            "download data\\y_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_test_reduced.pickle (562930 bytes)\n",
            "already exists: file length match\n",
            "download data\\example_prediction.parquet from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/example_prediction_reduced.parquet (668981 bytes)\n",
            "already exists: file length match\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train, X_test = crunch.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d193daf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pearson_correlation(dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, we compute the correlation-based features for each\n",
        "    varibale, which are the correlation between that variable with X and Y,\n",
        "    as well as summary statistics (max, min, mean, std) of all pairs\n",
        "    of correlations.\n",
        "    \"\"\"\n",
        "\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        tmp = dataset.corr().drop([variable], axis=\"columns\").loc[variable].abs()\n",
        "\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"corr(v,X)\": dataset[[variable, \"X\"]].corr().loc[variable, \"X\"],\n",
        "            \"corr(v,Y)\": dataset[[variable, \"Y\"]].corr().loc[variable, \"Y\"],\n",
        "            \"max(corr(v, others))\": tmp.max(),\n",
        "            \"min(corr(v, others))\": tmp.min(),\n",
        "            \"mean(corr(v, others))\": tmp.mean(),\n",
        "            \"std(corr(v, others))\": tmp.std(),\n",
        "            \"25%(corr(v, others))\": tmp.quantile(0.25), \n",
        "            \"75%(corr(v, others))\": tmp.quantile(0.75), \n",
        "        })\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    df[\"corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr().loc[\"X\", \"Y\"]\n",
        "\n",
        "    # pearsonr is NaN when the variance is 0, so we fill with 0\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def mutual_information(dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, we compute the mutual-information-based features\n",
        "    for each variable, which are the MI between that variable\n",
        "    and X and Y, as well as summary statistics (max, min, mean, std) of\n",
        "    all pairs of MI.\n",
        "    \"\"\"\n",
        "\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        tmp = mutual_info_regression(dataset.drop(columns=[variable]), dataset[variable])\n",
        "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
        "\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"MI(v,X)\": mutual_info_regression(dataset[[variable]], dataset[\"X\"], discrete_features=False)[0],\n",
        "            \"MI(v,Y)\": mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0],\n",
        "            \"max(MI(v, others))\": tmp.max(),\n",
        "            \"min(MI(v, others))\": tmp.min(),\n",
        "            \"mean(MI(v, others))\": tmp.mean(),\n",
        "            \"std(MI(v, others))\": tmp.std(),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    df[\"MI(X,Y)\"] = mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_dimension_feature(dataset):\n",
        "    \"\"\"\n",
        "    Add a dimension feature to the dataset.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "    dimension = len(variables)\n",
        "    square_dimension = dimension * dimension\n",
        "    df = pd.DataFrame({\n",
        "        \"variable\": variables,\n",
        "        \"dimension\": dimension,\n",
        "        \"square_dimension\": square_dimension\n",
        "    })\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    return df\n",
        "\n",
        "def spearman_correlation(dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, we compute the Spearman rank correlation-based features for each\n",
        "    variable, which are the Spearman correlation between that variable with X and Y,\n",
        "    as well as summary statistics (max, min, mean, std) of all pairs of Spearman correlations.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        tmp = dataset.corr(method='spearman').drop([variable], axis=\"columns\").loc[variable].abs()\n",
        "\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"spearman_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='spearman').loc[variable, \"X\"],\n",
        "            \"spearman_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='spearman').loc[variable, \"Y\"],\n",
        "            \"max(spearman_corr(v, others))\": tmp.max(),\n",
        "            \"min(spearman_corr(v, others))\": tmp.min(),\n",
        "            \"mean(spearman_corr(v, others))\": tmp.mean(),\n",
        "            \"std(spearman_corr(v, others))\": tmp.std(),\n",
        "            #TODO 分位数\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    df[\"spearman_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='spearman').loc[\"X\", \"Y\"]\n",
        "\n",
        "    # Spearman correlation is NaN when there are ties in rank, so we fill with 0\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def kendall_correlation(dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, we compute the Kendall's tau correlation-based features for each\n",
        "    variable, which are the Kendall's tau correlation between that variable with X and Y,\n",
        "    as well as summary statistics (max, min, mean, std) of all pairs of Kendall's tau correlations.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        tmp = dataset.corr(method='kendall').drop([variable], axis=\"columns\").loc[variable].abs()\n",
        "\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"kendall_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='kendall').loc[variable, \"X\"],\n",
        "            \"kendall_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='kendall').loc[variable, \"Y\"],\n",
        "            \"max(kendall_corr(v, others))\": tmp.max(),\n",
        "            \"min(kendall_corr(v, others))\": tmp.min(),\n",
        "            \"mean(kendall_corr(v, others))\": tmp.mean(),\n",
        "            \"std(kendall_corr(v, others))\": tmp.std(),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    df[\"kendall_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='kendall').loc[\"X\", \"Y\"]\n",
        "\n",
        "    # Kendall's tau correlation can be NaN in some cases, so we fill with 0\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def distance_correlation(dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, we compute the distance correlation-based features for each\n",
        "    variable, which are the distance correlation between that variable with X and Y,\n",
        "    as well as summary statistics (max, min, mean, std) of all pairs of distance correlations.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        tmp = []\n",
        "        # Compute distance correlation between 'variable' and all other variables (excluding itself)\n",
        "        other_variables = dataset.columns.drop([variable])\n",
        "        for other_var in other_variables:\n",
        "            corr = dcor.distance_correlation(dataset[variable], dataset[other_var])\n",
        "            tmp.append(corr)\n",
        "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
        "\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"dcor(v,X)\": dcor.distance_correlation(dataset[variable], dataset[\"X\"]),\n",
        "            \"dcor(v,Y)\": dcor.distance_correlation(dataset[variable], dataset[\"Y\"]),\n",
        "            \"max(dcor(v, others))\": tmp.max(),\n",
        "            \"min(dcor(v, others))\": tmp.min(),\n",
        "            \"mean(dcor(v, others))\": tmp.mean(),\n",
        "            \"std(dcor(v, others))\": tmp.std(),\n",
        "            \"25%(dcor(v, others))\": tmp.quantile(0.25),# Success\n",
        "            \"75%(dcor(v, others))\": tmp.quantile(0.75),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    df[\"dcor(X,Y)\"] = dcor.distance_correlation(dataset[\"X\"], dataset[\"Y\"])\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def Squared_term(dataset, variables):\n",
        "        for var in variables:\n",
        "            dataset[f'{var}_squared_term'] = dataset[var] ** 2\n",
        "        return dataset\n",
        "    \n",
        "def Interaction_term(dataset, variables):\n",
        "    for i in range(len(variables)):\n",
        "        for j in range(i + 1, len(variables)):\n",
        "            dataset[f'{variables[i]}_{variables[j]}'] = dataset[variables[i]] * dataset[variables[j]]\n",
        "    return dataset\n",
        "\n",
        "def Cos_Sin_term(dataset, variables):\n",
        "    for var in variables:\n",
        "        dataset[f'{var}_cos_term'] = np.cos(dataset[var])\n",
        "        dataset[f'{var}_sin_term'] = np.sin(dataset[var])\n",
        "    return dataset\n",
        "\n",
        "def linear_regression_feature(dataset):\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
        "\n",
        "    # model1: Fit X, v, v^2, v_i*v_j, v_i*X ~ Y\n",
        "    model1_features = [\"X\"] + variables\n",
        "    d1 = Squared_term(dataset[model1_features], model1_features)\n",
        "    d1 = Interaction_term(d1, model1_features)  # 0.4600-0.4648\n",
        "    # d1 = Cos_Sin_term(d1, variables)\n",
        "    model1_features = d1.columns.tolist()\n",
        "    scaler = StandardScaler()\n",
        "    d1_scaled = scaler.fit_transform(d1)\n",
        "    model1 = LinearRegression().fit(d1_scaled, dataset[[\"Y\"]])\n",
        "    model1_coefs = model1.coef_[0].tolist()\n",
        "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
        "    \n",
        "    # model2: Fit v, v^2, cos(v)/sin(v) ~ X\n",
        "    model2_features = variables\n",
        "    d2 = Squared_term(dataset[model2_features], model2_features)\n",
        "    d2 = Interaction_term(d2, model2_features)  # 0.4648-0.4643\n",
        "    d2 = Cos_Sin_term(d2, model2_features)\n",
        "    model2_features = d2.columns.tolist()\n",
        "    scaler = StandardScaler()\n",
        "    d2_scaled = scaler.fit_transform(d2)\n",
        "    model2 = LinearRegression().fit(d2_scaled, dataset[[\"X\"]])\n",
        "    model2_coefs = model2.coef_[0].tolist()\n",
        "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
        "    \n",
        "    df = []\n",
        "    for i, variable in enumerate(variables):\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"v~Y_coefficient\": model1_dict[variable],\n",
        "            \"v_squared~Y_coefficient\": model1_dict[f\"{variable}_squared_term\"],\n",
        "            # \"v_cos~Y_coefficient\": model1_dict[f\"{variable}_cos_term\"],  # 掉分\n",
        "            # \"v_sin~Y_coefficient\": model1_dict[f\"{variable}_sin_term\"],\n",
        "            \"v~X_coefficient\": model2_dict[variable],\n",
        "            \"v_squared~X_coefficient\": model2_dict[f\"{variable}_squared_term\"],\n",
        "            \"v_cos~X_coefficient\": model2_dict[f\"{variable}_cos_term\"], \n",
        "            \"v_sin~X_coefficient\": model2_dict[f\"{variable}_sin_term\"],\n",
        "        })\n",
        "        \n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    df[\"X~Y_coefficient\"] = model1_dict[\"X\"]\n",
        "    \n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def ridge_regression_feature(dataset):\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
        "    # 使用GridSearchCV来选择最佳的alpha值\n",
        "    param_grid = {'alpha': np.logspace(-6, 6, 13)}\n",
        "    \n",
        "    # model1: Fit X, v ~ Y\n",
        "    model1_features = [\"X\"] + variables\n",
        "    d1 = Squared_term(dataset[model1_features], model1_features)\n",
        "    # d1 = Interaction_term(d1, model1_features) # 掉分\n",
        "    model1_features = d1.columns.tolist()\n",
        "    scaler1 = StandardScaler()\n",
        "    d1_scaled = scaler1.fit_transform(d1)\n",
        "    model1 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
        "    model1.fit(d1_scaled, dataset[\"Y\"])\n",
        "    model1_coefs = model1.best_estimator_.coef_.tolist()\n",
        "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
        "    \n",
        "    # model2: Fit v ~ X\n",
        "    model2_features = variables\n",
        "    d2 = Squared_term(dataset[model2_features], model2_features)\n",
        "    d2 = Interaction_term(d2, model2_features)\n",
        "    # d2 = Cos_Sin_term(d2, model2_features)  # 掉分\n",
        "    model2_features = d2.columns.tolist()\n",
        "    scaler2 = StandardScaler()\n",
        "    d2_scaled = scaler2.fit_transform(d2)\n",
        "    model2 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
        "    model2.fit(d2_scaled, dataset[\"X\"])\n",
        "    model2_coefs = model2.best_estimator_.coef_.tolist()\n",
        "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
        "\n",
        "    # # 获取最优的 alpha 值\n",
        "    # best_alpha_model1 = model1.best_params_['alpha']   # 0.4730-0.4727\n",
        "    # best_alpha_model2 = model2.best_params_['alpha']\n",
        "    \n",
        "    df = []\n",
        "    for i, variable in enumerate(variables):\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"v~Y_ridge_coefficient\": model1_dict[variable],\n",
        "            # \"v_squared~Y_ridge_coefficient\": model1_dict[f\"{variable}_squared_term\"],  # 掉分\n",
        "            \"v~X_ridge_coefficient\": model2_dict[variable],\n",
        "            # \"v_squared~X_ridge_coefficient\": model2_dict[f\"{variable}_squared_term\"],  # 掉分\n",
        "            # \"v_cos~X_ridge_coefficient\": model2_dict[f\"{variable}_cos_term\"],  # 掉分\n",
        "            # \"v_sin~X_ridge_coefficient\": model2_dict[f\"{variable}_sin_term\"],  # 掉分\n",
        "            # \"v~Y_ridge_alpha\": best_alpha_model1,\n",
        "            # \"v~X_ridge_alpha\": best_alpha_model2\n",
        "        })\n",
        "        \n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    df[\"X~Y_ridge_coefficient\"] = model1_dict[\"X\"]\n",
        "    \n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def cluster_features(dataset):\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
        "    scaler = StandardScaler()\n",
        "    d_scaled = pd.DataFrame(scaler.fit_transform(dataset), columns=dataset.columns)\n",
        "\n",
        "    results = []\n",
        "    eps_values = [0.3]  # , 0.5, 0.7\n",
        "    \n",
        "    for variable in variables:\n",
        "        cluster_counts = []\n",
        "        noise_counts = []\n",
        "        avg_cluster_sizes = []\n",
        "        density_variations = []\n",
        "        feature_importances = []\n",
        "        silhouette_scores = []\n",
        "        \n",
        "        for eps in eps_values:\n",
        "            cluster_df = d_scaled[[variable, \"X\", \"Y\"]].copy()\n",
        "            dbscan = DBSCAN(eps=eps, min_samples=5)\n",
        "            cluster_df[\"cluster\"] = dbscan.fit_predict(cluster_df)\n",
        "            \n",
        "            # 基本统计\n",
        "            cluster_count = len(set(cluster_df[\"cluster\"])) - (1 if -1 in cluster_df[\"cluster\"] else 0)\n",
        "            noise_count = (cluster_df[\"cluster\"] == -1).sum()\n",
        "            cluster_counts.append(cluster_count)\n",
        "            noise_counts.append(noise_count)\n",
        "            \n",
        "            # 密度分析\n",
        "            cluster_sizes = cluster_df[cluster_df[\"cluster\"] != -1][\"cluster\"].value_counts()\n",
        "            avg_cluster_size = cluster_sizes.mean() if not cluster_sizes.empty else 0\n",
        "            density_variation = cluster_sizes.std() / avg_cluster_size if avg_cluster_size > 0 else 0\n",
        "            avg_cluster_sizes.append(avg_cluster_size)\n",
        "            density_variations.append(density_variation)\n",
        "            \n",
        "            # 特征重要性\n",
        "            if cluster_count > 1:  # 确保有多个簇\n",
        "                feature_importance = abs(np.corrcoef(cluster_df[variable], cluster_df[\"cluster\"]))[0, 1]\n",
        "            else:\n",
        "                feature_importance = 0\n",
        "            feature_importances.append(feature_importance)\n",
        "            \n",
        "            # 轮廓系数\n",
        "            non_noise_mask = cluster_df[\"cluster\"] != -1\n",
        "            if len(set(cluster_df.loc[non_noise_mask, \"cluster\"])) > 1:\n",
        "                sil_score = silhouette_score(cluster_df.loc[non_noise_mask, [variable, \"X\", \"Y\"]], \n",
        "                                             cluster_df.loc[non_noise_mask, \"cluster\"], \n",
        "                                             metric=\"euclidean\")\n",
        "            else:\n",
        "                sil_score = 0\n",
        "            silhouette_scores.append(sil_score)\n",
        "        \n",
        "        result = {\n",
        "            \"variable\": variable\n",
        "        }\n",
        "        for i, eps in enumerate(eps_values):\n",
        "            result.update({\n",
        "                f\"cluster_count_{eps}\": cluster_counts[i],             # 0.4730-0.4736\n",
        "                # f\"noise_count_{eps}\": noise_counts[i],                 # 0.4736-0.4740\n",
        "                # f\"avg_cluster_size_{eps}\": avg_cluster_sizes[i],     # 0.4740-0.4735\n",
        "                # f\"density_variation_{eps}\": density_variations[i],     # 0.4740-0.4741\n",
        "                # f\"feature_importance_{eps}\": feature_importances[i], # 0.4741-0.4736\n",
        "                # f\"silhouette_score_{eps}\": silhouette_scores[i]      # 0.4741-0.4723\n",
        "            })\n",
        "        results.append(result)\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def conditional_independence_tests(dataset):  # 太慢了，得三个小时\n",
        "    \"\"\"\n",
        "    A mixed-data residualization based conditional independence test[1].\n",
        "    Uses XGBoost estimator to compute LS residuals[2], and then does an association test (Pillai’s Trace) on the residuals.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        # v-X\n",
        "        coef1, p_value1 = CITests.ci_pillai(X=variable, Y=\"X\", Z=dataset.columns.drop([\"X\", variable]).tolist(), data=dataset, boolean=False)\n",
        "        # v-Y\n",
        "        coef2, p_value2 = CITests.ci_pillai(X=variable, Y=\"Y\", Z=dataset.columns.drop([\"Y\", variable]).tolist(), data=dataset, boolean=False)\n",
        "        # X-v\n",
        "        coef3, p_value3 = CITests.ci_pillai(X=\"X\", Y=variable, Z=dataset.columns.drop([\"X\", variable]).tolist(), data=dataset, boolean=False)\n",
        "        # Y-v\n",
        "        coef4, p_value4 = CITests.ci_pillai(X=\"Y\", Y=variable, Z=dataset.columns.drop([\"Y\", variable]).tolist(), data=dataset, boolean=False)\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"v~X_ci_pillai_coef\": coef1,\n",
        "            \"v~X_ci_pillai_p_value\": p_value1,\n",
        "            \"v~Y_ci_pillai_coef\": coef2,\n",
        "            \"v~Y_ci_pillai_p_value\": p_value2,\n",
        "            \"X~v_ci_pillai_coef\": coef3,\n",
        "            \"X~v_ci_pillai_p_value\": p_value3,\n",
        "            \"Y~v_ci_pillai_coef\": coef4,\n",
        "            \"Y~v_ci_pillai_p_value\": p_value4\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def dml_estimate(data, Y_var, T_var, X_vars, n_splits=4, use_gpu=False):\n",
        "    \"\"\"\n",
        "    使用双重机器学习估计T对Y的因果效应。\n",
        "    返回：\n",
        "    - result: 包含以下键的字典：\n",
        "        - 'theta': 估计的因果效应。\n",
        "        - 'se': 估计的标准误差。\n",
        "    \"\"\"\n",
        "    # 从DataFrame中提取变量\n",
        "    Y = data[Y_var].values\n",
        "    T = data[T_var].values\n",
        "    X = data[X_vars].values\n",
        "\n",
        "    # 初始化残差\n",
        "    Y_residuals = np.zeros_like(Y)\n",
        "    T_residuals = np.zeros_like(T)\n",
        "\n",
        "    # 设置交叉拟合\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # LightGBM参数\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'learning_rate': 0.1,\n",
        "        'num_leaves': 31,\n",
        "        'verbosity': -1,\n",
        "        'device_type': 'cuda' if use_gpu else 'cpu',\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    # 如果使用GPU，加上其他GPU相关参数\n",
        "    if use_gpu:\n",
        "        # 根据最新的LightGBM文档，这些参数只在特定情况下需要\n",
        "        # 这里只设置device_type为'gpu'，其余参数使用默认值\n",
        "        params['max_bin'] = 63  # 建议在GPU模式下使用较小的max_bin值\n",
        "\n",
        "    # 交叉拟合循环\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        # 将数据拆分为训练集和测试集\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "        T_train, T_test = T[train_index], T[test_index]\n",
        "\n",
        "        # 结果模型\n",
        "        Y_model = LGBMRegressor(**params)\n",
        "        Y_model.fit(X_train, Y_train)\n",
        "        Y_pred = Y_model.predict(X_test)\n",
        "        Y_residuals[test_index] = Y_test - Y_pred\n",
        "\n",
        "        # 处理模型\n",
        "        T_model = LGBMRegressor(**params)\n",
        "        T_model.fit(X_train, T_train)\n",
        "        T_pred = T_model.predict(X_test)\n",
        "        T_residuals[test_index] = T_test - T_pred\n",
        "\n",
        "    # 使用残差进行线性回归估计因果效应\n",
        "    causal_model = LinearRegression(fit_intercept=False)\n",
        "    causal_model.fit(T_residuals.reshape(-1, 1), Y_residuals)\n",
        "    theta = causal_model.coef_[0]\n",
        "\n",
        "    # 计算标准误差\n",
        "    n = len(Y_residuals)\n",
        "    residuals = Y_residuals - theta * T_residuals\n",
        "    sigma2 = np.sum(residuals ** 2) / (n - 1)\n",
        "    T_residuals_variance = np.var(T_residuals, ddof=1)\n",
        "    se = np.sqrt(sigma2 / (n * T_residuals_variance))\n",
        "\n",
        "    # 返回结果\n",
        "    result = {\n",
        "        'theta': theta,\n",
        "        'se': se\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def dml_estimate(data, Y_var, T_var, X_vars, n_splits=4, use_gpu=False):\n",
        "    \"\"\"\n",
        "    使用双重机器学习估计T对Y的因果效应。\n",
        "    返回：\n",
        "    - result: 包含以下键的字典：\n",
        "        - 'theta': 估计的因果效应。\n",
        "        - 'se': 估计的标准误差。\n",
        "    \"\"\"\n",
        "    # 从DataFrame中提取变量\n",
        "    Y = data[Y_var].values\n",
        "    T = data[T_var].values\n",
        "    X = data[X_vars].values\n",
        "\n",
        "    # 初始化残差\n",
        "    Y_residuals = np.zeros_like(Y)\n",
        "    T_residuals = np.zeros_like(T)\n",
        "\n",
        "    # 设置交叉拟合\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # LightGBM参数\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'learning_rate': 0.1,\n",
        "        'num_leaves': 31,\n",
        "        'verbosity': -1,\n",
        "        'device_type': 'gpu' if use_gpu else 'cpu',\n",
        "        'gpu_platform_id': 1,\n",
        "        'gpu_device_id': 0,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    # 如果使用GPU，加上其他GPU相关参数\n",
        "    if use_gpu:\n",
        "        # 根据最新的LightGBM文档，这些参数只在特定情况下需要\n",
        "        # 这里只设置device_type为'gpu'，其余参数使用默认值\n",
        "        params['max_bin'] = 63  # 建议在GPU模式下使用较小的max_bin值\n",
        "\n",
        "    # 交叉拟合循环\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        # 将数据拆分为训练集和测试集\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "        T_train, T_test = T[train_index], T[test_index]\n",
        "\n",
        "        # 结果模型\n",
        "        Y_model = LGBMRegressor(**params)\n",
        "        Y_model.fit(X_train, Y_train)\n",
        "        Y_pred = Y_model.predict(X_test)\n",
        "        Y_residuals[test_index] = Y_test - Y_pred\n",
        "\n",
        "        # 处理模型\n",
        "        T_model = LGBMRegressor(**params)\n",
        "        T_model.fit(X_train, T_train)\n",
        "        T_pred = T_model.predict(X_test)\n",
        "        T_residuals[test_index] = T_test - T_pred\n",
        "\n",
        "    # 使用残差进行线性回归估计因果效应\n",
        "    causal_model = LinearRegression(fit_intercept=False)\n",
        "    causal_model.fit(T_residuals.reshape(-1, 1), Y_residuals)\n",
        "    theta = causal_model.coef_[0]\n",
        "\n",
        "    # 计算标准误差\n",
        "    n = len(Y_residuals)\n",
        "    residuals = Y_residuals - theta * T_residuals\n",
        "    sigma2 = np.sum(residuals ** 2) / (n - 1)\n",
        "    T_residuals_variance = np.var(T_residuals, ddof=1)\n",
        "    se = np.sqrt(sigma2 / (n * T_residuals_variance))\n",
        "\n",
        "    # 返回结果\n",
        "    result = {\n",
        "        'theta': theta,\n",
        "        'se': se\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def double_machine_learning(dataset):\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        # 判断v-X的因果效应，设置variables中的其他v和Y为控制变量\n",
        "        Y_var = \"X\"\n",
        "        T_var = variable\n",
        "        X_vars = [var for var in dataset.columns.tolist() if var not in [Y_var, T_var]]\n",
        "        result = dml_estimate(dataset, Y_var, T_var, X_vars, n_splits=4, use_gpu=False)\n",
        "\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"v~X_DML_theta\": result['theta'],\n",
        "            # \"v~X_DML_se\": result['se']\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def conditional_mutual_information(dataset):\n",
        "    \"\"\"\n",
        "    Calculate conditional mutual information for each variable with X and Y.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "    \n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        # Calculate conditional MI(v, X | Y)\n",
        "        mi_vx_given_y = mutual_info_regression(dataset[[variable, \"Y\"]], dataset[\"X\"], discrete_features=False)[0] - \\\n",
        "                        mutual_info_regression(dataset[[\"Y\"]], dataset[\"X\"], discrete_features=False)[0]\n",
        "        \n",
        "        # Calculate conditional MI(v, Y | X)\n",
        "        mi_vy_given_x = mutual_info_regression(dataset[[variable, \"X\"]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
        "                        mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
        "        \n",
        "        # Calculate conditional MI(X, Y | v)\n",
        "        mi_xy_given_v = mutual_info_regression(dataset[[\"X\", variable]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
        "                        mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0]\n",
        "        \n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"conditional_MI(v,X|Y)\": mi_vx_given_y,\n",
        "            \"conditional_MI(v,Y|X)\": mi_vy_given_x,\n",
        "            \"conditional_MI(X,Y|v)\": mi_xy_given_v,\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "    \n",
        "    return df\n",
        "\n",
        "def partial_correlation(dataset):\n",
        "    \"\"\"\n",
        "    Compute partial correlation coefficients for each variable with X and Y,\n",
        "    controlling for the other variable, as well as the partial correlation\n",
        "    between X and Y controlling for each variable.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "    \n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        # Compute partial correlations\n",
        "        pcorr_vX_Y = pg.partial_corr(data=dataset, x=variable, y='X', covar='Y')['r'].iloc[0]\n",
        "        pcorr_vY_X = pg.partial_corr(data=dataset, x=variable, y='Y', covar='X')['r'].iloc[0]\n",
        "        pcorr_XY_v = pg.partial_corr(data=dataset, x='X', y='Y', covar=variable)['r'].iloc[0]\n",
        "        \n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"partial_corr(v,X|Y)\": pcorr_vX_Y,\n",
        "            \"partial_corr(v,Y|X)\": pcorr_vY_X,\n",
        "            \"partial_corr(X,Y|v)\": pcorr_XY_v,\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "    \n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "086680b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def label(adjacency_matrix):\n",
        "    \"\"\"\n",
        "    Given a graph as adjacency_matrix, create the class labels of each variable.\n",
        "    \"\"\"\n",
        "\n",
        "    adjacency_graph, adjacency_label = create_graph_label()\n",
        "    labels = get_labels(adjacency_matrix, adjacency_label)\n",
        "    variables = adjacency_matrix.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"variable\": variables,\n",
        "        \"label\": [labels[variable] for variable in variables],\n",
        "    })\n",
        "    df[\"dataset\"] = adjacency_matrix.name\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "174d66b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_some_columns(names_datasets, function):\n",
        "    \"\"\"\n",
        "    Apply an embedding function to a list of datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    df = []\n",
        "    for name, dataset in tqdm(names_datasets.items()):\n",
        "        dataset = names_datasets[name]\n",
        "        dataset.name = name\n",
        "    \n",
        "        try:\n",
        "            df_dataset = function(dataset)\n",
        "        except ValueError as e:\n",
        "            print(name, e)\n",
        "            raise NotImplementedError\n",
        "\n",
        "        df_dataset[\"dataset\"] = name\n",
        "        df.append(df_dataset)\n",
        "\n",
        "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d5d95f91",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_some_columns_parallel(names_datasets, function, n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Apply an embedding function to a list of datasets.\n",
        "\n",
        "    Parallel version.\n",
        "    \"\"\"\n",
        "\n",
        "    def f(name, dataset, function):\n",
        "        dataset.name = name\n",
        "        df_dataset = function(dataset)\n",
        "        df_dataset[\"dataset\"] = name\n",
        "        return df_dataset\n",
        "\n",
        "    df = joblib.Parallel(n_jobs=n_jobs)(\n",
        "        joblib.delayed(f)(name, dataset, function)\n",
        "        for name, dataset in tqdm(names_datasets.items())\n",
        "    )\n",
        "\n",
        "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f344d206",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_all_columns(functions_names_datasets, n_jobs=-1, create_dimension_feature = False):\n",
        "    \"\"\"\n",
        "    given a dictionary of {function1:names, function2:names,...} apply\n",
        "    the desired functions to the list of datasets and merge all of them\n",
        "    in a single X_y_group dataframe.\n",
        "    \"\"\"\n",
        "\n",
        "    columns = []\n",
        "    if create_dimension_feature:\n",
        "        dimension_feature = create_some_columns(functions_names_datasets[list(functions_names_datasets.keys())[0]], add_dimension_feature)\n",
        "        columns.append(dimension_feature)\n",
        "    \n",
        "    for function, names_datasets in functions_names_datasets.items():\n",
        "        print(f\"set: {function.__name__}\")\n",
        "\n",
        "        if n_jobs != 1:\n",
        "            feature_set = create_some_columns_parallel(names_datasets, function, n_jobs=n_jobs)\n",
        "        else:\n",
        "            feature_set = create_some_columns(names_datasets, function)\n",
        "\n",
        "        columns.append(feature_set)\n",
        "\n",
        "    # Merge all feature sets into a single dataframe:\n",
        "    columns = functools.reduce(\n",
        "        lambda left, right: pd.merge(left, right, on=[\"dataset\", \"variable\"]),\n",
        "        columns,\n",
        "    )\n",
        "\n",
        "    return columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d2a15476",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment what you need!\n",
        "def train(\n",
        "    X_train: typing.Dict[str, pd.DataFrame],\n",
        "    y_train: typing.Dict[str, pd.DataFrame],\n",
        "    # number_of_features: int,\n",
        "    model_directory_path: str,\n",
        "    # id_column_name: str,\n",
        "    # prediction_column_name: str,\n",
        "    # has_gpu: bool,\n",
        ") -> None:\n",
        "    # 1) Create the variable embedding:\n",
        "    X_y_group_train_pathname = \"/tmp/X_y_group_train.parquet\"\n",
        "    try:\n",
        "        print(f\"Loading {X_y_group_train_pathname}\")\n",
        "        X_y_group_train = pd.read_parquet(X_y_group_train_pathname)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Creating X_y_group_train\")\n",
        "        \n",
        "        X_train, y_train = augment_data(X_train, y_train)\n",
        "        \n",
        "        names_datasets_train = X_train\n",
        "        names_graphs_train = y_train\n",
        "        X_y_group_train = create_all_columns(\n",
        "            {\n",
        "                pearson_correlation: names_datasets_train,\n",
        "                mutual_information: names_datasets_train, \n",
        "                label: names_graphs_train,\n",
        "                spearman_correlation: names_datasets_train,\n",
        "                kendall_correlation: names_datasets_train,\n",
        "                distance_correlation: names_datasets_train,\n",
        "                conditional_mutual_information: names_datasets_train,\n",
        "                partial_correlation: names_datasets_train,\n",
        "                linear_regression_feature: names_datasets_train,\n",
        "                ridge_regression_feature: names_datasets_train,\n",
        "                cluster_features: names_datasets_train,\n",
        "                # conditional_independence_tests: names_datasets_train, \n",
        "                double_machine_learning: names_datasets_train,\n",
        "            },\n",
        "            n_jobs=-1,\n",
        "            create_dimension_feature=True,\n",
        "        )\n",
        "        X_y_group_train['MI(v,X)^2'] = X_y_group_train['MI(v,X)'] ** 2\n",
        "        X_y_group_train['MI(v,Y)^2'] = X_y_group_train['MI(v,Y)'] ** 2\n",
        "        X_y_group_train['MI(X,Y)^2'] = X_y_group_train['MI(X,Y)'] ** 2\n",
        "        X_y_group_train['max(MI(v, others))^2'] = X_y_group_train['max(MI(v, others))'] ** 2\n",
        "        X_y_group_train['min(MI(v, others))^2'] = X_y_group_train['min(MI(v, others))'] ** 2\n",
        "        \n",
        "        blacklist = [\"ttest(v,X)\", \"pvalue(ttest(v,X))<=0.05\", \"ttest(v,Y)\", \"pvalue(ttest(v,Y))<=0.05\", \"ttest(X,Y)\", \"pvalue(ttest(X,Y))<=0.05\"]\n",
        "        columns_to_drop = [col for col in blacklist if col in X_y_group_train.columns]\n",
        "        X_y_group_train = X_y_group_train.drop(columns=columns_to_drop)\n",
        "\n",
        "        # 2) Massage X_y_group_train to prepare what is needed by the model:\n",
        "        print(\"Adding numeric labels y\") # sklearn wants numeric labels\n",
        "        le = LabelEncoder()\n",
        "        le.classes_ = np.array([\n",
        "            'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
        "            'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
        "        ])\n",
        "        X_y_group_train[\"y\"] = le.transform(X_y_group_train[\"label\"])\n",
        "\n",
        "        # reordering columns:\n",
        "        X_y_group_train = X_y_group_train[[\"dataset\", \"variable\"] + X_y_group_train.columns.drop([\"dataset\", \"variable\", \"label\", \"y\"]).tolist() + [\"label\", \"y\"]]\n",
        "        display(X_y_group_train)\n",
        "        numeric_columns = X_y_group_train.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        # 对数值类型的列使用平均值填充\n",
        "        X_y_group_train[numeric_columns] = X_y_group_train[numeric_columns].fillna(X_y_group_train[numeric_columns].mean())\n",
        "        print(\"Saving X_y_group_train\")\n",
        "        os.makedirs(os.path.dirname(X_y_group_train_pathname), exist_ok=True)\n",
        "        X_y_group_train.to_parquet(X_y_group_train_pathname)\n",
        "\n",
        "    print(\"Extracting X_train, y_train, and group\")\n",
        "    X_train = X_y_group_train.drop([\"variable\", \"dataset\", \"label\", \"y\"], axis=\"columns\")\n",
        "    y_train = X_y_group_train[\"y\"]\n",
        "    group_train = X_y_group_train[\"dataset\"]\n",
        "\n",
        "    # Model:\n",
        "    model = RandomForestClassifier(n_estimators=100, max_depth=13, n_jobs=-1, class_weight=\"balanced\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    joblib.dump(\n",
        "        model,\n",
        "        os.path.join(model_directory_path, \"model.joblib\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "221c521a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_submission(X_y_pred_test):\n",
        "    \"\"\"\n",
        "    From the predicted test set, for each dataset, take predicted\n",
        "    classes of all variables, create the adjacency matrix, then create\n",
        "    the submission in the requested format.\n",
        "    \"\"\"\n",
        "\n",
        "    submission = {}\n",
        "    for name, prediction in tqdm(X_y_pred_test.groupby(\"dataset\"), delay=10):\n",
        "        variables_labels = prediction[[\"variable\", \"label_predicted\"]].set_index(\"variable\")\n",
        "        variables = variables_labels.index.tolist()\n",
        "        variables_all = [\"X\", \"Y\"] + variables\n",
        "\n",
        "        adjacency_matrix = pd.DataFrame(index=variables_all, columns=variables_all)\n",
        "        adjacency_matrix.index.name = \"parent\"\n",
        "        adjacency_matrix[:] = 0\n",
        "        adjacency_matrix.loc[\"X\", \"Y\"] = 1\n",
        "\n",
        "        for v in variables:\n",
        "            l = variables_labels.loc[v].item()\n",
        "            if l == \"Cause of X\":\n",
        "                adjacency_matrix.loc[v, \"X\"] = 1\n",
        "            elif l == \"Cause of Y\":\n",
        "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
        "            elif l == \"Consequence of X\":\n",
        "                adjacency_matrix.loc[\"X\", v] = 1\n",
        "            elif l == \"Consequence of Y\":\n",
        "                adjacency_matrix.loc[\"Y\", v] = 1\n",
        "            elif l == \"Confounder\":\n",
        "                adjacency_matrix.loc[v, \"X\"] = 1\n",
        "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
        "            elif l == \"Collider\":\n",
        "                adjacency_matrix.loc[\"X\", v] = 1\n",
        "                adjacency_matrix.loc[\"Y\", v] = 1\n",
        "            elif l == \"Mediator\":\n",
        "                adjacency_matrix.loc[\"X\", v] = 1\n",
        "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
        "            elif l == \"Confounder\":\n",
        "                pass\n",
        "\n",
        "        for i in variables_all:\n",
        "            for j in variables_all:\n",
        "                submission[f'{name}_{i}_{j}'] = int(adjacency_matrix.loc[i, j])\n",
        "\n",
        "    return submission\n",
        "\n",
        "\n",
        "# Uncomment what you need!\n",
        "def infer(\n",
        "    X_test: typing.Dict[str, pd.DataFrame],\n",
        "    # number_of_features: int,\n",
        "    model_directory_path: str,\n",
        "    id_column_name: str,\n",
        "    prediction_column_name: str,\n",
        "    # has_gpu: bool,\n",
        "    # has_trained: bool,\n",
        ") -> pd.DataFrame:\n",
        "    model = joblib.load(os.path.join(model_directory_path, \"model.joblib\"))\n",
        "\n",
        "    names_datasets_test = X_test\n",
        "    X_group_test = create_all_columns(\n",
        "        {\n",
        "            pearson_correlation: names_datasets_test,\n",
        "            mutual_information: names_datasets_test,  \n",
        "            spearman_correlation: names_datasets_test,\n",
        "            kendall_correlation: names_datasets_test,\n",
        "            distance_correlation: names_datasets_test,\n",
        "            conditional_mutual_information: names_datasets_test,\n",
        "            partial_correlation: names_datasets_test,\n",
        "            linear_regression_feature: names_datasets_test,\n",
        "            ridge_regression_feature: names_datasets_test,\n",
        "            cluster_features: names_datasets_test,\n",
        "            # conditional_independence_tests: names_datasets_test,\n",
        "            double_machine_learning: names_datasets_test,\n",
        "            },\n",
        "            n_jobs=-1,\n",
        "            create_dimension_feature=True,\n",
        "        )\n",
        "    X_group_test['MI(v,X)^2'] = X_group_test['MI(v,X)'] ** 2\n",
        "    X_group_test['MI(v,Y)^2'] = X_group_test['MI(v,Y)'] ** 2\n",
        "    X_group_test['MI(X,Y)^2'] = X_group_test['MI(X,Y)'] ** 2\n",
        "    X_group_test['max(MI(v, others))^2'] = X_group_test['max(MI(v, others))'] ** 2\n",
        "    X_group_test['min(MI(v, others))^2'] = X_group_test['min(MI(v, others))'] ** 2\n",
        "    \n",
        "    blacklist = [\"ttest(v,X)\", \"pvalue(ttest(v,X))<=0.05\", \"ttest(v,Y)\", \"pvalue(ttest(v,Y))<=0.05\", \"ttest(X,Y)\", \"pvalue(ttest(X,Y))<=0.05\"]\n",
        "    columns_to_drop = [col for col in blacklist if col in X_group_test.columns]\n",
        "    \n",
        "    X_group_test = X_group_test.drop(columns=columns_to_drop)\n",
        "    \n",
        "    numeric_columns = X_group_test.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    X_group_test[numeric_columns] = X_group_test[numeric_columns].fillna(X_group_test[numeric_columns].mean())\n",
        "\n",
        "    X_test = X_group_test.drop(columns=[\"dataset\", \"variable\"])\n",
        "    y_predicted = model.predict(X_test)\n",
        "    X_y_pred_test = X_group_test\n",
        "    X_y_pred_test[\"y_predicted\"] = y_predicted\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    le.classes_ = np.array([\n",
        "        'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
        "        'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
        "    ])\n",
        "\n",
        "    X_y_pred_test[\"label_predicted\"] = le.inverse_transform(y_predicted)\n",
        "\n",
        "    submission = create_submission(X_y_pred_test)\n",
        "\n",
        "    return pd.DataFrame(\n",
        "        submission.items(),\n",
        "        columns=[\n",
        "            id_column_name,\n",
        "            prediction_column_name\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fceff69-b855-4146-8c57-b32d0ecb666e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672,
          "referenced_widgets": [
            "5d4bf0130ac44e16a1d4370808370166",
            "da29edc9799d4e75aa295b2bd55a1b51",
            "a415eaf9f34a44668631676733ac54e0",
            "d78da2e4bc954c22bea79589fdf9dd48",
            "2d48ee12333d41dfa160bcc8e76d9f60",
            "3199aedf32034fe8b1af3314619b6adc",
            "3b80fc748a7a4e5ab2540c4aee162fbe",
            "f9007189f8be400883429ec748b879e7",
            "d9f86e16a9db4c78b7fa3d5bca8e033f",
            "ce75f0f9d92c4480b9a584319a8d76d8",
            "1b12c1b71ab64191a1ee6508179012e3",
            "477a34c495084301b7a1954ce704aebc",
            "d77bc032f7fb458b8acc60d26b8875b6",
            "26a1e0060aee4841921b8358153e6795",
            "cb0a36f6e0c048759f6f77a6db163e27",
            "08c97be01f87441580c9d6a97a199bc2",
            "af557519c67b413aa9f16a5665054cde",
            "248587de4d264e258321fcca56216cfb",
            "03270cd004de4c50bde33929d94babc4",
            "59200470439b44b1a55723c1786517fe",
            "cf4e1aa461404d2c8f63b25a50ae2513",
            "cfb1d1f914b9415284b39aeacbe632e3"
          ]
        },
        "id": "5fceff69-b855-4146-8c57-b32d0ecb666e",
        "outputId": "78d88eeb-80a6-4872-d674-6e63981f31e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m00:41:46\u001b[0m \u001b[33mno forbidden library found\u001b[0m\n",
            "\u001b[32m00:41:46\u001b[0m \u001b[33m\u001b[0m\n",
            "\u001b[32m00:41:46\u001b[0m started\n",
            "\u001b[32m00:41:46\u001b[0m running local test\n",
            "\u001b[32m00:41:46\u001b[0m \u001b[33minternet access isn't restricted, no check will be done\u001b[0m\n",
            "\u001b[32m00:41:46\u001b[0m \n",
            "\u001b[32m00:41:53\u001b[0m starting dag process...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "download data\\X_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_train.pickle (1523944532 bytes)\n",
            "already exists: file length match\n",
            "download data\\y_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_train.pickle (7017735 bytes)\n",
            "already exists: file length match\n",
            "download data\\X_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_test_reduced.pickle (122341879 bytes)\n",
            "already exists: file length match\n",
            "download data\\y_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_test_reduced.pickle (562930 bytes)\n",
            "already exists: file length match\n",
            "download data\\example_prediction.parquet from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/example_prediction_reduced.parquet (668981 bytes)\n",
            "already exists: file length match\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m00:42:04\u001b[0m \u001b[33mcall: train\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading /tmp/X_y_group_train.parquet\n",
            "Extracting X_train, y_train, and group\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m00:42:12\u001b[0m \u001b[33mcall: infer\u001b[0m\n",
            "100%|██████████| 1880/1880 [00:01<00:00, 1775.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set: pearson_correlation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1880/1880 [00:08<00:00, 228.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set: mutual_information\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m00:43:59\u001b[0m \u001b[33mduration - time=00:02:13\u001b[0m\n",
            "\u001b[32m00:43:59\u001b[0m \u001b[33mmemory - before=\"4.59 GB\" after=\"5.48 GB\" consumed=\"914.05 MB\"\u001b[0m\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcrunch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_determinism_check\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload this notebook and submit it to the platform: https://hub.crunchdao.com/competitions/causality-discovery/submit/via/notebook\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\inline.py:69\u001b[0m, in \u001b[0;36m_Inline.test\u001b[1;34m(self, force_first_train, train_frequency, raise_abort, round_number, no_checks, no_determinism_check, read_kwargs, write_kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     library\u001b[38;5;241m.\u001b[39mscan(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule)\n\u001b[0;32m     67\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_first_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mround_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompetition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mno_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mno_determinism_check\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m click\u001b[38;5;241m.\u001b[39mAbort \u001b[38;5;28;01mas\u001b[39;00m abort:\n\u001b[0;32m     83\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAborted!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\tester.py:52\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(module, model_directory_path, force_first_train, train_frequency, round_number, competition_format, has_gpu, checks, determinism_check_enabled, read_kwargs, write_kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocalRunner\n\u001b[0;32m     38\u001b[0m runner \u001b[38;5;241m=\u001b[39m LocalRunner(\n\u001b[0;32m     39\u001b[0m     module,\n\u001b[0;32m     40\u001b[0m     model_directory_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     write_kwargs,\n\u001b[0;32m     50\u001b[0m )\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\runner\\local.py:47\u001b[0m, in \u001b[0;36mLocalRunner.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration - time=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     51\u001b[0m         time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mgmtime(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start))\n\u001b[0;32m     52\u001b[0m     )\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\runner\\runner.py:37\u001b[0m, in \u001b[0;36mRunner.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompetition_format \u001b[38;5;241m==\u001b[39m api\u001b[38;5;241m.\u001b[39mCompetitionFormat\u001b[38;5;241m.\u001b[39mDAG:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting dag process...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_dag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompetition_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\runner\\runner.py:90\u001b[0m, in \u001b[0;36mRunner.start_dag\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_dag\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 90\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdag_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic:\n\u001b[0;32m     93\u001b[0m         prediction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdag_loop(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\runner\\local.py:206\u001b[0m, in \u001b[0;36mLocalRunner.dag_loop\u001b[1;34m(self, train)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall: infer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 206\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m     ensure\u001b[38;5;241m.\u001b[39mreturn_infer(\n\u001b[0;32m    212\u001b[0m         prediction,\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;241m.\u001b[39moutputs,\n\u001b[0;32m    216\u001b[0m     )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prediction\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\utils.py:231\u001b[0m, in \u001b[0;36msmart_call\u001b[1;34m(function, default_values, specific_values, log)\u001b[0m\n\u001b[0;32m    228\u001b[0m     debug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    229\u001b[0m     arguments[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marguments)\n",
            "Cell \u001b[1;32mIn[23], line 61\u001b[0m, in \u001b[0;36minfer\u001b[1;34m(X_test, model_directory_path, id_column_name, prediction_column_name)\u001b[0m\n\u001b[0;32m     58\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_directory_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     60\u001b[0m names_datasets_test \u001b[38;5;241m=\u001b[39m X_test\n\u001b[1;32m---> 61\u001b[0m X_group_test \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_all_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpearson_correlation\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutual_information\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspearman_correlation\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkendall_correlation\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistance_correlation\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconditional_mutual_information\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial_correlation\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinear_regression_feature\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mridge_regression_feature\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcluster_features\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconditional_independence_tests\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdouble_machine_learning\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_dimension_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m X_group_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMI(v,X)^2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_group_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMI(v,X)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     80\u001b[0m X_group_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMI(v,Y)^2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_group_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMI(v,Y)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
            "Cell \u001b[1;32mIn[18], line 17\u001b[0m, in \u001b[0;36mcreate_all_columns\u001b[1;34m(functions_names_datasets, n_jobs, create_dimension_feature)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 17\u001b[0m     feature_set \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_some_columns_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     feature_set \u001b[38;5;241m=\u001b[39m create_some_columns(names_datasets, function)\n",
            "Cell \u001b[1;32mIn[17], line 14\u001b[0m, in \u001b[0;36mcreate_some_columns_parallel\u001b[1;34m(names_datasets, function, n_jobs)\u001b[0m\n\u001b[0;32m     11\u001b[0m     df_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_dataset\n\u001b[1;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames_datasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(df, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "crunch.test(\n",
        "    no_determinism_check=True\n",
        ")\n",
        "\n",
        "print(\"Download this notebook and submit it to the platform: https://hub.crunchdao.com/competitions/causality-discovery/submit/via/notebook\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4016510",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03270cd004de4c50bde33929d94babc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08c97be01f87441580c9d6a97a199bc2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b12c1b71ab64191a1ee6508179012e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "248587de4d264e258321fcca56216cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26a1e0060aee4841921b8358153e6795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03270cd004de4c50bde33929d94babc4",
            "max": 1880,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59200470439b44b1a55723c1786517fe",
            "value": 1880
          }
        },
        "2d48ee12333d41dfa160bcc8e76d9f60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3199aedf32034fe8b1af3314619b6adc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b80fc748a7a4e5ab2540c4aee162fbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "477a34c495084301b7a1954ce704aebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d77bc032f7fb458b8acc60d26b8875b6",
              "IPY_MODEL_26a1e0060aee4841921b8358153e6795",
              "IPY_MODEL_cb0a36f6e0c048759f6f77a6db163e27"
            ],
            "layout": "IPY_MODEL_08c97be01f87441580c9d6a97a199bc2"
          }
        },
        "59200470439b44b1a55723c1786517fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d4bf0130ac44e16a1d4370808370166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da29edc9799d4e75aa295b2bd55a1b51",
              "IPY_MODEL_a415eaf9f34a44668631676733ac54e0",
              "IPY_MODEL_d78da2e4bc954c22bea79589fdf9dd48"
            ],
            "layout": "IPY_MODEL_2d48ee12333d41dfa160bcc8e76d9f60"
          }
        },
        "a415eaf9f34a44668631676733ac54e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9007189f8be400883429ec748b879e7",
            "max": 1880,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9f86e16a9db4c78b7fa3d5bca8e033f",
            "value": 1880
          }
        },
        "af557519c67b413aa9f16a5665054cde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0a36f6e0c048759f6f77a6db163e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf4e1aa461404d2c8f63b25a50ae2513",
            "placeholder": "​",
            "style": "IPY_MODEL_cfb1d1f914b9415284b39aeacbe632e3",
            "value": " 1880/1880 [00:10&lt;00:00, 206.39it/s]"
          }
        },
        "ce75f0f9d92c4480b9a584319a8d76d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf4e1aa461404d2c8f63b25a50ae2513": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb1d1f914b9415284b39aeacbe632e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d77bc032f7fb458b8acc60d26b8875b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af557519c67b413aa9f16a5665054cde",
            "placeholder": "​",
            "style": "IPY_MODEL_248587de4d264e258321fcca56216cfb",
            "value": "100%"
          }
        },
        "d78da2e4bc954c22bea79589fdf9dd48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce75f0f9d92c4480b9a584319a8d76d8",
            "placeholder": "​",
            "style": "IPY_MODEL_1b12c1b71ab64191a1ee6508179012e3",
            "value": " 1880/1880 [00:46&lt;00:00, 47.91it/s]"
          }
        },
        "d9f86e16a9db4c78b7fa3d5bca8e033f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da29edc9799d4e75aa295b2bd55a1b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3199aedf32034fe8b1af3314619b6adc",
            "placeholder": "​",
            "style": "IPY_MODEL_3b80fc748a7a4e5ab2540c4aee162fbe",
            "value": "100%"
          }
        },
        "f9007189f8be400883429ec748b879e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
