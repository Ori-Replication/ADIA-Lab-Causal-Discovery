{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-21T11:20:39.508775Z",
                    "start_time": "2024-09-21T11:20:32.677062Z"
                },
                "metadata": {}
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "2024-09-27 01:45:40,454\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
                    ]
                }
            ],
            "source": [
                "import functools\n",
                "import os\n",
                "import typing\n",
                "import random\n",
                "import joblib\n",
                "import networkx as nx\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import pingouin as pg\n",
                "import dcor\n",
                "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
                "from scipy.stats import pearsonr, ttest_rel\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.feature_selection import mutual_info_regression\n",
                "from sklearn.metrics import balanced_accuracy_score, silhouette_score\n",
                "from sklearn.model_selection import GridSearchCV, GroupKFold, cross_val_score\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import Ridge, LinearRegression, LassoCV\n",
                "from sklearn.cluster import DBSCAN\n",
                "from sklearn.feature_selection import mutual_info_regression\n",
                "from sklearn.metrics.pairwise import rbf_kernel\n",
                "from sklearn.utils import check_random_state\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.feature_selection import mutual_info_regression\n",
                "from sklearn.preprocessing import KBinsDiscretizer\n",
                "from sklearn.model_selection import KFold\n",
                "from lightgbm import LGBMRegressor\n",
                "from scipy.special import softmax\n",
                "from scipy.stats import chi2_contingency, ks_2samp\n",
                "from scipy.spatial.distance import pdist, squareform\n",
                "from tqdm.auto import tqdm\n",
                "from scipy import stats\n",
                "from scipy.stats import entropy\n",
                "import statsmodels.api as sm\n",
                "import pingouin as pg\n",
                "from pgmpy.estimators import CITests\n",
                "from scipy.spatial.distance import pdist, squareform\n",
                "from econml.dml import CausalForestDML, DML\n",
                "import lightgbm as lgb\n",
                "from sklearn.base import BaseEstimator, RegressorMixin\n",
                "from sklearn.exceptions import NotFittedError\n",
                "from sklearn.model_selection import KFold\n",
                "from lightgbm import LGBMRegressor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-20T02:42:13.047359Z",
                    "start_time": "2024-09-20T02:42:12.325055Z"
                },
                "metadata": {}
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "loaded inline runner with module: <module '__main__'>\n"
                    ]
                }
            ],
            "source": [
                "import crunch\n",
                "crunch = crunch.load_notebook()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:30:54.182971Z",
                    "start_time": "2024-09-18T09:30:54.145678Z"
                }
            },
            "outputs": [],
            "source": [
                "def graph_nodes_representation(graph, nodelist):\n",
                "    \"\"\"\n",
                "    Create an alternative representation of a graph which is hashable\n",
                "    and equivalent graphs have the same hash.\n",
                "\n",
                "    Python cannot PROPERLY use nx.Graph/DiGraph as key for\n",
                "    dictionaries, because two equivalent graphs with just different\n",
                "    order of the nodes would result in different keys. This is\n",
                "    undesirable here.\n",
                "\n",
                "    So here we transform the graph into an equivalent form that is\n",
                "    based on a specific nodelist and that is hashable. In this way,\n",
                "    two equivalent graphs, once transformed, will result in identical\n",
                "    keys.\n",
                "\n",
                "    So we use the following trick: extract the adjacency matrix\n",
                "    (with nodes in a fixed order) and then make a hashable thing out\n",
                "    of it, through tuple(array.flatten()):\n",
                "    \"\"\"\n",
                "\n",
                "    # This get the adjacency matrix with nodes in a given order, as\n",
                "    # numpy array (which is not hashable):\n",
                "    adjacency_matrix = nx.adjacency_matrix(graph, nodelist=nodelist).todense()\n",
                "\n",
                "    # This transforms the numpy array into a hashable object:\n",
                "    hashable = tuple(adjacency_matrix.flatten())\n",
                "\n",
                "    return hashable\n",
                "\n",
                "def create_graph_label():\n",
                "    \"\"\"\n",
                "    Create a dictionary from graphs to labels, in two formats.\n",
                "    \"\"\"\n",
                "    graph_label = {\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]):             \"Cause of X\",\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]):             \"Cause of Y\",\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]):             \"Consequence of X\",\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]):             \"Consequence of Y\",\n",
                "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}):                \"Independent\",\n",
                "    }\n",
                "\n",
                "    nodelist = [\"v\", \"X\", \"Y\"]\n",
                "\n",
                "    # This is an equivalent alternative to graph_label but in a form\n",
                "    # for which two equivalent graphs have the same key:\n",
                "    adjacency_label = {\n",
                "        graph_nodes_representation(graph, nodelist): label\n",
                "        for graph, label in graph_label.items()\n",
                "    }\n",
                "\n",
                "    return graph_label, adjacency_label\n",
                "\n",
                "def get_labels(adjacency_matrix, adjacency_label):\n",
                "    \"\"\"\n",
                "    Transform an adjacency_matrix (as pd.DataFrame) into a dictionary of variable:label\n",
                "    \"\"\"\n",
                "\n",
                "    result = {}\n",
                "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
                "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]  # this is not hashable\n",
                "        key = tuple(submatrix.values.flatten())  # this is hashable and a compatible with adjacency_label\n",
                "    \n",
                "        result[variable] = adjacency_label[key]\n",
                "\n",
                "    return result\n",
                "\n",
                "graph_label, adjacency_label = create_graph_label()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def apply_mapping(df, mapping):\n",
                "    df_new = df.copy()\n",
                "    # 创建临时映射以避免冲突\n",
                "    temp_mapping = {k: f'_temp_{k}' for k in mapping.keys()}\n",
                "    df_new.rename(columns=temp_mapping, inplace=True)\n",
                "    if df_new.shape[0] == df_new.shape[1]:  # 如果是方阵，如标签矩阵\n",
                "        df_new.rename(index=temp_mapping, inplace=True)\n",
                "    # 应用最终映射\n",
                "    final_mapping = {f'_temp_{k}': v for k, v in mapping.items()}\n",
                "    df_new.rename(columns=final_mapping, inplace=True)\n",
                "    if df_new.shape[0] == df_new.shape[1]:\n",
                "        df_new.rename(index=final_mapping, inplace=True)\n",
                "    return df_new\n",
                "\n",
                "def check_duplicate_columns(df):\n",
                "    \"\"\"检查是否存在重复的列名\"\"\"\n",
                "    return df.columns.duplicated().any()\n",
                "\n",
                "def augment_data(X_train, y_train):\n",
                "    new_X_train = X_train.copy()\n",
                "    new_y_train = y_train.copy()\n",
                "    for sample_id in X_train.keys():\n",
                "        X = X_train[sample_id]\n",
                "        y = y_train[sample_id]\n",
                "        variables = list(X.columns)\n",
                "        dim = len(variables)\n",
                "        # 提取因果关系对\n",
                "        edges = []\n",
                "        for u in y.index:\n",
                "            for v in y.columns:\n",
                "                if y.loc[u, v] == 1:\n",
                "                    edges.append((u, v))\n",
                "        # 排除涉及 X 和 Y 的边\n",
                "        edges_no_XY = [(u, v) for (u, v) in edges if u not in ['X', 'Y'] and v not in ['X', 'Y']]\n",
                "        if dim >= 4:\n",
                "            edges_to_use = edges_no_XY\n",
                "            attempts = 0\n",
                "            success = False\n",
                "            while attempts < 3 and not success:\n",
                "                if not edges_to_use:\n",
                "                    break  # 没有合适的边，跳出循环\n",
                "                u, v = random.choice(edges_to_use)\n",
                "                mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
                "                # 应用映射到特征矩阵和标签矩阵\n",
                "                X_new = apply_mapping(X, mapping)\n",
                "                y_new = apply_mapping(y, mapping)\n",
                "                # 检查特征矩阵是否有重复列\n",
                "                if check_duplicate_columns(X_new):\n",
                "                    attempts += 1\n",
                "                    continue  # 重试\n",
                "                else:\n",
                "                    # 没有重复列，存储新的数据\n",
                "                    new_sample_id = '0' + sample_id\n",
                "                    new_X_train[new_sample_id] = X_new\n",
                "                    new_y_train[new_sample_id] = y_new\n",
                "                    success = True\n",
                "            if not success:\n",
                "                # 没有找到合适的映射，复制原始数据\n",
                "                new_sample_id = '0' + sample_id\n",
                "                new_X_train[new_sample_id] = X.copy()\n",
                "                new_y_train[new_sample_id] = y.copy()\n",
                "        else:\n",
                "            # 对于维度较低的数据，允许涉及 X 和 Y 的边\n",
                "            edges_to_use = edges\n",
                "            if not edges_to_use:\n",
                "                # 没有边，复制原始数据\n",
                "                new_sample_id = '0' + sample_id\n",
                "                new_X_train[new_sample_id] = X.copy()\n",
                "                new_y_train[new_sample_id] = y.copy()\n",
                "                continue\n",
                "            u, v = random.choice(edges_to_use)\n",
                "            mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
                "            # 应用映射到特征矩阵和标签矩阵\n",
                "            X_new = apply_mapping(X, mapping)\n",
                "            y_new = apply_mapping(y, mapping)\n",
                "            # 检查特征矩阵是否有重复列\n",
                "            if check_duplicate_columns(X_new):\n",
                "                # 如果有重复列，复制原始数据\n",
                "                new_sample_id = '0' + sample_id\n",
                "                new_X_train[new_sample_id] = X.copy()\n",
                "                new_y_train[new_sample_id] = y.copy()\n",
                "            else:\n",
                "                # 没有重复列，存储新的数据\n",
                "                new_sample_id = '0' + sample_id\n",
                "                new_X_train[new_sample_id] = X_new\n",
                "                new_y_train[new_sample_id] = y_new\n",
                "    return new_X_train, new_y_train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-20T02:43:04.532840Z",
                    "start_time": "2024-09-20T02:42:51.402011Z"
                },
                "metadata": {}
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "download data\\X_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_train.pickle (1523944532 bytes)\n",
                        "already exists: file length match\n",
                        "download data\\y_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_train.pickle (7017735 bytes)\n",
                        "already exists: file length match\n",
                        "download data\\X_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_test_reduced.pickle (122341879 bytes)\n",
                        "already exists: file length match\n",
                        "download data\\y_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_test_reduced.pickle (562930 bytes)\n",
                        "already exists: file length match\n",
                        "download data\\example_prediction.parquet from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/example_prediction_reduced.parquet (668981 bytes)\n",
                        "already exists: file length match\n",
                        "23500 23500 1880\n"
                    ]
                }
            ],
            "source": [
                "X_train, y_train, X_test = crunch.load_data()\n",
                "print(len(X_train), len(y_train), len(X_test))\n",
                "# X_train, y_train = augment_data(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:31:23.461861Z",
                    "start_time": "2024-09-18T09:31:23.368017Z"
                },
                "metadata": {}
            },
            "outputs": [],
            "source": [
                "def pearson_correlation(dataset):\n",
                "    \"\"\"\n",
                "    Given a dataset, we compute the correlation-based features for each\n",
                "    varibale, which are the correlation between that variable with X and Y,\n",
                "    as well as summary statistics (max, min, mean, std) of all pairs\n",
                "    of correlations.\n",
                "    \"\"\"\n",
                "\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        tmp = dataset.corr().drop([variable], axis=\"columns\").loc[variable].abs()\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"corr(v,X)\": dataset[[variable, \"X\"]].corr().loc[variable, \"X\"],\n",
                "            \"corr(v,Y)\": dataset[[variable, \"Y\"]].corr().loc[variable, \"Y\"],\n",
                "            \"max(corr(v, others))\": tmp.max(),\n",
                "            \"min(corr(v, others))\": tmp.min(),\n",
                "            \"mean(corr(v, others))\": tmp.mean(),\n",
                "            \"std(corr(v, others))\": tmp.std(),\n",
                "            \"25%(corr(v, others))\": tmp.quantile(0.25), \n",
                "            \"75%(corr(v, others))\": tmp.quantile(0.75), \n",
                "        })\n",
                "\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    df[\"corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr().loc[\"X\", \"Y\"]\n",
                "\n",
                "    # pearsonr is NaN when the variance is 0, so we fill with 0\n",
                "    df.fillna(0, inplace=True)\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def mutual_information(dataset):\n",
                "    \"\"\"\n",
                "    Given a dataset, we compute the mutual-information-based features\n",
                "    for each variable, which are the MI between that variable\n",
                "    and X and Y, as well as summary statistics (max, min, mean, std) of\n",
                "    all pairs of MI.\n",
                "    \"\"\"\n",
                "\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        tmp = mutual_info_regression(dataset.drop(columns=[variable]), dataset[variable])\n",
                "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"MI(v,X)\": mutual_info_regression(dataset[[variable]], dataset[\"X\"], discrete_features=False)[0],\n",
                "            \"MI(v,Y)\": mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0],\n",
                "            \"max(MI(v, others))\": tmp.max(),\n",
                "            \"min(MI(v, others))\": tmp.min(),\n",
                "            \"mean(MI(v, others))\": tmp.mean(),\n",
                "            \"std(MI(v, others))\": tmp.std(),\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    df[\"MI(X,Y)\"] = mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def add_dimension_feature(dataset):\n",
                "    \"\"\"\n",
                "    Add a dimension feature to the dataset.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "    dimension = len(variables)\n",
                "    square_dimension = dimension * dimension\n",
                "    df = pd.DataFrame({\n",
                "        \"variable\": variables,\n",
                "        \"dimension\": dimension,\n",
                "        \"square_dimension\": square_dimension\n",
                "    })\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    return df\n",
                "\n",
                "def spearman_correlation(dataset):\n",
                "    \"\"\"\n",
                "    Given a dataset, we compute the Spearman rank correlation-based features for each\n",
                "    variable, which are the Spearman correlation between that variable with X and Y,\n",
                "    as well as summary statistics (max, min, mean, std) of all pairs of Spearman correlations.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        tmp = dataset.corr(method='spearman').drop([variable], axis=\"columns\").loc[variable].abs()\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"spearman_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='spearman').loc[variable, \"X\"],\n",
                "            \"spearman_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='spearman').loc[variable, \"Y\"],\n",
                "            \"max(spearman_corr(v, others))\": tmp.max(),\n",
                "            \"min(spearman_corr(v, others))\": tmp.min(),\n",
                "            \"mean(spearman_corr(v, others))\": tmp.mean(),\n",
                "            \"std(spearman_corr(v, others))\": tmp.std(),\n",
                "            #TODO 分位数\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    df[\"spearman_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='spearman').loc[\"X\", \"Y\"]\n",
                "\n",
                "    # Spearman correlation is NaN when there are ties in rank, so we fill with 0\n",
                "    df.fillna(0, inplace=True)\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def kendall_correlation(dataset):\n",
                "    \"\"\"\n",
                "    Given a dataset, we compute the Kendall's tau correlation-based features for each\n",
                "    variable, which are the Kendall's tau correlation between that variable with X and Y,\n",
                "    as well as summary statistics (max, min, mean, std) of all pairs of Kendall's tau correlations.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        tmp = dataset.corr(method='kendall').drop([variable], axis=\"columns\").loc[variable].abs()\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"kendall_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='kendall').loc[variable, \"X\"],\n",
                "            \"kendall_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='kendall').loc[variable, \"Y\"],\n",
                "            \"max(kendall_corr(v, others))\": tmp.max(),\n",
                "            \"min(kendall_corr(v, others))\": tmp.min(),\n",
                "            \"mean(kendall_corr(v, others))\": tmp.mean(),\n",
                "            \"std(kendall_corr(v, others))\": tmp.std(),\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    df[\"kendall_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='kendall').loc[\"X\", \"Y\"]\n",
                "\n",
                "    # Kendall's tau correlation can be NaN in some cases, so we fill with 0\n",
                "    df.fillna(0, inplace=True)\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def distance_correlation(dataset):\n",
                "    \"\"\"\n",
                "    Given a dataset, we compute the distance correlation-based features for each\n",
                "    variable, which are the distance correlation between that variable with X and Y,\n",
                "    as well as summary statistics (max, min, mean, std) of all pairs of distance correlations.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        tmp = []\n",
                "        # Compute distance correlation between 'variable' and all other variables (excluding itself)\n",
                "        other_variables = dataset.columns.drop([variable])\n",
                "        for other_var in other_variables:\n",
                "            corr = dcor.distance_correlation(dataset[variable], dataset[other_var])\n",
                "            tmp.append(corr)\n",
                "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
                "\n",
                "        distance_correlation_v_X = dcor.distance_correlation(dataset[variable], dataset[\"X\"])\n",
                "        distance_correlation_v_Y = dcor.distance_correlation(dataset[variable], dataset[\"Y\"])\n",
                "        distance_correlation_X_Y = dcor.distance_correlation(dataset[\"X\"], dataset[\"Y\"])\n",
                "        distance_correlation_v_X_square = distance_correlation_v_X ** 2\n",
                "        distance_correlation_v_Y_square = distance_correlation_v_Y ** 2\n",
                "        distance_correlation_X_Y_square = distance_correlation_X_Y ** 2\n",
                "        \n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"dcor(v,X)\": distance_correlation_v_X,\n",
                "            \"dcor(v,Y)\": distance_correlation_v_Y,\n",
                "            \"dcor(v,X)^2\": distance_correlation_v_X_square,\n",
                "            \"dcor(v,Y)^2\": distance_correlation_v_Y_square,\n",
                "            \"max(dcor(v, others))\": tmp.max(),\n",
                "            \"min(dcor(v, others))\": tmp.min(),\n",
                "            \"mean(dcor(v, others))\": tmp.mean(),\n",
                "            \"std(dcor(v, others))\": tmp.std(),\n",
                "            \"25%(dcor(v, others))\": tmp.quantile(0.25),# Success\n",
                "            \"75%(dcor(v, others))\": tmp.quantile(0.75),\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    df[\"dcor(X,Y)\"] = distance_correlation_X_Y\n",
                "    df[\"dcor(X,Y)^2\"] = distance_correlation_X_Y_square\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "\n",
                "def double_centered_distance_matrix(X):\n",
                "    \"\"\"Compute the double centered distance matrix for X.\"\"\"\n",
                "    n = X.shape[0]\n",
                "    dist_matrix = squareform(pdist(X[:, None]))  # Compute pairwise distances\n",
                "    row_mean = np.mean(dist_matrix, axis=1, keepdims=True)\n",
                "    col_mean = np.mean(dist_matrix, axis=0, keepdims=True)\n",
                "    total_mean = np.mean(dist_matrix)\n",
                "\n",
                "    # Double-centering the matrix\n",
                "    A = dist_matrix - row_mean - col_mean + total_mean\n",
                "    return A\n",
                "\n",
                "def distance_covariance(X, Y):\n",
                "    \"\"\"Compute distance covariance between X and Y.\"\"\"\n",
                "    A = double_centered_distance_matrix(X)\n",
                "    B = double_centered_distance_matrix(Y)\n",
                "    dcov = np.mean(A * B)\n",
                "    return dcov\n",
                "\n",
                "def partial_distance_covariance(X, Y, Z):\n",
                "    \"\"\"Compute partial distance covariance between X and Y given Z.\"\"\"\n",
                "    dcov_XY = distance_covariance(X, Y)\n",
                "    dcov_XZ = distance_covariance(X, Z)\n",
                "    dcov_YZ = distance_covariance(Y, Z)\n",
                "    dcov_ZZ = distance_covariance(Z, Z)\n",
                "\n",
                "    # Partial distance covariance formula\n",
                "    pdcov = dcov_XY - (dcov_XZ * dcov_YZ) / dcov_ZZ\n",
                "    return pdcov\n",
                "\n",
                "def partial_distance_covariance_dataset(dataset):\n",
                "    \"\"\"\n",
                "    Given a dataset, we compute the partial distance covariance-based features for each\n",
                "    variable, which are the partial distance covariance between that variable with X and Y,\n",
                "    using each as the conditioning variable.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        pdcov_v_X_given_Y = partial_distance_covariance(dataset[variable].values, dataset[\"X\"].values, dataset[\"Y\"].values)\n",
                "        pdcov_v_Y_given_X = partial_distance_covariance(dataset[variable].values, dataset[\"Y\"].values, dataset[\"X\"].values)\n",
                "        pdcov_X_Y_given_v = partial_distance_covariance(dataset[\"X\"].values, dataset[\"Y\"].values, dataset[variable].values)\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"pdcov(v,X|Y)\": pdcov_v_X_given_Y,\n",
                "            \"pdcov(v,Y|X)\": pdcov_v_Y_given_X,\n",
                "            \"pdcov(X,Y|v)\": pdcov_X_Y_given_v,\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\", \"variable\", \"pdcov(v,X|Y)\", \"pdcov(v,Y|X)\", \"pdcov(X,Y|v)\"]]\n",
                "\n",
                "    return df\n",
                "def Squared_term(dataset, variables):\n",
                "        for var in variables:\n",
                "            dataset[f'{var}_squared_term'] = dataset[var] ** 2\n",
                "        return dataset\n",
                "    \n",
                "def Interaction_term(dataset, variables):\n",
                "    for i in range(len(variables)):\n",
                "        for j in range(i + 1, len(variables)):\n",
                "            dataset[f'{variables[i]}_{variables[j]}'] = dataset[variables[i]] * dataset[variables[j]]\n",
                "    return dataset\n",
                "\n",
                "def Cos_Sin_term(dataset, variables):\n",
                "    for var in variables:\n",
                "        dataset[f'{var}_cos_term'] = np.cos(dataset[var])\n",
                "        dataset[f'{var}_sin_term'] = np.sin(dataset[var])\n",
                "    return dataset\n",
                "\n",
                "def linear_regression_feature(dataset):\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                "\n",
                "    # model1: Fit X, v, v^2, v_i*v_j, v_i*X ~ Y\n",
                "    model1_features = [\"X\"] + variables\n",
                "    d1 = Squared_term(dataset[model1_features], model1_features)\n",
                "    d1 = Interaction_term(d1, model1_features)  # 0.4600-0.4648\n",
                "    # d1 = Cos_Sin_term(d1, variables)\n",
                "    model1_features = d1.columns.tolist()\n",
                "    scaler = StandardScaler()\n",
                "    d1_scaled = scaler.fit_transform(d1)\n",
                "    model1 = LinearRegression().fit(d1_scaled, dataset[[\"Y\"]])\n",
                "    model1_coefs = model1.coef_[0].tolist()\n",
                "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
                "    \n",
                "    # model2: Fit v, v^2, cos(v)/sin(v) ~ X\n",
                "    model2_features = variables\n",
                "    d2 = Squared_term(dataset[model2_features], model2_features)\n",
                "    d2 = Interaction_term(d2, model2_features)  # 0.4648-0.4643\n",
                "    d2 = Cos_Sin_term(d2, model2_features)\n",
                "    model2_features = d2.columns.tolist()\n",
                "    scaler = StandardScaler()\n",
                "    d2_scaled = scaler.fit_transform(d2)\n",
                "    model2 = LinearRegression().fit(d2_scaled, dataset[[\"X\"]])\n",
                "    model2_coefs = model2.coef_[0].tolist()\n",
                "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
                "    \n",
                "    df = []\n",
                "    for i, variable in enumerate(variables):\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"v~Y_coefficient\": model1_dict[variable],\n",
                "            \"v_squared~Y_coefficient\": model1_dict[f\"{variable}_squared_term\"],\n",
                "            # \"v_cos~Y_coefficient\": model1_dict[f\"{variable}_cos_term\"],  # 掉分\n",
                "            # \"v_sin~Y_coefficient\": model1_dict[f\"{variable}_sin_term\"],\n",
                "            \"v~X_coefficient\": model2_dict[variable],\n",
                "            \"v_squared~X_coefficient\": model2_dict[f\"{variable}_squared_term\"],\n",
                "            \"v_cos~X_coefficient\": model2_dict[f\"{variable}_cos_term\"], \n",
                "            \"v_sin~X_coefficient\": model2_dict[f\"{variable}_sin_term\"],\n",
                "        })\n",
                "        \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    df[\"X~Y_coefficient\"] = model1_dict[\"X\"]\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def ridge_regression_feature(dataset):\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                "    # 使用GridSearchCV来选择最佳的alpha值\n",
                "    param_grid = {'alpha': np.logspace(-6, 6, 13)}\n",
                "    \n",
                "    # model1: Fit X, v ~ Y\n",
                "    model1_features = [\"X\"] + variables\n",
                "    d1 = Squared_term(dataset[model1_features], model1_features)\n",
                "    # d1 = Interaction_term(d1, model1_features) # 掉分\n",
                "    model1_features = d1.columns.tolist()\n",
                "    scaler1 = StandardScaler()\n",
                "    d1_scaled = scaler1.fit_transform(d1)\n",
                "    model1 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
                "    model1.fit(d1_scaled, dataset[\"Y\"])\n",
                "    model1_coefs = model1.best_estimator_.coef_.tolist()\n",
                "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
                "    \n",
                "    # model2: Fit v ~ X\n",
                "    model2_features = variables\n",
                "    d2 = Squared_term(dataset[model2_features], model2_features)\n",
                "    d2 = Interaction_term(d2, model2_features)\n",
                "    # d2 = Cos_Sin_term(d2, model2_features)  # 掉分\n",
                "    model2_features = d2.columns.tolist()\n",
                "    scaler2 = StandardScaler()\n",
                "    d2_scaled = scaler2.fit_transform(d2)\n",
                "    model2 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
                "    model2.fit(d2_scaled, dataset[\"X\"])\n",
                "    model2_coefs = model2.best_estimator_.coef_.tolist()\n",
                "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
                "\n",
                "    # # 获取最优的 alpha 值\n",
                "    # best_alpha_model1 = model1.best_params_['alpha']   # 0.4730-0.4727\n",
                "    # best_alpha_model2 = model2.best_params_['alpha']\n",
                "    \n",
                "    df = []\n",
                "    for i, variable in enumerate(variables):\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"v~Y_ridge_coefficient\": model1_dict[variable],\n",
                "            # \"v_squared~Y_ridge_coefficient\": model1_dict[f\"{variable}_squared_term\"],  # 掉分\n",
                "            \"v~X_ridge_coefficient\": model2_dict[variable],\n",
                "            # \"v_squared~X_ridge_coefficient\": model2_dict[f\"{variable}_squared_term\"],  # 掉分\n",
                "            # \"v_cos~X_ridge_coefficient\": model2_dict[f\"{variable}_cos_term\"],  # 掉分\n",
                "            # \"v_sin~X_ridge_coefficient\": model2_dict[f\"{variable}_sin_term\"],  # 掉分\n",
                "            # \"v~Y_ridge_alpha\": best_alpha_model1,\n",
                "            # \"v~X_ridge_alpha\": best_alpha_model2\n",
                "        })\n",
                "        \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    df[\"X~Y_ridge_coefficient\"] = model1_dict[\"X\"]\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def cluster_features(dataset):\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                "    scaler = StandardScaler()\n",
                "    d_scaled = pd.DataFrame(scaler.fit_transform(dataset), columns=dataset.columns)\n",
                "\n",
                "    results = []\n",
                "    eps_values = [0.3]  # , 0.5, 0.7\n",
                "    \n",
                "    for variable in variables:\n",
                "        cluster_counts = []\n",
                "        noise_counts = []\n",
                "        avg_cluster_sizes = []\n",
                "        density_variations = []\n",
                "        feature_importances = []\n",
                "        silhouette_scores = []\n",
                "        \n",
                "        for eps in eps_values:\n",
                "            cluster_df = d_scaled[[variable, \"X\", \"Y\"]].copy()\n",
                "            dbscan = DBSCAN(eps=eps, min_samples=5)\n",
                "            cluster_df[\"cluster\"] = dbscan.fit_predict(cluster_df)\n",
                "            \n",
                "            # 基本统计\n",
                "            cluster_count = len(set(cluster_df[\"cluster\"])) - (1 if -1 in cluster_df[\"cluster\"] else 0)\n",
                "            noise_count = (cluster_df[\"cluster\"] == -1).sum()\n",
                "            cluster_counts.append(cluster_count)\n",
                "            noise_counts.append(noise_count)\n",
                "            \n",
                "            # 密度分析\n",
                "            cluster_sizes = cluster_df[cluster_df[\"cluster\"] != -1][\"cluster\"].value_counts()\n",
                "            avg_cluster_size = cluster_sizes.mean() if not cluster_sizes.empty else 0\n",
                "            density_variation = cluster_sizes.std() / avg_cluster_size if avg_cluster_size > 0 else 0\n",
                "            avg_cluster_sizes.append(avg_cluster_size)\n",
                "            density_variations.append(density_variation)\n",
                "            \n",
                "            # 特征重要性\n",
                "            if cluster_count > 1:  # 确保有多个簇\n",
                "                feature_importance = abs(np.corrcoef(cluster_df[variable], cluster_df[\"cluster\"]))[0, 1]\n",
                "            else:\n",
                "                feature_importance = 0\n",
                "            feature_importances.append(feature_importance)\n",
                "            \n",
                "            # 轮廓系数\n",
                "            non_noise_mask = cluster_df[\"cluster\"] != -1\n",
                "            if len(set(cluster_df.loc[non_noise_mask, \"cluster\"])) > 1:\n",
                "                sil_score = silhouette_score(cluster_df.loc[non_noise_mask, [variable, \"X\", \"Y\"]], \n",
                "                                             cluster_df.loc[non_noise_mask, \"cluster\"], \n",
                "                                             metric=\"euclidean\")\n",
                "            else:\n",
                "                sil_score = 0\n",
                "            silhouette_scores.append(sil_score)\n",
                "        \n",
                "        result = {\n",
                "            \"variable\": variable\n",
                "        }\n",
                "        for i, eps in enumerate(eps_values):\n",
                "            result.update({\n",
                "                f\"cluster_count_{eps}\": cluster_counts[i],             # 0.4730-0.4736\n",
                "                # f\"noise_count_{eps}\": noise_counts[i],                 # 0.4736-0.4740\n",
                "                # f\"avg_cluster_size_{eps}\": avg_cluster_sizes[i],     # 0.4740-0.4735\n",
                "                # f\"density_variation_{eps}\": density_variations[i],     # 0.4740-0.4741\n",
                "                # f\"feature_importance_{eps}\": feature_importances[i], # 0.4741-0.4736\n",
                "                # f\"silhouette_score_{eps}\": silhouette_scores[i]      # 0.4741-0.4723\n",
                "            })\n",
                "        results.append(result)\n",
                "\n",
                "    df = pd.DataFrame(results)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def conditional_independence_tests(dataset):  # 太慢了，得三个小时\n",
                "    \"\"\"\n",
                "    A mixed-data residualization based conditional independence test[1].\n",
                "    Uses XGBoost estimator to compute LS residuals[2], and then does an association test (Pillai’s Trace) on the residuals.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        # v-X\n",
                "        coef1, p_value1 = CITests.ci_pillai(X=variable, Y=\"X\", Z=dataset.columns.drop([\"X\", variable]).tolist(), data=dataset, boolean=False)\n",
                "        # v-Y\n",
                "        coef2, p_value2 = CITests.ci_pillai(X=variable, Y=\"Y\", Z=dataset.columns.drop([\"Y\", variable]).tolist(), data=dataset, boolean=False)\n",
                "        # X-v\n",
                "        coef3, p_value3 = CITests.ci_pillai(X=\"X\", Y=variable, Z=dataset.columns.drop([\"X\", variable]).tolist(), data=dataset, boolean=False)\n",
                "        # Y-v\n",
                "        coef4, p_value4 = CITests.ci_pillai(X=\"Y\", Y=variable, Z=dataset.columns.drop([\"Y\", variable]).tolist(), data=dataset, boolean=False)\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"v~X_ci_pillai_coef\": coef1,\n",
                "            \"v~X_ci_pillai_p_value\": p_value1,\n",
                "            \"v~Y_ci_pillai_coef\": coef2,\n",
                "            \"v~Y_ci_pillai_p_value\": p_value2,\n",
                "            \"X~v_ci_pillai_coef\": coef3,\n",
                "            \"X~v_ci_pillai_p_value\": p_value3,\n",
                "            \"Y~v_ci_pillai_coef\": coef4,\n",
                "            \"Y~v_ci_pillai_p_value\": p_value4\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "\n",
                "def DML(T_, Y_, X_, data):\n",
                "    \"\"\"\n",
                "    双重机器学习 (Double Machine Learning, DML) 函数，使用 LassoCV 作为第一阶段模型。\n",
                "    参数:\n",
                "        T_ (str): 处理变量的列名。\n",
                "        Y_ (str): 结果变量的列名。\n",
                "        X_ (list of str): 控制变量的列名。\n",
                "        data (pd.DataFrame): 包含上述列的数据集。\n",
                "    返回:\n",
                "        amte (float): 平均处理效应 (Average Treatment Effect, ATE)。\n",
                "        amte_inference (object): ATE 的推断结果。\n",
                "        amte_interval (tuple): ATE 的置信区间。\n",
                "    \"\"\"\n",
                "    # 提取处理变量、结果变量和控制变量\n",
                "    T = data[T_].values.reshape(-1, 1)\n",
                "    Y = data[Y_].values.reshape(-1, 1)\n",
                "    X = data[X_].values\n",
                "\n",
                "    # # 定义 LassoCV 作为第一阶段模型\n",
                "    model_t = LassoCV(cv=4, random_state=42, n_jobs=-1)\n",
                "    model_y = LassoCV(cv=4, random_state=42, n_jobs=-1)\n",
                "\n",
                "    # 初始化 CausalForestDML 使用自定义的 LassoCV 估计器\n",
                "    model = CausalForestDML(\n",
                "        model_t=model_t,\n",
                "        model_y=model_y,\n",
                "        n_jobs=-1,\n",
                "        random_state=42,  # 确保随机性的一致性\n",
                "        inference=True,\n",
                "    )\n",
                "    model.fit(Y, T, X=X)\n",
                "\n",
                "    # 计算平均处理效应 (ATE)\n",
                "    amte = model.ate(X=X)\n",
                "    # ATE 的置信区间\n",
                "    amte_interval = model.ate_interval(X=X, alpha=0.05)\n",
                "    # 推断结果（这里等同于置信区间）\n",
                "    amte_inference = amte_interval\n",
                "\n",
                "    return amte, amte_inference, amte_interval\n",
                "\n",
                "def double_machine_learning(dataset):\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "    dataset = dataset.sample(frac=0.5, random_state=42)\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        # 判断v-X的因果效应，设置variables中的其他v和Y为控制变量\n",
                "        amte, amte_inference, amte_interval = DML(variable, \"X\", [\"Y\"] + list(variables.drop(variable)), dataset)\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"v~X_DML_AMTE\": amte.item(),  \n",
                "            # \"v~X_DML_AMTE_stderr\": amte_inference.stderr_mean.item(),\n",
                "            # \"v~X_DML_AMTE_pvalue\": amte_inference.pvalue_mean.item(),\n",
                "            # \"v~X_DML_AMTE_lower\": amte_interval[0].item(),\n",
                "            # \"v~X_DML_AMTE_upper\": amte_interval[1].item()\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def conditional_mutual_information(dataset):\n",
                "    \"\"\"\n",
                "    Calculate conditional mutual information for each variable with X and Y.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "    \n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        # Calculate conditional MI(v, X | Y)\n",
                "        mi_vx_given_y = mutual_info_regression(dataset[[variable, \"Y\"]], dataset[\"X\"], discrete_features=False)[0] - \\\n",
                "                        mutual_info_regression(dataset[[\"Y\"]], dataset[\"X\"], discrete_features=False)[0]\n",
                "        \n",
                "        # Calculate conditional MI(v, Y | X)\n",
                "        mi_vy_given_x = mutual_info_regression(dataset[[variable, \"X\"]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
                "                        mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
                "        \n",
                "        # Calculate conditional MI(X, Y | v)\n",
                "        mi_xy_given_v = mutual_info_regression(dataset[[\"X\", variable]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
                "                        mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0]\n",
                "        \n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"conditional_MI(v,X|Y)\": mi_vx_given_y,\n",
                "            \"conditional_MI(v,Y|X)\": mi_vy_given_x,\n",
                "            \"conditional_MI(X,Y|v)\": mi_xy_given_v,\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "    \n",
                "    return df\n",
                "\n",
                "def partial_correlation(dataset):\n",
                "    \"\"\"\n",
                "    Compute partial correlation coefficients for each variable with X and Y,\n",
                "    controlling for the other variable, as well as the partial correlation\n",
                "    between X and Y controlling for each variable.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "    \n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        # Compute partial correlations\n",
                "        pcorr_vX_Y = pg.partial_corr(data=dataset, x=variable, y='X', covar='Y')['r'].iloc[0]\n",
                "        pcorr_vY_X = pg.partial_corr(data=dataset, x=variable, y='Y', covar='X')['r'].iloc[0]\n",
                "        pcorr_XY_v = pg.partial_corr(data=dataset, x='X', y='Y', covar=variable)['r'].iloc[0]\n",
                "        \n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"partial_corr(v,X|Y)\": pcorr_vX_Y,\n",
                "            \"partial_corr(v,Y|X)\": pcorr_vY_X,\n",
                "            \"partial_corr(X,Y|v)\": pcorr_XY_v,\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "    \n",
                "    return df\n",
                "\n",
                "def loess_regression(dataset):\n",
                "    \"\"\"\n",
                "    Perform LOESS (Locally Weighted Scatterplot Smoothing) regression for each variable with X and Y.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "    \n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        # LOESS regression for v vs X\n",
                "        loess_vx = lowess(dataset[\"X\"], dataset[variable], frac=0.6667, it=5)\n",
                "        \n",
                "        # LOESS regression for v vs Y\n",
                "        loess_vy = lowess(dataset[\"Y\"], dataset[variable], frac=0.6667, it=5)\n",
                "        \n",
                "        # Calculate some summary statistics from the LOESS results\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"loess_vx_slope\": np.polyfit(loess_vx[:, 0], loess_vx[:, 1], 1)[0],\n",
                "            \"loess_vy_slope\": np.polyfit(loess_vy[:, 0], loess_vy[:, 1], 1)[0],\n",
                "            \"loess_vx_intercept\": np.polyfit(loess_vx[:, 0], loess_vx[:, 1], 1)[1],\n",
                "            \"loess_vy_intercept\": np.polyfit(loess_vy[:, 0], loess_vy[:, 1], 1)[1],\n",
                "            \"loess_vx_residual_std\": np.std(dataset[\"X\"] - np.interp(dataset[variable], loess_vx[:, 0], loess_vx[:, 1])),\n",
                "            \"loess_vy_residual_std\": np.std(dataset[\"Y\"] - np.interp(dataset[variable], loess_vy[:, 0], loess_vy[:, 1])),\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "    \n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:31:25.402707Z",
                    "start_time": "2024-09-18T09:31:25.388081Z"
                },
                "metadata": {}
            },
            "outputs": [],
            "source": [
                "def label(adjacency_matrix):\n",
                "    \"\"\"\n",
                "    Given a graph as adjacency_matrix, create the class labels of each variable.\n",
                "    \"\"\"\n",
                "\n",
                "    adjacency_graph, adjacency_label = create_graph_label()\n",
                "    labels = get_labels(adjacency_matrix, adjacency_label)\n",
                "    variables = adjacency_matrix.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = pd.DataFrame({\n",
                "        \"variable\": variables,\n",
                "        \"label\": [labels[variable] for variable in variables],\n",
                "    })\n",
                "    df[\"dataset\"] = adjacency_matrix.name\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:31:26.636102Z",
                    "start_time": "2024-09-18T09:31:26.629666Z"
                },
                "metadata": {}
            },
            "outputs": [],
            "source": [
                "def create_some_columns(names_datasets, function):\n",
                "    \"\"\"\n",
                "    Apply an embedding function to a list of datasets.\n",
                "    \"\"\"\n",
                "\n",
                "    df = []\n",
                "    for name, dataset in tqdm(names_datasets.items()):\n",
                "        dataset = names_datasets[name]\n",
                "        dataset.name = name\n",
                "    \n",
                "        try:\n",
                "            df_dataset = function(dataset)\n",
                "        except ValueError as e:\n",
                "            print(name, e)\n",
                "            raise NotImplementedError\n",
                "\n",
                "        df_dataset[\"dataset\"] = name\n",
                "        df.append(df_dataset)\n",
                "\n",
                "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:31:27.391364Z",
                    "start_time": "2024-09-18T09:31:27.380030Z"
                },
                "metadata": {}
            },
            "outputs": [],
            "source": [
                "def create_some_columns_parallel(names_datasets, function, n_jobs=-1):\n",
                "    \"\"\"\n",
                "    Apply an embedding function to a list of datasets.\n",
                "\n",
                "    Parallel version.\n",
                "    \"\"\"\n",
                "\n",
                "    def f(name, dataset, function):\n",
                "        dataset.name = name\n",
                "        df_dataset = function(dataset)\n",
                "        df_dataset[\"dataset\"] = name\n",
                "        return df_dataset\n",
                "\n",
                "    df = joblib.Parallel(n_jobs=n_jobs)(\n",
                "        joblib.delayed(f)(name, dataset, function)\n",
                "        for name, dataset in tqdm(names_datasets.items())\n",
                "    )\n",
                "\n",
                "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:31:28.411964Z",
                    "start_time": "2024-09-18T09:31:28.402880Z"
                },
                "metadata": {}
            },
            "outputs": [],
            "source": [
                "def create_all_columns(functions_names_datasets, n_jobs=-1, create_dimension_feature = False):\n",
                "    \"\"\"\n",
                "    given a dictionary of {function1:names, function2:names,...} apply\n",
                "    the desired functions to the list of datasets and merge all of them\n",
                "    in a single X_y_group dataframe.\n",
                "    \"\"\"\n",
                "\n",
                "    columns = []\n",
                "    if create_dimension_feature:\n",
                "        dimension_feature = create_some_columns(functions_names_datasets[list(functions_names_datasets.keys())[0]], add_dimension_feature)\n",
                "        columns.append(dimension_feature)\n",
                "    \n",
                "    for function, names_datasets in functions_names_datasets.items():\n",
                "        print(f\"set: {function.__name__}\")\n",
                "\n",
                "        if n_jobs != 1:\n",
                "            feature_set = create_some_columns_parallel(names_datasets, function, n_jobs=n_jobs)\n",
                "        else:\n",
                "            feature_set = create_some_columns(names_datasets, function)\n",
                "\n",
                "        columns.append(feature_set)\n",
                "\n",
                "    # Merge all feature sets into a single dataframe:\n",
                "    columns = functools.reduce(\n",
                "        lambda left, right: pd.merge(left, right, on=[\"dataset\", \"variable\"]),\n",
                "        columns,\n",
                "    )\n",
                "\n",
                "    return columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "names_datasets_train = X_train\n",
                "names_graphs_train = y_train\n",
                "print(f\"Creating X_y_group_train from {len(names_datasets_train)} datasets and graphs\")\n",
                "X_y_group_train = create_all_columns(\n",
                "    {\n",
                "        pearson_correlation: names_datasets_train,\n",
                "        mutual_information: names_datasets_train,  # uncomment this line to add features but at high computational cost\n",
                "        label: names_graphs_train,\n",
                "    },\n",
                "    n_jobs=-1,\n",
                "    create_dimension_feature=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "names_datasets_train = X_train\n",
                "names_graphs_train = y_train\n",
                "X_y_group_train_additional = create_all_columns(\n",
                "    {\n",
                "        spearman_correlation: names_datasets_train,\n",
                "        kendall_correlation: names_datasets_train,\n",
                "        distance_correlation: names_datasets_train,\n",
                "        conditional_mutual_information: names_datasets_train,\n",
                "        linear_regression_feature: names_datasets_train,\n",
                "    },\n",
                "    n_jobs=-1,\n",
                ")\n",
                "\n",
                "\n",
                "X_y_group_train = pd.concat([X_y_group_train, X_y_group_train_additional], axis=1)\n",
                "# 去掉重复的列\n",
                "X_y_group_train = X_y_group_train.loc[:,~X_y_group_train.columns.duplicated()]\n",
                "X_y_group_train['MI(v,X)^2'] = X_y_group_train['MI(v,X)'] ** 2\n",
                "X_y_group_train['MI(v,Y)^2'] = X_y_group_train['MI(v,Y)'] ** 2\n",
                "X_y_group_train['MI(X,Y)^2'] = X_y_group_train['MI(X,Y)'] ** 2\n",
                "X_y_group_train['max(MI(v, others))^2'] = X_y_group_train['max(MI(v, others))'] ** 2\n",
                "X_y_group_train['min(MI(v, others))^2'] = X_y_group_train['min(MI(v, others))'] ** 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:44:41.738202Z",
                    "start_time": "2024-09-18T09:44:40.143933Z"
                },
                "metadata": {}
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(142910, 64)\n",
                        "Index(['dataset', 'variable', 'dimension', 'corr(v,X)', 'corr(v,Y)',\n",
                        "       'max(corr(v, others))', 'min(corr(v, others))', 'mean(corr(v, others))',\n",
                        "       'std(corr(v, others))', 'corr(X,Y)', 'ttest(v,X)',\n",
                        "       'pvalue(ttest(v,X))<=0.05', 'ttest(v,Y)', 'pvalue(ttest(v,Y))<=0.05',\n",
                        "       'ttest(X,Y)', 'pvalue(ttest(X,Y))<=0.05', 'MI(v,X)', 'MI(v,Y)',\n",
                        "       'max(MI(v, others))', 'min(MI(v, others))', 'mean(MI(v, others))',\n",
                        "       'std(MI(v, others))', 'MI(X,Y)', 'label', 'y', 'spearman_corr(v,X)',\n",
                        "       'spearman_corr(v,Y)', 'spearman_corr(X,Y)', 'kendall_corr(v,X)',\n",
                        "       'kendall_corr(v,Y)', 'kendall_corr(X,Y)', 'dcor(v,X)', 'dcor(v,Y)',\n",
                        "       'dcor(X,Y)', 'MI(v,X)^2', 'MI(v,Y)^2', 'MI(X,Y)^2',\n",
                        "       'max(MI(v, others))^2', 'min(MI(v, others))^2', 'conditional_MI(v,X|Y)',\n",
                        "       'conditional_MI(v,Y|X)', 'conditional_MI(X,Y|v)', 'square_dimension',\n",
                        "       'max(spearman_corr(v, others))', 'min(spearman_corr(v, others))',\n",
                        "       'mean(spearman_corr(v, others))', 'std(spearman_corr(v, others))',\n",
                        "       'max(kendall_corr(v, others))', 'min(kendall_corr(v, others))',\n",
                        "       'mean(kendall_corr(v, others))', 'std(kendall_corr(v, others))',\n",
                        "       'partial_corr(v,X|Y)', 'partial_corr(v,Y|X)', 'partial_corr(X,Y|v)',\n",
                        "       'v~Y_coefficient', 'v_squared~Y_coefficient', 'v~X_coefficient',\n",
                        "       'v_squared~X_coefficient', 'v_cos~X_coefficient', 'v_sin~X_coefficient',\n",
                        "       'X~Y_coefficient', 'v~Y_ridge_coefficient', 'v~X_ridge_coefficient',\n",
                        "       'X~Y_ridge_coefficient'],\n",
                        "      dtype='object')\n"
                    ]
                }
            ],
            "source": [
                "X_y_group_train = pd.read_csv('./mid_data/X_y_group_train_updated_v8.7_线性回归v7&岭回归v2.csv')\n",
                "print(X_y_group_train.shape)\n",
                "print(X_y_group_train.columns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "set: loess_regression\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 23500/23500 [2:08:41<00:00,  3.04it/s]  \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "X_y_group_train.shape (142910, 72)\n",
                        "去重后X_y_group_train.shape (142910, 70)\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "names_datasets_train = X_train\n",
                "names_graphs_train = y_train\n",
                "X_y_group_train_additional = create_all_columns(\n",
                "    {\n",
                "        # distance_correlation: names_datasets_train,\n",
                "        loess_regression: names_datasets_train,\n",
                "    },\n",
                "    n_jobs=-1,\n",
                ")\n",
                "X_y_group_train = pd.concat([X_y_group_train, X_y_group_train_additional], axis=1)\n",
                "print('X_y_group_train.shape', X_y_group_train.shape)\n",
                "# 去掉重复的列\n",
                "X_y_group_train = X_y_group_train.loc[:,~X_y_group_train.columns.duplicated()]\n",
                "print('去重后X_y_group_train.shape', X_y_group_train.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "set: double_machine_learning\n",
                "  1%|          | 288/23500 [02:17<2:48:55,  2.29it/s]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:34:49.212104Z",
                    "start_time": "2024-09-18T09:34:48.478623Z"
                },
                "metadata": {}
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Adding numeric labels y\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>dataset</th>\n",
                            "      <th>variable</th>\n",
                            "      <th>dimension</th>\n",
                            "      <th>corr(v,X)</th>\n",
                            "      <th>corr(v,Y)</th>\n",
                            "      <th>max(corr(v, others))</th>\n",
                            "      <th>min(corr(v, others))</th>\n",
                            "      <th>mean(corr(v, others))</th>\n",
                            "      <th>std(corr(v, others))</th>\n",
                            "      <th>corr(X,Y)</th>\n",
                            "      <th>...</th>\n",
                            "      <th>v~X_ridge_coefficient</th>\n",
                            "      <th>X~Y_ridge_coefficient</th>\n",
                            "      <th>loess_vx_slope</th>\n",
                            "      <th>loess_vy_slope</th>\n",
                            "      <th>loess_vx_intercept</th>\n",
                            "      <th>loess_vy_intercept</th>\n",
                            "      <th>loess_vx_residual_std</th>\n",
                            "      <th>loess_vy_residual_std</th>\n",
                            "      <th>label</th>\n",
                            "      <th>y</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>8</td>\n",
                            "      <td>0.169735</td>\n",
                            "      <td>-0.113595</td>\n",
                            "      <td>0.791467</td>\n",
                            "      <td>0.027355</td>\n",
                            "      <td>0.282376</td>\n",
                            "      <td>0.298969</td>\n",
                            "      <td>-0.771058</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.746844</td>\n",
                            "      <td>-0.210404</td>\n",
                            "      <td>0.191133</td>\n",
                            "      <td>-0.122925</td>\n",
                            "      <td>0.004577</td>\n",
                            "      <td>-0.000252</td>\n",
                            "      <td>0.562051</td>\n",
                            "      <td>0.565542</td>\n",
                            "      <td>Consequence of Y</td>\n",
                            "      <td>5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>0</td>\n",
                            "      <td>2</td>\n",
                            "      <td>8</td>\n",
                            "      <td>-0.072334</td>\n",
                            "      <td>0.139419</td>\n",
                            "      <td>0.230139</td>\n",
                            "      <td>0.003233</td>\n",
                            "      <td>0.090511</td>\n",
                            "      <td>0.082201</td>\n",
                            "      <td>-0.771058</td>\n",
                            "      <td>...</td>\n",
                            "      <td>-0.015475</td>\n",
                            "      <td>-0.210404</td>\n",
                            "      <td>-0.074108</td>\n",
                            "      <td>0.147385</td>\n",
                            "      <td>-0.001299</td>\n",
                            "      <td>-0.002253</td>\n",
                            "      <td>0.575814</td>\n",
                            "      <td>0.571669</td>\n",
                            "      <td>Independent</td>\n",
                            "      <td>6</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>8</td>\n",
                            "      <td>0.123115</td>\n",
                            "      <td>-0.238769</td>\n",
                            "      <td>0.781051</td>\n",
                            "      <td>0.012225</td>\n",
                            "      <td>0.257037</td>\n",
                            "      <td>0.288165</td>\n",
                            "      <td>-0.771058</td>\n",
                            "      <td>...</td>\n",
                            "      <td>-0.235369</td>\n",
                            "      <td>-0.210404</td>\n",
                            "      <td>0.141846</td>\n",
                            "      <td>-0.270144</td>\n",
                            "      <td>-0.002616</td>\n",
                            "      <td>0.007450</td>\n",
                            "      <td>0.571703</td>\n",
                            "      <td>0.560155</td>\n",
                            "      <td>Cause of Y</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>8</td>\n",
                            "      <td>-0.001935</td>\n",
                            "      <td>0.013921</td>\n",
                            "      <td>0.147408</td>\n",
                            "      <td>0.001935</td>\n",
                            "      <td>0.052451</td>\n",
                            "      <td>0.053457</td>\n",
                            "      <td>-0.771058</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.045268</td>\n",
                            "      <td>-0.210404</td>\n",
                            "      <td>0.002993</td>\n",
                            "      <td>0.010833</td>\n",
                            "      <td>-0.000937</td>\n",
                            "      <td>-0.002850</td>\n",
                            "      <td>0.576726</td>\n",
                            "      <td>0.576398</td>\n",
                            "      <td>Cause of Y</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>8</td>\n",
                            "      <td>0.284323</td>\n",
                            "      <td>-0.466570</td>\n",
                            "      <td>0.791467</td>\n",
                            "      <td>0.064815</td>\n",
                            "      <td>0.426306</td>\n",
                            "      <td>0.250048</td>\n",
                            "      <td>-0.771058</td>\n",
                            "      <td>...</td>\n",
                            "      <td>1.085383</td>\n",
                            "      <td>-0.210404</td>\n",
                            "      <td>0.312578</td>\n",
                            "      <td>-0.500287</td>\n",
                            "      <td>-0.008287</td>\n",
                            "      <td>0.010079</td>\n",
                            "      <td>0.550607</td>\n",
                            "      <td>0.507198</td>\n",
                            "      <td>Mediator</td>\n",
                            "      <td>7</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>142905</th>\n",
                            "      <td>46997</td>\n",
                            "      <td>6</td>\n",
                            "      <td>7</td>\n",
                            "      <td>0.014740</td>\n",
                            "      <td>0.697089</td>\n",
                            "      <td>0.748635</td>\n",
                            "      <td>0.013858</td>\n",
                            "      <td>0.376630</td>\n",
                            "      <td>0.352655</td>\n",
                            "      <td>-0.089641</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.000815</td>\n",
                            "      <td>-0.049793</td>\n",
                            "      <td>0.015583</td>\n",
                            "      <td>0.733127</td>\n",
                            "      <td>-0.000826</td>\n",
                            "      <td>-0.014398</td>\n",
                            "      <td>0.576368</td>\n",
                            "      <td>0.410596</td>\n",
                            "      <td>Cause of Y</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>142906</th>\n",
                            "      <td>46997</td>\n",
                            "      <td>7</td>\n",
                            "      <td>7</td>\n",
                            "      <td>-0.007397</td>\n",
                            "      <td>-0.116481</td>\n",
                            "      <td>0.998843</td>\n",
                            "      <td>0.007397</td>\n",
                            "      <td>0.180741</td>\n",
                            "      <td>0.333308</td>\n",
                            "      <td>-0.089641</td>\n",
                            "      <td>...</td>\n",
                            "      <td>-0.000532</td>\n",
                            "      <td>-0.049793</td>\n",
                            "      <td>-0.002825</td>\n",
                            "      <td>-0.131278</td>\n",
                            "      <td>-0.005518</td>\n",
                            "      <td>-0.003408</td>\n",
                            "      <td>0.574699</td>\n",
                            "      <td>0.573146</td>\n",
                            "      <td>Cause of Y</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>142907</th>\n",
                            "      <td>46997</td>\n",
                            "      <td>8</td>\n",
                            "      <td>7</td>\n",
                            "      <td>-0.011658</td>\n",
                            "      <td>0.908029</td>\n",
                            "      <td>0.970364</td>\n",
                            "      <td>0.011658</td>\n",
                            "      <td>0.469777</td>\n",
                            "      <td>0.441105</td>\n",
                            "      <td>-0.089641</td>\n",
                            "      <td>...</td>\n",
                            "      <td>-0.000334</td>\n",
                            "      <td>-0.049793</td>\n",
                            "      <td>-0.015487</td>\n",
                            "      <td>0.944120</td>\n",
                            "      <td>0.003218</td>\n",
                            "      <td>0.009358</td>\n",
                            "      <td>0.576956</td>\n",
                            "      <td>0.237622</td>\n",
                            "      <td>Cause of Y</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>142908</th>\n",
                            "      <td>46998</td>\n",
                            "      <td>0</td>\n",
                            "      <td>2</td>\n",
                            "      <td>0.083546</td>\n",
                            "      <td>-0.019665</td>\n",
                            "      <td>0.083546</td>\n",
                            "      <td>0.016856</td>\n",
                            "      <td>0.040022</td>\n",
                            "      <td>0.037719</td>\n",
                            "      <td>0.036862</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.044703</td>\n",
                            "      <td>0.120088</td>\n",
                            "      <td>0.081391</td>\n",
                            "      <td>-0.022989</td>\n",
                            "      <td>-0.005820</td>\n",
                            "      <td>-0.001575</td>\n",
                            "      <td>0.574716</td>\n",
                            "      <td>0.577094</td>\n",
                            "      <td>Consequence of X</td>\n",
                            "      <td>4</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>142909</th>\n",
                            "      <td>46998</td>\n",
                            "      <td>3</td>\n",
                            "      <td>2</td>\n",
                            "      <td>-0.201922</td>\n",
                            "      <td>0.666742</td>\n",
                            "      <td>0.666742</td>\n",
                            "      <td>0.016856</td>\n",
                            "      <td>0.295173</td>\n",
                            "      <td>0.334828</td>\n",
                            "      <td>0.036862</td>\n",
                            "      <td>...</td>\n",
                            "      <td>-0.114152</td>\n",
                            "      <td>0.120088</td>\n",
                            "      <td>-0.202515</td>\n",
                            "      <td>0.724177</td>\n",
                            "      <td>-0.009695</td>\n",
                            "      <td>-0.008161</td>\n",
                            "      <td>0.550647</td>\n",
                            "      <td>0.429517</td>\n",
                            "      <td>Collider</td>\n",
                            "      <td>2</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>142910 rows × 64 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "        dataset  variable  dimension  corr(v,X)  corr(v,Y)  \\\n",
                            "0             0         0          8   0.169735  -0.113595   \n",
                            "1             0         2          8  -0.072334   0.139419   \n",
                            "2             0         3          8   0.123115  -0.238769   \n",
                            "3             0         4          8  -0.001935   0.013921   \n",
                            "4             0         5          8   0.284323  -0.466570   \n",
                            "...         ...       ...        ...        ...        ...   \n",
                            "142905    46997         6          7   0.014740   0.697089   \n",
                            "142906    46997         7          7  -0.007397  -0.116481   \n",
                            "142907    46997         8          7  -0.011658   0.908029   \n",
                            "142908    46998         0          2   0.083546  -0.019665   \n",
                            "142909    46998         3          2  -0.201922   0.666742   \n",
                            "\n",
                            "        max(corr(v, others))  min(corr(v, others))  mean(corr(v, others))  \\\n",
                            "0                   0.791467              0.027355               0.282376   \n",
                            "1                   0.230139              0.003233               0.090511   \n",
                            "2                   0.781051              0.012225               0.257037   \n",
                            "3                   0.147408              0.001935               0.052451   \n",
                            "4                   0.791467              0.064815               0.426306   \n",
                            "...                      ...                   ...                    ...   \n",
                            "142905              0.748635              0.013858               0.376630   \n",
                            "142906              0.998843              0.007397               0.180741   \n",
                            "142907              0.970364              0.011658               0.469777   \n",
                            "142908              0.083546              0.016856               0.040022   \n",
                            "142909              0.666742              0.016856               0.295173   \n",
                            "\n",
                            "        std(corr(v, others))  corr(X,Y)  ...  v~X_ridge_coefficient  \\\n",
                            "0                   0.298969  -0.771058  ...               0.746844   \n",
                            "1                   0.082201  -0.771058  ...              -0.015475   \n",
                            "2                   0.288165  -0.771058  ...              -0.235369   \n",
                            "3                   0.053457  -0.771058  ...               0.045268   \n",
                            "4                   0.250048  -0.771058  ...               1.085383   \n",
                            "...                      ...        ...  ...                    ...   \n",
                            "142905              0.352655  -0.089641  ...               0.000815   \n",
                            "142906              0.333308  -0.089641  ...              -0.000532   \n",
                            "142907              0.441105  -0.089641  ...              -0.000334   \n",
                            "142908              0.037719   0.036862  ...               0.044703   \n",
                            "142909              0.334828   0.036862  ...              -0.114152   \n",
                            "\n",
                            "        X~Y_ridge_coefficient  loess_vx_slope  loess_vy_slope  \\\n",
                            "0                   -0.210404        0.191133       -0.122925   \n",
                            "1                   -0.210404       -0.074108        0.147385   \n",
                            "2                   -0.210404        0.141846       -0.270144   \n",
                            "3                   -0.210404        0.002993        0.010833   \n",
                            "4                   -0.210404        0.312578       -0.500287   \n",
                            "...                       ...             ...             ...   \n",
                            "142905              -0.049793        0.015583        0.733127   \n",
                            "142906              -0.049793       -0.002825       -0.131278   \n",
                            "142907              -0.049793       -0.015487        0.944120   \n",
                            "142908               0.120088        0.081391       -0.022989   \n",
                            "142909               0.120088       -0.202515        0.724177   \n",
                            "\n",
                            "        loess_vx_intercept  loess_vy_intercept  loess_vx_residual_std  \\\n",
                            "0                 0.004577           -0.000252               0.562051   \n",
                            "1                -0.001299           -0.002253               0.575814   \n",
                            "2                -0.002616            0.007450               0.571703   \n",
                            "3                -0.000937           -0.002850               0.576726   \n",
                            "4                -0.008287            0.010079               0.550607   \n",
                            "...                    ...                 ...                    ...   \n",
                            "142905           -0.000826           -0.014398               0.576368   \n",
                            "142906           -0.005518           -0.003408               0.574699   \n",
                            "142907            0.003218            0.009358               0.576956   \n",
                            "142908           -0.005820           -0.001575               0.574716   \n",
                            "142909           -0.009695           -0.008161               0.550647   \n",
                            "\n",
                            "        loess_vy_residual_std             label  y  \n",
                            "0                    0.565542  Consequence of Y  5  \n",
                            "1                    0.571669       Independent  6  \n",
                            "2                    0.560155        Cause of Y  1  \n",
                            "3                    0.576398        Cause of Y  1  \n",
                            "4                    0.507198          Mediator  7  \n",
                            "...                       ...               ... ..  \n",
                            "142905               0.410596        Cause of Y  1  \n",
                            "142906               0.573146        Cause of Y  1  \n",
                            "142907               0.237622        Cause of Y  1  \n",
                            "142908               0.577094  Consequence of X  4  \n",
                            "142909               0.429517          Collider  2  \n",
                            "\n",
                            "[142910 rows x 64 columns]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracting X_train, y_train, and group\n"
                    ]
                }
            ],
            "source": [
                "print(\"Adding numeric labels y\")\n",
                "le = LabelEncoder()\n",
                "X_y_group_train[\"y\"] = le.fit_transform(X_y_group_train[\"label\"])\n",
                "# reordering columns:\n",
                "X_y_group_train = X_y_group_train[[\"dataset\", \"variable\"] + X_y_group_train.columns.drop([\"dataset\", \"variable\", \"label\", \"y\"]).tolist() + [\"label\", \"y\"]]\n",
                "\n",
                "numeric_columns = X_y_group_train.select_dtypes(include=[np.number]).columns\n",
                "X_y_group_train[numeric_columns] = X_y_group_train[numeric_columns].fillna(X_y_group_train[numeric_columns].mean())\n",
                "\n",
                "blacklist = [\"ttest(v,X)\", \"pvalue(ttest(v,X))<=0.05\", \"ttest(v,Y)\", \"pvalue(ttest(v,Y))<=0.05\", \"ttest(X,Y)\", \"pvalue(ttest(X,Y))<=0.05\", \"max(energy_dist(v, others))\", \"min(energy_dist(v, others))\",\"mean(energy_dist(v, others))\", \"std(energy_dist(v, others))\", \"25%(energy_dist(v, others))\", \"75%(energy_dist(v, others))\"]\n",
                "columns_to_drop = [col for col in blacklist if col in X_y_group_train.columns]\n",
                "X_y_group_train = X_y_group_train.drop(columns=columns_to_drop)\n",
                "\n",
                "display(X_y_group_train)\n",
                "\n",
                "print(\"Extracting X_train, y_train, and group\")\n",
                "X_train = X_y_group_train.drop([\"variable\", \"dataset\", \"label\", \"y\"], axis=\"columns\")\n",
                "y_train = X_y_group_train[\"y\"]\n",
                "group_train = X_y_group_train[\"dataset\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:40:09.797137Z",
                    "start_time": "2024-09-18T09:39:33.165697Z"
                },
                "metadata": {}
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style>#sk-container-id-1 {\n",
                            "  /* Definition of color scheme common for light and dark mode */\n",
                            "  --sklearn-color-text: black;\n",
                            "  --sklearn-color-line: gray;\n",
                            "  /* Definition of color scheme for unfitted estimators */\n",
                            "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
                            "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
                            "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
                            "  --sklearn-color-unfitted-level-3: chocolate;\n",
                            "  /* Definition of color scheme for fitted estimators */\n",
                            "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
                            "  --sklearn-color-fitted-level-1: #d4ebff;\n",
                            "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
                            "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
                            "\n",
                            "  /* Specific color for light theme */\n",
                            "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
                            "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
                            "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
                            "  --sklearn-color-icon: #696969;\n",
                            "\n",
                            "  @media (prefers-color-scheme: dark) {\n",
                            "    /* Redefinition of color scheme for dark theme */\n",
                            "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
                            "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
                            "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
                            "    --sklearn-color-icon: #878787;\n",
                            "  }\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 pre {\n",
                            "  padding: 0;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 input.sk-hidden--visually {\n",
                            "  border: 0;\n",
                            "  clip: rect(1px 1px 1px 1px);\n",
                            "  clip: rect(1px, 1px, 1px, 1px);\n",
                            "  height: 1px;\n",
                            "  margin: -1px;\n",
                            "  overflow: hidden;\n",
                            "  padding: 0;\n",
                            "  position: absolute;\n",
                            "  width: 1px;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-dashed-wrapped {\n",
                            "  border: 1px dashed var(--sklearn-color-line);\n",
                            "  margin: 0 0.4em 0.5em 0.4em;\n",
                            "  box-sizing: border-box;\n",
                            "  padding-bottom: 0.4em;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-container {\n",
                            "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
                            "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
                            "     so we also need the `!important` here to be able to override the\n",
                            "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
                            "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
                            "  display: inline-block !important;\n",
                            "  position: relative;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-text-repr-fallback {\n",
                            "  display: none;\n",
                            "}\n",
                            "\n",
                            "div.sk-parallel-item,\n",
                            "div.sk-serial,\n",
                            "div.sk-item {\n",
                            "  /* draw centered vertical line to link estimators */\n",
                            "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
                            "  background-size: 2px 100%;\n",
                            "  background-repeat: no-repeat;\n",
                            "  background-position: center center;\n",
                            "}\n",
                            "\n",
                            "/* Parallel-specific style estimator block */\n",
                            "\n",
                            "#sk-container-id-1 div.sk-parallel-item::after {\n",
                            "  content: \"\";\n",
                            "  width: 100%;\n",
                            "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
                            "  flex-grow: 1;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-parallel {\n",
                            "  display: flex;\n",
                            "  align-items: stretch;\n",
                            "  justify-content: center;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "  position: relative;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-parallel-item {\n",
                            "  display: flex;\n",
                            "  flex-direction: column;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
                            "  align-self: flex-end;\n",
                            "  width: 50%;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
                            "  align-self: flex-start;\n",
                            "  width: 50%;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
                            "  width: 0;\n",
                            "}\n",
                            "\n",
                            "/* Serial-specific style estimator block */\n",
                            "\n",
                            "#sk-container-id-1 div.sk-serial {\n",
                            "  display: flex;\n",
                            "  flex-direction: column;\n",
                            "  align-items: center;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "  padding-right: 1em;\n",
                            "  padding-left: 1em;\n",
                            "}\n",
                            "\n",
                            "\n",
                            "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
                            "clickable and can be expanded/collapsed.\n",
                            "- Pipeline and ColumnTransformer use this feature and define the default style\n",
                            "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
                            "*/\n",
                            "\n",
                            "/* Pipeline and ColumnTransformer style (default) */\n",
                            "\n",
                            "#sk-container-id-1 div.sk-toggleable {\n",
                            "  /* Default theme specific background. It is overwritten whether we have a\n",
                            "  specific estimator or a Pipeline/ColumnTransformer */\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "}\n",
                            "\n",
                            "/* Toggleable label */\n",
                            "#sk-container-id-1 label.sk-toggleable__label {\n",
                            "  cursor: pointer;\n",
                            "  display: block;\n",
                            "  width: 100%;\n",
                            "  margin-bottom: 0;\n",
                            "  padding: 0.5em;\n",
                            "  box-sizing: border-box;\n",
                            "  text-align: center;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
                            "  /* Arrow on the left of the label */\n",
                            "  content: \"▸\";\n",
                            "  float: left;\n",
                            "  margin-right: 0.25em;\n",
                            "  color: var(--sklearn-color-icon);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "}\n",
                            "\n",
                            "/* Toggleable content - dropdown */\n",
                            "\n",
                            "#sk-container-id-1 div.sk-toggleable__content {\n",
                            "  max-height: 0;\n",
                            "  max-width: 0;\n",
                            "  overflow: hidden;\n",
                            "  text-align: left;\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-toggleable__content pre {\n",
                            "  margin: 0.2em;\n",
                            "  border-radius: 0.25em;\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
                            "  /* Expand drop-down */\n",
                            "  max-height: 200px;\n",
                            "  max-width: 100%;\n",
                            "  overflow: auto;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
                            "  content: \"▾\";\n",
                            "}\n",
                            "\n",
                            "/* Pipeline/ColumnTransformer-specific style */\n",
                            "\n",
                            "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  background-color: var(--sklearn-color-unfitted-level-2);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
                            "  background-color: var(--sklearn-color-fitted-level-2);\n",
                            "}\n",
                            "\n",
                            "/* Estimator-specific style */\n",
                            "\n",
                            "/* Colorize estimator box */\n",
                            "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-2);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-2);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
                            "#sk-container-id-1 div.sk-label label {\n",
                            "  /* The background is the default theme color */\n",
                            "  color: var(--sklearn-color-text-on-default-background);\n",
                            "}\n",
                            "\n",
                            "/* On hover, darken the color of the background */\n",
                            "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  background-color: var(--sklearn-color-unfitted-level-2);\n",
                            "}\n",
                            "\n",
                            "/* Label box, darken color on hover, fitted */\n",
                            "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  background-color: var(--sklearn-color-fitted-level-2);\n",
                            "}\n",
                            "\n",
                            "/* Estimator label */\n",
                            "\n",
                            "#sk-container-id-1 div.sk-label label {\n",
                            "  font-family: monospace;\n",
                            "  font-weight: bold;\n",
                            "  display: inline-block;\n",
                            "  line-height: 1.2em;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-label-container {\n",
                            "  text-align: center;\n",
                            "}\n",
                            "\n",
                            "/* Estimator-specific */\n",
                            "#sk-container-id-1 div.sk-estimator {\n",
                            "  font-family: monospace;\n",
                            "  border: 1px dotted var(--sklearn-color-border-box);\n",
                            "  border-radius: 0.25em;\n",
                            "  box-sizing: border-box;\n",
                            "  margin-bottom: 0.5em;\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-estimator.fitted {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-0);\n",
                            "}\n",
                            "\n",
                            "/* on hover */\n",
                            "#sk-container-id-1 div.sk-estimator:hover {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-2);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-2);\n",
                            "}\n",
                            "\n",
                            "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
                            "\n",
                            "/* Common style for \"i\" and \"?\" */\n",
                            "\n",
                            ".sk-estimator-doc-link,\n",
                            "a:link.sk-estimator-doc-link,\n",
                            "a:visited.sk-estimator-doc-link {\n",
                            "  float: right;\n",
                            "  font-size: smaller;\n",
                            "  line-height: 1em;\n",
                            "  font-family: monospace;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "  border-radius: 1em;\n",
                            "  height: 1em;\n",
                            "  width: 1em;\n",
                            "  text-decoration: none !important;\n",
                            "  margin-left: 1ex;\n",
                            "  /* unfitted */\n",
                            "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
                            "  color: var(--sklearn-color-unfitted-level-1);\n",
                            "}\n",
                            "\n",
                            ".sk-estimator-doc-link.fitted,\n",
                            "a:link.sk-estimator-doc-link.fitted,\n",
                            "a:visited.sk-estimator-doc-link.fitted {\n",
                            "  /* fitted */\n",
                            "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
                            "  color: var(--sklearn-color-fitted-level-1);\n",
                            "}\n",
                            "\n",
                            "/* On hover */\n",
                            "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
                            ".sk-estimator-doc-link:hover,\n",
                            "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
                            ".sk-estimator-doc-link:hover {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-3);\n",
                            "  color: var(--sklearn-color-background);\n",
                            "  text-decoration: none;\n",
                            "}\n",
                            "\n",
                            "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
                            ".sk-estimator-doc-link.fitted:hover,\n",
                            "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
                            ".sk-estimator-doc-link.fitted:hover {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-3);\n",
                            "  color: var(--sklearn-color-background);\n",
                            "  text-decoration: none;\n",
                            "}\n",
                            "\n",
                            "/* Span, style for the box shown on hovering the info icon */\n",
                            ".sk-estimator-doc-link span {\n",
                            "  display: none;\n",
                            "  z-index: 9999;\n",
                            "  position: relative;\n",
                            "  font-weight: normal;\n",
                            "  right: .2ex;\n",
                            "  padding: .5ex;\n",
                            "  margin: .5ex;\n",
                            "  width: min-content;\n",
                            "  min-width: 20ex;\n",
                            "  max-width: 50ex;\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  box-shadow: 2pt 2pt 4pt #999;\n",
                            "  /* unfitted */\n",
                            "  background: var(--sklearn-color-unfitted-level-0);\n",
                            "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
                            "}\n",
                            "\n",
                            ".sk-estimator-doc-link.fitted span {\n",
                            "  /* fitted */\n",
                            "  background: var(--sklearn-color-fitted-level-0);\n",
                            "  border: var(--sklearn-color-fitted-level-3);\n",
                            "}\n",
                            "\n",
                            ".sk-estimator-doc-link:hover span {\n",
                            "  display: block;\n",
                            "}\n",
                            "\n",
                            "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
                            "\n",
                            "#sk-container-id-1 a.estimator_doc_link {\n",
                            "  float: right;\n",
                            "  font-size: 1rem;\n",
                            "  line-height: 1em;\n",
                            "  font-family: monospace;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "  border-radius: 1rem;\n",
                            "  height: 1rem;\n",
                            "  width: 1rem;\n",
                            "  text-decoration: none;\n",
                            "  /* unfitted */\n",
                            "  color: var(--sklearn-color-unfitted-level-1);\n",
                            "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
                            "  /* fitted */\n",
                            "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
                            "  color: var(--sklearn-color-fitted-level-1);\n",
                            "}\n",
                            "\n",
                            "/* On hover */\n",
                            "#sk-container-id-1 a.estimator_doc_link:hover {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-3);\n",
                            "  color: var(--sklearn-color-background);\n",
                            "  text-decoration: none;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-3);\n",
                            "}\n",
                            "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=13, n_jobs=-1,\n",
                            "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=13, n_jobs=-1,\n",
                            "                       random_state=42)</pre></div> </div></div></div></div>"
                        ],
                        "text/plain": [
                            "RandomForestClassifier(class_weight='balanced', max_depth=13, n_jobs=-1,\n",
                            "                       random_state=42)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "multiclass balanced accuracy: mean=0.48664955476112337\n"
                    ]
                }
            ],
            "source": [
                "model = RandomForestClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=13,\n",
                "    n_jobs=-1,\n",
                "    class_weight=\"balanced\",\n",
                "    random_state=42\n",
                ")\n",
                "display(model)\n",
                "\n",
                "cv = GroupKFold(n_splits=4)\n",
                "results = cross_val_score(\n",
                "    model,\n",
                "    X_train,\n",
                "    y_train,\n",
                "    groups=group_train,\n",
                "    cv=cv,\n",
                "    verbose=True,\n",
                "    scoring=\"balanced_accuracy\"\n",
                ")\n",
                "\n",
                "print(f\"multiclass balanced accuracy: mean={results.mean()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "before:0.489\n",
                "after: 0.488, 差别不大...\n",
                "\n",
                "0.488120992009518"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import cross_val_predict\n",
                "from sklearn.metrics import balanced_accuracy_score\n",
                "\n",
                "\n",
                "cv = GroupKFold(n_splits=4)\n",
                "\n",
                "# 使用cross_val_predict获取每个折叠的预测结果\n",
                "y_pred = cross_val_predict(model, X_train, y_train, groups=group_train, cv=cv, n_jobs=-1)\n",
                "\n",
                "# 计算每个折叠的平衡准确率\n",
                "fold_scores = []\n",
                "for train_index, val_index in cv.split(X_train, y_train, groups=group_train):\n",
                "    y_true_fold = y_train[val_index]\n",
                "    y_pred_fold = y_pred[val_index]\n",
                "    fold_score = balanced_accuracy_score(y_true_fold, y_pred_fold)\n",
                "    fold_scores.append(fold_score)\n",
                "\n",
                "# 找出最佳折叠的索引\n",
                "best_fold_index = fold_scores.index(max(fold_scores))\n",
                "\n",
                "# 在最佳折叠上训练模型\n",
                "train_index, val_index = list(cv.split(X_train, y_train, groups=group_train))[best_fold_index]\n",
                "X_train_best, X_val_best = X_train[train_index], X_train[val_index]\n",
                "y_train_best, y_val_best = y_train[train_index], y_train[val_index]\n",
                "\n",
                "best_model = RandomForestClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=13,\n",
                "    n_jobs=-1,\n",
                "    class_weight=\"balanced\",\n",
                "    random_state=42\n",
                ")\n",
                "best_model.fit(X_train_best, y_train_best)\n",
                "\n",
                "# 保存最佳模型\n",
                "import joblib\n",
                "joblib.dump(best_model, 'best_random_forest_model.joblib')\n",
                "\n",
                "print(f\"Best fold balanced accuracy: {max(fold_scores)}\")\n",
                "print(f\"Average balanced accuracy: {sum(fold_scores) / len(fold_scores)}\")\n",
                "print(\"Best model saved as 'best_random_forest_model.joblib'\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "\n",
                "X = X_train\n",
                "y = y_train\n",
                "# Assuming X and y are your features and target variables\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Fit the model\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# Get predictions\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "\n",
                "# Calculate accuracy for each dimension\n",
                "dimensions = sorted(X_test['dimension'].unique())\n",
                "accuracies = []\n",
                "\n",
                "for dim in dimensions:\n",
                "    mask = X_test['dimension'] == dim\n",
                "    y_true_dim = y_test[mask]\n",
                "    y_pred_dim = y_pred[mask]\n",
                "    acc = accuracy_score(y_true_dim, y_pred_dim)\n",
                "    accuracies.append(acc)\n",
                "\n",
                "# Visualize the results\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(dimensions, accuracies, marker='o')\n",
                "plt.xlabel('Dimension')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.title('Model Accuracy vs Dimension')\n",
                "plt.grid(True)\n",
                "plt.xticks(dimensions)  # Ensure all dimensions are shown on x-axis\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print accuracies\n",
                "print(\"Accuracies for each dimension:\")\n",
                "for dim, acc in zip(dimensions, accuracies):\n",
                "    print(f\"Dimension {dim}: Accuracy = {acc:.4f}\")\n",
                "\n",
                "# Calculate and print overall accuracy\n",
                "overall_accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")\n",
                "\n",
                "# Get feature importances\n",
                "importances = model.feature_importances_\n",
                "feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n",
                "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
                "\n",
                "# Display top 20 most important features\n",
                "print(\"\\nTop 20 most important features:\")\n",
                "print(feature_importances)\n",
                "\n",
                "# Save the model\n",
                "import joblib\n",
                "joblib.dump(model, './resources/random_forest_model_v1.joblib')\n",
                "\n",
                "print(\"\\nModel and feature importances have been saved.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import balanced_accuracy_score\n",
                "from pytorch_tabnet.tab_model import TabNetClassifier\n",
                "import torch\n",
                "import re\n",
                "# 假设 X_train, y_train 已经准备好\n",
                "\n",
                "# 清理特征名称\n",
                "def clean_feature_names(X):\n",
                "    def clean_name(name):\n",
                "        name = re.sub(r'[^\\w\\s-]', '_', name)\n",
                "        if name[0].isdigit():\n",
                "            name = 'f_' + name\n",
                "        return name\n",
                "    \n",
                "    X.columns = [clean_name(col) for col in X.columns]\n",
                "    return X\n",
                "\n",
                "X_train = clean_feature_names(X_train)\n",
                "\n",
                "# 划分训练集和验证集\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
                "\n",
                "# 创建TabNet模型\n",
                "tabnet_model = TabNetClassifier(\n",
                "    n_d=64,  # 决策步骤的维度\n",
                "    n_a=64,  # 注意力步骤的维度\n",
                "    n_steps=5,  # 总共的决策步骤数\n",
                "    gamma=1.5,  # 控制稀疏性的参数\n",
                "    n_independent=2,  # 每个步骤的独立层数\n",
                "    n_shared=2,  # 共享层数\n",
                "    cat_idxs=[],  # 分类特征的索引\n",
                "    cat_dims=[],  # 每个分类特征的唯一值数量\n",
                "    cat_emb_dim=[],  # 每个分类特征的嵌入维度\n",
                "    lambda_sparse=1e-3,  # L1正则化参数\n",
                "    optimizer_fn=torch.optim.Adam,\n",
                "    optimizer_params=dict(lr=2e-2),\n",
                "    scheduler_params=dict(\n",
                "        mode=\"min\", patience=5, min_lr=1e-5, factor=0.5\n",
                "    ),\n",
                "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
                "    mask_type=\"entmax\",  # \"sparsemax\" or \"entmax\"\n",
                "    device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
                ")\n",
                "\n",
                "# 训练模型\n",
                "tabnet_model.fit(\n",
                "    X_train=X_train.values, \n",
                "    y_train=y_train.values,\n",
                "    eval_set=[(X_val.values, y_val.values)],\n",
                "    eval_name=['val'],\n",
                "    eval_metric=['balanced_accuracy'],\n",
                "    max_epochs=100,\n",
                "    patience=10,\n",
                "    batch_size=1024,\n",
                "    virtual_batch_size=256,\n",
                "    num_workers=0,\n",
                "    weights=1,\n",
                "    drop_last=False\n",
                ")\n",
                "\n",
                "# 打印训练结果\n",
                "print(f\"Best epoch: {tabnet_model.best_epoch}\")\n",
                "print(f\"Best validation balanced accuracy: {tabnet_model.best_cost}\")\n",
                "\n",
                "# 在训练集和验证集上评估模型\n",
                "y_train_pred = tabnet_model.predict(X_train.values)\n",
                "y_val_pred = tabnet_model.predict(X_val.values)\n",
                "\n",
                "train_accuracy = balanced_accuracy_score(y_train, y_train_pred)\n",
                "val_accuracy = balanced_accuracy_score(y_val, y_val_pred)\n",
                "\n",
                "print(f\"Train balanced accuracy: {train_accuracy:.4f}\")\n",
                "print(f\"Validation balanced accuracy: {val_accuracy:.4f}\")\n",
                "\n",
                "# 保存模型\n",
                "import joblib\n",
                "# joblib.dump(tabnet_model, './resources/tabnet_model_v0.joblib')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "def clean_feature_names(X):\n",
                "    # 函数用于清理特征名称\n",
                "    def clean_name(name):\n",
                "        # 移除或替换特殊字符\n",
                "        name = re.sub(r'[^\\w\\s-]', '_', name)\n",
                "        # 确保名称不以数字开头\n",
                "        if name[0].isdigit():\n",
                "            name = 'f_' + name\n",
                "        return name\n",
                "\n",
                "    X.columns = [clean_name(col) for col in X.columns]\n",
                "    return X\n",
                "\n",
                "# 清理特征名称\n",
                "X_train = clean_feature_names(X_train)\n",
                "\n",
                "\n",
                "from lightgbm import LGBMClassifier\n",
                "import lightgbm as lgb\n",
                "model = LGBMClassifier(\n",
                "    n_estimators=2000,\n",
                "    learning_rate=0.03,\n",
                "    max_depth=6,\n",
                "    num_leaves=29,\n",
                "    min_child_samples=20,\n",
                "    subsample=0.7,\n",
                "    colsample_bytree=0.7,\n",
                "    reg_alpha=0.1,\n",
                "    reg_lambda=0.1,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    class_weight='balanced',\n",
                "    device='gpu',\n",
                "    gpu_platform_id=1,\n",
                "    gpu_device_id=0,\n",
                ")\n",
                "display(model)\n",
                "\n",
                "from sklearn.model_selection import GroupKFold\n",
                "from sklearn.metrics import balanced_accuracy_score\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "\n",
                "cv = GroupKFold(n_splits=4)\n",
                "train_scores = []\n",
                "val_scores = []\n",
                "\n",
                "for train_idx, val_idx in cv.split(X_train, y_train, groups=group_train):\n",
                "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
                "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
                "    \n",
                "    callbacks = [lgb.log_evaluation(period=1), lgb.early_stopping(stopping_rounds=10)]\n",
                "    \n",
                "    model.fit(X_train_fold, y_train_fold, callbacks=callbacks, eval_set=[(X_val_fold, y_val_fold)])\n",
                "\n",
                "    \n",
                "    y_train_pred = model.predict(X_train_fold)\n",
                "    y_val_pred = model.predict(X_val_fold)\n",
                "    \n",
                "    train_score = balanced_accuracy_score(y_train_fold, y_train_pred)\n",
                "    val_score = balanced_accuracy_score(y_val_fold, y_val_pred)\n",
                "    \n",
                "    train_scores.append(train_score)\n",
                "    val_scores.append(val_score)\n",
                "    \n",
                "    print(f\"Fold train balanced accuracy: {train_score:.4f}\")\n",
                "    print(f\"Fold validation balanced accuracy: {val_score:.4f}\")\n",
                "    print(\"---\")\n",
                "\n",
                "print(f\"Average train balanced accuracy: {np.mean(train_scores):.4f} ± {np.std(train_scores):.4f}\")\n",
                "print(f\"Average validation balanced accuracy: {np.mean(val_scores):.4f} ± {np.std(val_scores):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment what you need!\n",
                "def train(\n",
                "    X_train: typing.Dict[str, pd.DataFrame],\n",
                "    y_train: typing.Dict[str, pd.DataFrame],\n",
                "    # number_of_features: int,\n",
                "    model_directory_path: str,\n",
                "    # id_column_name: str,\n",
                "    # prediction_column_name: str,\n",
                "    # has_gpu: bool,\n",
                ") -> None:\n",
                "    # 1) Create the variable embedding:\n",
                "    return\n",
                "    X_y_group_train_pathname = \"/tmp/X_y_group_train.parquet\"\n",
                "    try:\n",
                "        print(f\"Loading {X_y_group_train_pathname}\")\n",
                "        X_y_group_train = pd.read_parquet(X_y_group_train_pathname)\n",
                "    except FileNotFoundError:\n",
                "        print(\"Creating X_y_group_train\")\n",
                "        names_datasets_train = X_train\n",
                "        names_graphs_train = y_train\n",
                "        X_y_group_train = create_all_columns(\n",
                "            {\n",
                "                pearson_correlation: names_datasets_train,\n",
                "                ttest: names_datasets_train,\n",
                "                mutual_information: names_datasets_train,  # comment this line to greatly reduce computation\n",
                "                label: names_graphs_train,\n",
                "                spearman_correlation: names_datasets_train,\n",
                "                kendall_correlation: names_datasets_train,\n",
                "                distance_correlation: names_datasets_train,\n",
                "                conditional_mutual_information: names_datasets_train,\n",
                "            },\n",
                "            n_jobs=-1,\n",
                "        )\n",
                "        X_y_group_train = remove_outliers(X_y_group_train, ['corr(v,X)', 'corr(v,Y)', 'ttest(v,X)', 'ttest(v,Y)'])\n",
                "        # 2) Massage X_y_group_train to prepare what is needed by the model:\n",
                "        print(\"Adding numeric labels y\") # sklearn wants numeric labels\n",
                "        le = LabelEncoder()\n",
                "        le.classes_ = np.array([\n",
                "            'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
                "            'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
                "        ])\n",
                "        X_y_group_train[\"y\"] = le.transform(X_y_group_train[\"label\"])\n",
                "\n",
                "        # reordering columns:\n",
                "        X_y_group_train = X_y_group_train[[\"dataset\", \"variable\"] + X_y_group_train.columns.drop([\"dataset\", \"variable\", \"label\", \"y\"]).tolist() + [\"label\", \"y\"]]\n",
                "        display(X_y_group_train)\n",
                "\n",
                "        print(\"Saving X_y_group_train\")\n",
                "        os.makedirs(os.path.dirname(X_y_group_train_pathname), exist_ok=True)\n",
                "        X_y_group_train.to_parquet(X_y_group_train_pathname)\n",
                "\n",
                "    print(\"Extracting X_train, y_train, and group\")\n",
                "    X_train = X_y_group_train.drop([\"variable\", \"dataset\", \"label\", \"y\"], axis=\"columns\")\n",
                "    y_train = X_y_group_train[\"y\"]\n",
                "    group_train = X_y_group_train[\"dataset\"]\n",
                "\n",
                "    # Model:\n",
                "    model = RandomForestClassifier(n_estimators=100, max_depth=11, n_jobs=-1, class_weight=\"balanced\")\n",
                "    model.fit(X_train, y_train)\n",
                "\n",
                "    joblib.dump(\n",
                "        model,\n",
                "        os.path.join(model_directory_path, \"model.joblib\")\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_submission(X_y_pred_test):\n",
                "    \"\"\"\n",
                "    From the predicted test set, for each dataset, take predicted\n",
                "    classes of all variables, create the adjacency matrix, then create\n",
                "    the submission in the requested format.\n",
                "    \"\"\"\n",
                "\n",
                "    submission = {}\n",
                "    for name, prediction in tqdm(X_y_pred_test.groupby(\"dataset\"), delay=10):\n",
                "        variables_labels = prediction[[\"variable\", \"label_predicted\"]].set_index(\"variable\")\n",
                "        variables = variables_labels.index.tolist()\n",
                "        variables_all = [\"X\", \"Y\"] + variables\n",
                "\n",
                "        adjacency_matrix = pd.DataFrame(index=variables_all, columns=variables_all)\n",
                "        adjacency_matrix.index.name = \"parent\"\n",
                "        adjacency_matrix[:] = 0\n",
                "        adjacency_matrix.loc[\"X\", \"Y\"] = 1\n",
                "\n",
                "        for v in variables:\n",
                "            l = variables_labels.loc[v].item()\n",
                "            if l == \"Cause of X\":\n",
                "                adjacency_matrix.loc[v, \"X\"] = 1\n",
                "            elif l == \"Cause of Y\":\n",
                "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
                "            elif l == \"Consequence of X\":\n",
                "                adjacency_matrix.loc[\"X\", v] = 1\n",
                "            elif l == \"Consequence of Y\":\n",
                "                adjacency_matrix.loc[\"Y\", v] = 1\n",
                "            elif l == \"Confounder\":\n",
                "                adjacency_matrix.loc[v, \"X\"] = 1\n",
                "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
                "            elif l == \"Collider\":\n",
                "                adjacency_matrix.loc[\"X\", v] = 1\n",
                "                adjacency_matrix.loc[\"Y\", v] = 1\n",
                "            elif l == \"Mediator\":\n",
                "                adjacency_matrix.loc[\"X\", v] = 1\n",
                "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
                "            elif l == \"Confounder\":\n",
                "                pass\n",
                "\n",
                "        for i in variables_all:\n",
                "            for j in variables_all:\n",
                "                submission[f'{name}_{i}_{j}'] = int(adjacency_matrix.loc[i, j])\n",
                "\n",
                "    return submission\n",
                "\n",
                "\n",
                "# Uncomment what you need!\n",
                "def infer(\n",
                "    X_test: typing.Dict[str, pd.DataFrame],\n",
                "    # number_of_features: int,\n",
                "    model_directory_path: str,\n",
                "    id_column_name: str,\n",
                "    prediction_column_name: str,\n",
                "    # has_gpu: bool,\n",
                "    # has_trained: bool,\n",
                ") -> pd.DataFrame:\n",
                "    model = joblib.load(os.path.join(model_directory_path, \"random_forest_model_v0.joblib\"))\n",
                "\n",
                "    names_datasets_test = X_test\n",
                "    X_group_test = create_all_columns(\n",
                "        {\n",
                "                pearson_correlation: names_datasets_train,\n",
                "                ttest: names_datasets_train,\n",
                "                mutual_information: names_datasets_train,  # comment this line to greatly reduce computation\n",
                "                label: names_graphs_train,\n",
                "                spearman_correlation: names_datasets_train,\n",
                "                kendall_correlation: names_datasets_train,\n",
                "                distance_correlation: names_datasets_train,\n",
                "                conditional_mutual_information: names_datasets_train,\n",
                "        },\n",
                "        n_jobs=-1,\n",
                "    )\n",
                "\n",
                "    X_test = X_group_test.drop(columns=[\"dataset\", \"variable\"])\n",
                "    y_predicted = model.predict(X_test)\n",
                "    X_y_pred_test = X_group_test\n",
                "    X_y_pred_test[\"y_predicted\"] = y_predicted\n",
                "\n",
                "    le = LabelEncoder()\n",
                "    le.classes_ = np.array([\n",
                "        'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
                "        'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
                "    ])\n",
                "\n",
                "    X_y_pred_test[\"label_predicted\"] = le.inverse_transform(y_predicted)\n",
                "\n",
                "    submission = create_submission(X_y_pred_test)\n",
                "\n",
                "    return pd.DataFrame(\n",
                "        submission.items(),\n",
                "        columns=[\n",
                "            id_column_name,\n",
                "            prediction_column_name\n",
                "        ]\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "crunch.test(\n",
                "    no_determinism_check=True\n",
                ")\n",
                "\n",
                "print(\"Download this notebook and submit it to the platform: https://hub.crunchdao.com/competitions/causality-discovery/submit/via/notebook\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "casual",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
