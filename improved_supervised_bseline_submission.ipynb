{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import os\n",
    "import typing\n",
    "\n",
    "import joblib\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dcor\n",
    "from scipy.stats import pearsonr, ttest_rel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.utils import check_random_state\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "from scipy.stats import entropy\n",
    "import statsmodels.api as sm\n",
    "import pingouin as pg\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n"
     ]
    }
   ],
   "source": [
    "import crunch\n",
    "crunch = crunch.load_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_nodes_representation(graph, nodelist):\n",
    "    \"\"\"\n",
    "    Create an alternative representation of a graph which is hashable\n",
    "    and equivalent graphs have the same hash.\n",
    "\n",
    "    Python cannot PROPERLY use nx.Graph/DiGraph as key for\n",
    "    dictionaries, because two equivalent graphs with just different\n",
    "    order of the nodes would result in different keys. This is\n",
    "    undesirable here.\n",
    "\n",
    "    So here we transform the graph into an equivalent form that is\n",
    "    based on a specific nodelist and that is hashable. In this way,\n",
    "    two equivalent graphs, once transformed, will result in identical\n",
    "    keys.\n",
    "\n",
    "    So we use the following trick: extract the adjacency matrix\n",
    "    (with nodes in a fixed order) and then make a hashable thing out\n",
    "    of it, through tuple(array.flatten()):\n",
    "    \"\"\"\n",
    "\n",
    "    # This get the adjacency matrix with nodes in a given order, as\n",
    "    # numpy array (which is not hashable):\n",
    "    adjacency_matrix = nx.adjacency_matrix(graph, nodelist=nodelist).todense()\n",
    "\n",
    "    # This transforms the numpy array into a hashable object:\n",
    "    hashable = tuple(adjacency_matrix.flatten())\n",
    "\n",
    "    return hashable\n",
    "\n",
    "def create_graph_label():\n",
    "    \"\"\"\n",
    "    Create a dictionary from graphs to labels, in two formats.\n",
    "    \"\"\"\n",
    "    graph_label = {\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]):             \"Cause of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]):             \"Cause of Y\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]):             \"Consequence of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]):             \"Consequence of Y\",\n",
    "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}):                \"Independent\",\n",
    "    }\n",
    "\n",
    "    nodelist = [\"v\", \"X\", \"Y\"]\n",
    "\n",
    "    # This is an equivalent alternative to graph_label but in a form\n",
    "    # for which two equivalent graphs have the same key:\n",
    "    adjacency_label = {\n",
    "        graph_nodes_representation(graph, nodelist): label\n",
    "        for graph, label in graph_label.items()\n",
    "    }\n",
    "\n",
    "    return graph_label, adjacency_label\n",
    "\n",
    "def get_labels(adjacency_matrix, adjacency_label):\n",
    "    \"\"\"\n",
    "    Transform an adjacency_matrix (as pd.DataFrame) into a dictionary of variable:label\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
    "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]  # this is not hashable\n",
    "        key = tuple(submatrix.values.flatten())  # this is hashable and a compatible with adjacency_label\n",
    "    \n",
    "        result[variable] = adjacency_label[key]\n",
    "\n",
    "    return result\n",
    "\n",
    "graph_label, adjacency_label = create_graph_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download data\\X_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_train.pickle (1523944532 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_train.pickle (7017735 bytes)\n",
      "already exists: file length match\n",
      "download data\\X_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_test_reduced.pickle (122341879 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_test_reduced.pickle (562930 bytes)\n",
      "already exists: file length match\n",
      "download data\\example_prediction.parquet from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/example_prediction_reduced.parquet (668981 bytes)\n",
      "already exists: file length match\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test = crunch.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the correlation-based features for each\n",
    "    varibale, which are the correlation between that variable with X and Y,\n",
    "    as well as summary statistics (max, min, mean, std) of all pairs\n",
    "    of correlations.\n",
    "    \"\"\"\n",
    "\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = dataset.corr().drop([variable], axis=\"columns\").loc[variable].abs()\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"corr(v,X)\": dataset[[variable, \"X\"]].corr().loc[variable, \"X\"],\n",
    "            \"corr(v,Y)\": dataset[[variable, \"Y\"]].corr().loc[variable, \"Y\"],\n",
    "            \"max(corr(v, others))\": tmp.max(),\n",
    "            \"min(corr(v, others))\": tmp.min(),\n",
    "            \"mean(corr(v, others))\": tmp.mean(),\n",
    "            \"std(corr(v, others))\": tmp.std(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr().loc[\"X\", \"Y\"]\n",
    "\n",
    "    # pearsonr is NaN when the variance is 0, so we fill with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def ttest(dataset, pvalue_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Given a dataset, this function computes the t-test between the\n",
    "    values each variable v and X, Y. The t value and the result of the\n",
    "    t-test with a given pvalue_threshold, are used to create features\n",
    "    to describe/embed v, as well as the t-test result between the\n",
    "    values of X and Y.\n",
    "    \"\"\"\n",
    "\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        ttest_vX = ttest_rel(dataset[variable], dataset[\"X\"])\n",
    "        ttest_vY = ttest_rel(dataset[variable], dataset[\"Y\"])\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"ttest(v,X)\": ttest_vX.statistic,\n",
    "            f\"pvalue(ttest(v,X))<={pvalue_threshold}\": (ttest_vX.pvalue <= pvalue_threshold).astype(float),\n",
    "            \"ttest(v,Y)\": ttest_vY.statistic,\n",
    "            f\"pvalue(ttest(v,Y))<={pvalue_threshold}\": (ttest_vY.pvalue <= pvalue_threshold).astype(float),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    ttest_XY = ttest_rel(dataset[\"X\"], dataset[\"Y\"])\n",
    "    df[\"ttest(X,Y)\"] = ttest_XY.statistic\n",
    "    df[f\"pvalue(ttest(X,Y))<={pvalue_threshold}\"] = (ttest_XY.pvalue <= pvalue_threshold).astype(float)\n",
    "\n",
    "    # some the ttest returns NaN when the variance is 0, so we fill with 0:\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def mutual_information(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the mutual-information-based features\n",
    "    for each varibale, which are the correlation between that variable\n",
    "    X and Y, as well as summary statistics (max, min, mean, std) of\n",
    "    all pairs of correlations.\n",
    "    \"\"\"\n",
    "\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = mutual_info_regression(dataset.drop(columns=[variable]), dataset[variable])\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"MI(v,X)\": mutual_info_regression(dataset[[variable]], dataset[\"X\"], discrete_features=False)[0],\n",
    "            \"MI(v,Y)\": mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0],\n",
    "            \"max(MI(v, others))\": tmp.max(),\n",
    "            \"min(MI(v, others))\": tmp.min(),\n",
    "            \"mean(MI(v, others))\": tmp.mean(),\n",
    "            \"std(MI(v, others))\": tmp.std(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"MI(X,Y)\"] = mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_dimension_feature(dataset):\n",
    "    \"\"\"\n",
    "    Add a dimension feature to the dataset.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "    dimension = len(variables)\n",
    "    square_dimension = dimension * dimension\n",
    "    df = pd.DataFrame({\n",
    "        \"variable\": variables,\n",
    "        \"dimension\": dimension,\n",
    "        \"square_dimension\": square_dimension\n",
    "    })\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    return df\n",
    "\n",
    "def spearman_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the Spearman rank correlation-based features for each\n",
    "    variable, which are the Spearman correlation between that variable with X and Y,\n",
    "    as well as summary statistics (max, min, mean, std) of all pairs of Spearman correlations.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = dataset.corr(method='spearman').drop([variable], axis=\"columns\").loc[variable].abs()\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"spearman_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='spearman').loc[variable, \"X\"],\n",
    "            \"spearman_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='spearman').loc[variable, \"Y\"],\n",
    "            \"max(spearman_corr(v, others))\": tmp.max(),\n",
    "            \"min(spearman_corr(v, others))\": tmp.min(),\n",
    "            \"mean(spearman_corr(v, others))\": tmp.mean(),\n",
    "            \"std(spearman_corr(v, others))\": tmp.std(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"spearman_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='spearman').loc[\"X\", \"Y\"]\n",
    "\n",
    "    # Spearman correlation is NaN when there are ties in rank, so we fill with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def kendall_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the Kendall's tau correlation-based features for each\n",
    "    variable, which are the Kendall's tau correlation between that variable with X and Y,\n",
    "    as well as summary statistics (max, min, mean, std) of all pairs of Kendall's tau correlations.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = dataset.corr(method='kendall').drop([variable], axis=\"columns\").loc[variable].abs()\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"kendall_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='kendall').loc[variable, \"X\"],\n",
    "            \"kendall_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='kendall').loc[variable, \"Y\"],\n",
    "            \"max(kendall_corr(v, others))\": tmp.max(),\n",
    "            \"min(kendall_corr(v, others))\": tmp.min(),\n",
    "            \"mean(kendall_corr(v, others))\": tmp.mean(),\n",
    "            \"std(kendall_corr(v, others))\": tmp.std(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"kendall_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='kendall').loc[\"X\", \"Y\"]\n",
    "\n",
    "    # Kendall's tau correlation can be NaN in some cases, so we fill with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def distance_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the distance correlation-based features for each\n",
    "    variable, which are the distance correlation between that variable with X and Y,\n",
    "    as well as the distance correlation between X and Y.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # todo:尝试不同的exponent（欧氏距离的系数）\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"dcor(v,X)\": dcor.distance_correlation(dataset[variable], dataset[\"X\"]),\n",
    "            \"dcor(v,Y)\": dcor.distance_correlation(dataset[variable], dataset[\"Y\"]),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"dcor(X,Y)\"] = dcor.distance_correlation(dataset[\"X\"], dataset[\"Y\"])\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def distance_correlation_v2(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the distance correlation-based features for each\n",
    "    variable, which are the distance correlation between that variable with X and Y,\n",
    "    as well as the distance correlation between X and Y.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    def distcorr(X, Y):\n",
    "        X = np.atleast_1d(X)\n",
    "        Y = np.atleast_1d(Y)\n",
    "        if np.prod(X.shape) == len(X):\n",
    "            X = X[:, None]\n",
    "        if np.prod(Y.shape) == len(Y):\n",
    "            Y = Y[:, None]\n",
    "        X = np.atleast_2d(X)\n",
    "        Y = np.atleast_2d(Y)\n",
    "        n = X.shape[0]\n",
    "        if Y.shape[0] != X.shape[0]:\n",
    "            raise ValueError('Number of samples must match')\n",
    "        a = squareform(pdist(X))\n",
    "        b = squareform(pdist(Y))\n",
    "        A = a - a.mean(axis=0)[None, :] - a.mean(axis=1)[:, None] + a.mean()\n",
    "        B = b - b.mean(axis=0)[None, :] - b.mean(axis=1)[:, None] + b.mean()\n",
    "        \n",
    "        dcov2_xy = (A * B).sum()/float(n * n)\n",
    "        dcov2_xx = (A * A).sum()/float(n * n)\n",
    "        dcov2_yy = (B * B).sum()/float(n * n)\n",
    "        dcor = np.sqrt(dcov2_xy)/np.sqrt(np.sqrt(dcov2_xx) * np.sqrt(dcov2_yy))\n",
    "        return dcor\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"dcor(v,X)_v2\": distcorr(dataset[variable], dataset[\"X\"]),\n",
    "            \"dcor(v,Y)_v2\": distcorr(dataset[variable], dataset[\"Y\"]),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"dcor(X,Y)_v2\"] = distcorr(dataset[\"X\"], dataset[\"Y\"])\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def linear_regression_feature(dataset):\n",
    "    def Squared_term(dataset, variables):\n",
    "        for var in variables:\n",
    "            dataset[f'{var}_squared_term'] = dataset[var] ** 2\n",
    "        return dataset\n",
    "\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "\n",
    "    # model1: Fit X, v, v^2 ~ Y\n",
    "    model1_features = [\"X\"] + variables\n",
    "    d1 = Squared_term(dataset[model1_features], variables)\n",
    "    model1_features = d1.columns.tolist()\n",
    "    scaler = StandardScaler()\n",
    "    d1_scaled = scaler.fit_transform(d1)\n",
    "    model1 = LinearRegression().fit(d1_scaled, dataset[[\"Y\"]])\n",
    "    model1_coefs = model1.coef_[0].tolist()\n",
    "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
    "    \n",
    "    # model2: Fit v, v^2 ~ X\n",
    "    model2_features = variables\n",
    "    d2 = Squared_term(dataset[model2_features], variables)\n",
    "    model2_features = d2.columns.tolist()\n",
    "    scaler = StandardScaler()\n",
    "    d2_scaled = scaler.fit_transform(d2)\n",
    "    model2 = LinearRegression().fit(d2_scaled, dataset[[\"X\"]])\n",
    "    model2_coefs = model2.coef_[0].tolist()\n",
    "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
    "    \n",
    "    df = []\n",
    "    for i, variable in enumerate(variables):\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"v~Y_coefficient\": model1_dict[variable],\n",
    "            \"v_squared~Y_coefficient\": model1_dict[f\"{variable}_squared_term\"],\n",
    "            \"v~X_coefficient\": model2_dict[variable],\n",
    "            \"v_squared~X_coefficient\": model2_dict[f\"{variable}_squared_term\"],\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    df[\"X~Y_coefficient\"] = model1_dict[\"X\"]\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def ridge_regression_feature(dataset):\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
    "    # 使用GridSearchCV来选择最佳的alpha值\n",
    "    param_grid = {'alpha': np.logspace(-6, 6, 13)}\n",
    "    \n",
    "    # model1: Fit X, v ~ Y\n",
    "    model1_features = [\"X\"] + variables\n",
    "    scaler1 = StandardScaler()\n",
    "    d1_scaled = scaler1.fit_transform(dataset[model1_features])\n",
    "    model1 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
    "    model1.fit(d1_scaled, dataset[\"Y\"])\n",
    "    model1_coefs = model1.best_estimator_.coef_.tolist()\n",
    "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
    "    \n",
    "    # model2: Fit v ~ X\n",
    "    model2_features = variables\n",
    "    scaler2 = StandardScaler()\n",
    "    d2_scaled = scaler2.fit_transform(dataset[model2_features])\n",
    "    model2 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
    "    model2.fit(d2_scaled, dataset[\"X\"])\n",
    "    model2_coefs = model2.best_estimator_.coef_.tolist()\n",
    "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
    "    \n",
    "    df = []\n",
    "    for i, variable in enumerate(variables):\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"v~Y_ridge_coefficient\": model1_dict[variable],\n",
    "            \"v~X_ridge_coefficient\": model2_dict[variable]\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    df[\"X~Y_ridge_coefficient\"] = model1_dict[\"X\"]\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def conditional_mutual_information(dataset):\n",
    "    \"\"\"\n",
    "    Calculate conditional mutual information for each variable with X and Y.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "    \n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # Calculate conditional MI(v, X | Y)\n",
    "        mi_vx_given_y = mutual_info_regression(dataset[[variable, \"Y\"]], dataset[\"X\"], discrete_features=False)[0] - \\\n",
    "                        mutual_info_regression(dataset[[\"Y\"]], dataset[\"X\"], discrete_features=False)[0]\n",
    "        \n",
    "        # Calculate conditional MI(v, Y | X)\n",
    "        mi_vy_given_x = mutual_info_regression(dataset[[variable, \"X\"]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
    "                        mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
    "        \n",
    "        # Calculate conditional MI(X, Y | v)\n",
    "        mi_xy_given_v = mutual_info_regression(dataset[[\"X\", variable]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
    "                        mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0]\n",
    "        \n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"conditional_MI(v,X|Y)\": mi_vx_given_y,\n",
    "            \"conditional_MI(v,Y|X)\": mi_vy_given_x,\n",
    "            \"conditional_MI(X,Y|v)\": mi_xy_given_v,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def partial_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Compute partial correlation coefficients for each variable with X and Y,\n",
    "    controlling for the other variable, as well as the partial correlation\n",
    "    between X and Y controlling for each variable.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "    \n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # Compute partial correlations\n",
    "        pcorr_vX_Y = pg.partial_corr(data=dataset, x=variable, y='X', covar='Y')['r'].iloc[0]\n",
    "        pcorr_vY_X = pg.partial_corr(data=dataset, x=variable, y='Y', covar='X')['r'].iloc[0]\n",
    "        pcorr_XY_v = pg.partial_corr(data=dataset, x='X', y='Y', covar=variable)['r'].iloc[0]\n",
    "        \n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"partial_corr(v,X|Y)\": pcorr_vX_Y,\n",
    "            \"partial_corr(v,Y|X)\": pcorr_vY_X,\n",
    "            \"partial_corr(X,Y|v)\": pcorr_XY_v,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def entropy_features(dataset):\n",
    "    \"\"\"\n",
    "    Calculate entropy and conditional entropy features for each variable.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "    \n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        \n",
    "        # Calculate conditional entropies\n",
    "        joint_vX, _, _ = np.histogram2d(dataset[variable], dataset[\"X\"], bins=20)\n",
    "        joint_vY, _, _ = np.histogram2d(dataset[variable], dataset[\"Y\"], bins=20)\n",
    "        \n",
    "        # Normalize the joint distributions\n",
    "        joint_vX = joint_vX / joint_vX.sum()\n",
    "        joint_vY = joint_vY / joint_vY.sum()\n",
    "        \n",
    "        # Calculate marginal distributions\n",
    "        p_v_X = joint_vX.sum(axis=1)\n",
    "        p_v_Y = joint_vY.sum(axis=1)\n",
    "        \n",
    "        # Calculate conditional entropies\n",
    "        cond_entropy_X_v = np.sum(p_v_X * entropy(joint_vX.T, axis=0))\n",
    "        cond_entropy_Y_v = np.sum(p_v_Y * entropy(joint_vY.T, axis=0))\n",
    "        \n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"conditional_entropy(X|v)\": cond_entropy_X_v,\n",
    "            \"conditional_entropy(Y|v)\": cond_entropy_Y_v,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def hilbert_schmidt_independence(dataset):\n",
    "    \"\"\"\n",
    "    Compute the Hilbert-Schmidt Independence Criterion (HSIC) between variables.\n",
    "    Features:\n",
    "        - hsic(v, X)\n",
    "        - hsic(v, Y)\n",
    "        - hsic(X, Y)\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "    \n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        hsic_vX = hsic(dataset[variable].values, dataset[\"X\"].values)\n",
    "        hsic_vY = hsic(dataset[variable].values, dataset[\"Y\"].values)\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"hsic(v,X)\": hsic_vX,\n",
    "            \"hsic(v,Y)\": hsic_vY,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    hsic_XY = hsic(dataset[\"X\"].values, dataset[\"Y\"].values)\n",
    "    df[\"hsic(X,Y)\"] = hsic_XY\n",
    "    \n",
    "    # Reorder columns\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def hsic(x, y, sigma=None):\n",
    "    \"\"\"\n",
    "    Compute the HSIC between two variables x and y.\n",
    "    x and y are numpy arrays of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    x = x.reshape((n, -1))\n",
    "    y = y.reshape((n, -1))\n",
    "    \n",
    "    if sigma is None:\n",
    "        sigma = np.std(np.concatenate((x, y), axis=0))\n",
    "        if sigma == 0:\n",
    "            sigma = 1.0\n",
    "\n",
    "    # Compute the Gram matrices using RBF kernel\n",
    "    K = rbf_kernel(x, gamma=1.0/(2*sigma**2))\n",
    "    L = rbf_kernel(y, gamma=1.0/(2*sigma**2))\n",
    "    \n",
    "    # Center the Gram matrices\n",
    "    H = np.eye(n) - np.ones((n, n))/n\n",
    "    Kc = H @ K @ H\n",
    "    Lc = H @ L @ H\n",
    "    \n",
    "    # Compute HSIC\n",
    "    hsic_value = (1/(n-1)**2) * np.trace(Kc @ Lc)\n",
    "    return hsic_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Given a graph as adjacency_matrix, create the class labels of each variable.\n",
    "    \"\"\"\n",
    "\n",
    "    adjacency_graph, adjacency_label = create_graph_label()\n",
    "    labels = get_labels(adjacency_matrix, adjacency_label)\n",
    "    variables = adjacency_matrix.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"variable\": variables,\n",
    "        \"label\": [labels[variable] for variable in variables],\n",
    "    })\n",
    "    df[\"dataset\"] = adjacency_matrix.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_some_columns(names_datasets, function):\n",
    "    \"\"\"\n",
    "    Apply an embedding function to a list of datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    df = []\n",
    "    for name, dataset in tqdm(names_datasets.items()):\n",
    "        dataset = names_datasets[name]\n",
    "        dataset.name = name\n",
    "    \n",
    "        try:\n",
    "            df_dataset = function(dataset)\n",
    "        except ValueError as e:\n",
    "            print(name, e)\n",
    "            raise NotImplementedError\n",
    "\n",
    "        df_dataset[\"dataset\"] = name\n",
    "        df.append(df_dataset)\n",
    "\n",
    "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_some_columns_parallel(names_datasets, function, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Apply an embedding function to a list of datasets.\n",
    "\n",
    "    Parallel version.\n",
    "    \"\"\"\n",
    "\n",
    "    def f(name, dataset, function):\n",
    "        dataset.name = name\n",
    "        df_dataset = function(dataset)\n",
    "        df_dataset[\"dataset\"] = name\n",
    "        return df_dataset\n",
    "\n",
    "    df = joblib.Parallel(n_jobs=n_jobs)(\n",
    "        joblib.delayed(f)(name, dataset, function)\n",
    "        for name, dataset in tqdm(names_datasets.items())\n",
    "    )\n",
    "\n",
    "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_columns(functions_names_datasets, n_jobs=-1, create_dimension_feature = False):\n",
    "    \"\"\"\n",
    "    given a dictionary of {function1:names, function2:names,...} apply\n",
    "    the desired functions to the list of datasets and merge all of them\n",
    "    in a single X_y_group dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    columns = []\n",
    "    if create_dimension_feature:\n",
    "        dimension_feature = create_some_columns(functions_names_datasets[list(functions_names_datasets.keys())[0]], add_dimension_feature)\n",
    "        columns.append(dimension_feature)\n",
    "    \n",
    "    for function, names_datasets in functions_names_datasets.items():\n",
    "        print(f\"set: {function.__name__}\")\n",
    "\n",
    "        if n_jobs != 1:\n",
    "            feature_set = create_some_columns_parallel(names_datasets, function, n_jobs=n_jobs)\n",
    "        else:\n",
    "            feature_set = create_some_columns(names_datasets, function)\n",
    "\n",
    "        columns.append(feature_set)\n",
    "\n",
    "    # Merge all feature sets into a single dataframe:\n",
    "    columns = functools.reduce(\n",
    "        lambda left, right: pd.merge(left, right, on=[\"dataset\", \"variable\"]),\n",
    "        columns,\n",
    "    )\n",
    "\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_datasets_train = X_train\n",
    "names_graphs_train = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, columns, threshold=3):\n",
    "    z_scores = np.abs(stats.zscore(df[columns]))\n",
    "    df['z_score_max'] = np.max(z_scores, axis=1)\n",
    "    df['is_outlier'] = df['z_score_max'] > threshold\n",
    "    df_cleaned = df[df['is_outlier'] == False].copy()\n",
    "    df_cleaned.drop(columns=['z_score_max', 'is_outlier'], inplace=True)\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment what you need!\n",
    "def train(\n",
    "    X_train: typing.Dict[str, pd.DataFrame],\n",
    "    y_train: typing.Dict[str, pd.DataFrame],\n",
    "    # number_of_features: int,\n",
    "    model_directory_path: str,\n",
    "    # id_column_name: str,\n",
    "    # prediction_column_name: str,\n",
    "    # has_gpu: bool,\n",
    ") -> None:\n",
    "    # 1) Create the variable embedding:\n",
    "    return\n",
    "    X_y_group_train_pathname = \"/tmp/X_y_group_train.parquet\"\n",
    "    try:\n",
    "        print(f\"Loading {X_y_group_train_pathname}\")\n",
    "        X_y_group_train = pd.read_parquet(X_y_group_train_pathname)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Creating X_y_group_train\")\n",
    "        names_datasets_train = X_train\n",
    "        names_graphs_train = y_train\n",
    "        X_y_group_train = create_all_columns(\n",
    "            {\n",
    "                pearson_correlation: names_datasets_train,\n",
    "                ttest: names_datasets_train,\n",
    "                mutual_information: names_datasets_train,  # comment this line to greatly reduce computation\n",
    "                label: names_graphs_train,\n",
    "                spearman_correlation: names_datasets_train,\n",
    "                kendall_correlation: names_datasets_train,\n",
    "                distance_correlation: names_datasets_train,\n",
    "                conditional_mutual_information: names_datasets_train,\n",
    "            },\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        X_y_group_train = remove_outliers(X_y_group_train, ['corr(v,X)', 'corr(v,Y)', 'ttest(v,X)', 'ttest(v,Y)'])\n",
    "        # 2) Massage X_y_group_train to prepare what is needed by the model:\n",
    "        print(\"Adding numeric labels y\") # sklearn wants numeric labels\n",
    "        le = LabelEncoder()\n",
    "        le.classes_ = np.array([\n",
    "            'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
    "            'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
    "        ])\n",
    "        X_y_group_train[\"y\"] = le.transform(X_y_group_train[\"label\"])\n",
    "\n",
    "        # reordering columns:\n",
    "        X_y_group_train = X_y_group_train[[\"dataset\", \"variable\"] + X_y_group_train.columns.drop([\"dataset\", \"variable\", \"label\", \"y\"]).tolist() + [\"label\", \"y\"]]\n",
    "        display(X_y_group_train)\n",
    "\n",
    "        print(\"Saving X_y_group_train\")\n",
    "        os.makedirs(os.path.dirname(X_y_group_train_pathname), exist_ok=True)\n",
    "        X_y_group_train.to_parquet(X_y_group_train_pathname)\n",
    "\n",
    "    print(\"Extracting X_train, y_train, and group\")\n",
    "    X_train = X_y_group_train.drop([\"variable\", \"dataset\", \"label\", \"y\"], axis=\"columns\")\n",
    "    y_train = X_y_group_train[\"y\"]\n",
    "    group_train = X_y_group_train[\"dataset\"]\n",
    "\n",
    "    # Model:\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=11, n_jobs=-1, class_weight=\"balanced\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    joblib.dump(\n",
    "        model,\n",
    "        os.path.join(model_directory_path, \"model.joblib\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_features(X_test, model):\n",
    "    \"\"\"\n",
    "    过滤测试数据集，只保留模型训练时使用的特征。\n",
    "\n",
    "    参数:\n",
    "    X_test : pandas.DataFrame 或 numpy.array\n",
    "        需要进行预测的测试数据\n",
    "    model : 已训练的模型\n",
    "        包含 feature_names_in_ 属性的模型（如sklearn的大多数模型）\n",
    "\n",
    "    返回:\n",
    "    pandas.DataFrame 或 numpy.array\n",
    "        只包含模型训练时使用的特征的测试数据\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_names_in_'):\n",
    "        # 获取模型训练时使用的特征名称\n",
    "        model_features = model.feature_names_in_\n",
    "        \n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            # 对于DataFrame，我们可以直接使用列名\n",
    "            common_features = list(set(X_test.columns) & set(model_features))\n",
    "            if len(common_features) < len(model_features):\n",
    "                print(f\"警告: 测试数据缺少 {len(model_features) - len(common_features)} 个训练时使用的特征。\")\n",
    "            extra_features = set(X_test.columns) - set(model_features)\n",
    "            if extra_features:\n",
    "                print(f\"警告: 移除了 {len(extra_features)} 个在训练时未使用的特征: {extra_features}\")\n",
    "            return X_test[common_features]\n",
    "        elif isinstance(X_test, np.ndarray):\n",
    "            # 对于numpy数组，我们假设特征的顺序与训练时相同\n",
    "            if X_test.shape[1] > len(model_features):\n",
    "                print(f\"警告: 测试数据包含额外的特征。只使用前 {len(model_features)} 个特征。\")\n",
    "                return X_test[:, :len(model_features)]\n",
    "            elif X_test.shape[1] < len(model_features):\n",
    "                raise ValueError(f\"错误: 测试数据的特征数 ({X_test.shape[1]}) 少于模型训练时的特征数 ({len(model_features)})\")\n",
    "            return X_test\n",
    "    else:\n",
    "        print(\"警告: 模型没有 feature_names_in_ 属性。无法验证特征。\")\n",
    "        return X_test\n",
    "\n",
    "def align_features(X, model):\n",
    "    \"\"\"\n",
    "    调整输入特征的顺序，使其与模型训练时的特征顺序一致。\n",
    "\n",
    "    参数:\n",
    "    X : pandas.DataFrame 或 numpy.ndarray\n",
    "        需要调整顺序的输入特征\n",
    "    model : 已训练的模型\n",
    "        包含 feature_names_in_ 属性的模型（如sklearn的大多数模型）\n",
    "\n",
    "    返回:\n",
    "    pandas.DataFrame 或 numpy.ndarray\n",
    "        特征顺序调整后的数据\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'feature_names_in_'):\n",
    "        print(\"警告: 模型没有 feature_names_in_ 属性。无法调整特征顺序。\")\n",
    "        return X\n",
    "\n",
    "    model_features = model.feature_names_in_\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        # 对于DataFrame，我们可以直接使用列名重新排序\n",
    "        if set(X.columns) != set(model_features):\n",
    "            raise ValueError(\"输入特征与模型特征不完全匹配。\")\n",
    "        return X.reindex(columns=model_features)\n",
    "\n",
    "    elif isinstance(X, np.ndarray):\n",
    "        if X.shape[1] != len(model_features):\n",
    "            raise ValueError(\"输入特征数量与模型特征数量不匹配。\")\n",
    "        \n",
    "        # 对于numpy数组，我们需要创建一个映射来重新排序\n",
    "        current_features = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "        df = pd.DataFrame(X, columns=current_features)\n",
    "        feature_mapping = dict(zip(current_features, X.columns if isinstance(X, pd.DataFrame) else model_features))\n",
    "        df = df.rename(columns=feature_mapping)\n",
    "        return df.reindex(columns=model_features).values\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"输入X必须是pandas.DataFrame或numpy.ndarray。\")\n",
    "\n",
    "def create_submission(X_y_pred_test):\n",
    "    \"\"\"\n",
    "    From the predicted test set, for each dataset, take predicted\n",
    "    classes of all variables, create the adjacency matrix, then create\n",
    "    the submission in the requested format.\n",
    "    \"\"\"\n",
    "\n",
    "    submission = {}\n",
    "    for name, prediction in tqdm(X_y_pred_test.groupby(\"dataset\"), delay=10):\n",
    "        variables_labels = prediction[[\"variable\", \"label_predicted\"]].set_index(\"variable\")\n",
    "        variables = variables_labels.index.tolist()\n",
    "        variables_all = [\"X\", \"Y\"] + variables\n",
    "\n",
    "        adjacency_matrix = pd.DataFrame(index=variables_all, columns=variables_all)\n",
    "        adjacency_matrix.index.name = \"parent\"\n",
    "        adjacency_matrix[:] = 0\n",
    "        adjacency_matrix.loc[\"X\", \"Y\"] = 1\n",
    "\n",
    "        for v in variables:\n",
    "            l = variables_labels.loc[v].item()\n",
    "            if l == \"Cause of X\":\n",
    "                adjacency_matrix.loc[v, \"X\"] = 1\n",
    "            elif l == \"Cause of Y\":\n",
    "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
    "            elif l == \"Consequence of X\":\n",
    "                adjacency_matrix.loc[\"X\", v] = 1\n",
    "            elif l == \"Consequence of Y\":\n",
    "                adjacency_matrix.loc[\"Y\", v] = 1\n",
    "            elif l == \"Confounder\":\n",
    "                adjacency_matrix.loc[v, \"X\"] = 1\n",
    "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
    "            elif l == \"Collider\":\n",
    "                adjacency_matrix.loc[\"X\", v] = 1\n",
    "                adjacency_matrix.loc[\"Y\", v] = 1\n",
    "            elif l == \"Mediator\":\n",
    "                adjacency_matrix.loc[\"X\", v] = 1\n",
    "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
    "            elif l == \"Confounder\":\n",
    "                pass\n",
    "\n",
    "        for i in variables_all:\n",
    "            for j in variables_all:\n",
    "                submission[f'{name}_{i}_{j}'] = int(adjacency_matrix.loc[i, j])\n",
    "\n",
    "    return submission\n",
    "\n",
    "\n",
    "# Uncomment what you need!\n",
    "def infer(\n",
    "    X_test: typing.Dict[str, pd.DataFrame],\n",
    "    # number_of_features: int,\n",
    "    model_directory_path: str,\n",
    "    id_column_name: str,\n",
    "    prediction_column_name: str,\n",
    "    # has_gpu: bool,\n",
    "    # has_trained: bool,\n",
    ") -> pd.DataFrame:\n",
    "    model = joblib.load(os.path.join(model_directory_path, \"best_random_forest_model.joblib\"))\n",
    "\n",
    "    names_datasets_test = X_test\n",
    "    X_group_test = create_all_columns(\n",
    "        {\n",
    "                pearson_correlation: names_datasets_test,\n",
    "                ttest: names_datasets_test,\n",
    "                mutual_information: names_datasets_test, \n",
    "                spearman_correlation: names_datasets_test,\n",
    "                kendall_correlation: names_datasets_test,\n",
    "                distance_correlation: names_datasets_test,\n",
    "                conditional_mutual_information: names_datasets_test,\n",
    "                linear_regression_feature: names_datasets_test,\n",
    "                partial_correlation: names_datasets_test,\n",
    "                ridge_regression_feature: names_datasets_test,\n",
    "        },\n",
    "        n_jobs=-1,\n",
    "        create_dimension_feature=True\n",
    "    )\n",
    "    blacklist = [\"ttest(v,X)\", \"pvalue(ttest(v,X))<=0.05\", \"ttest(v,Y)\", \"pvalue(ttest(v,Y))<=0.05\", \"ttest(X,Y)\", \"pvalue(ttest(X,Y))<=0.05\"]\n",
    "    columns_to_drop = [col for col in blacklist if col in X_group_test.columns]\n",
    "    X_group_test = X_group_test.drop(columns=columns_to_drop)\n",
    "\n",
    "    X_group_test['MI(v,X)^2'] = X_group_test['MI(v,X)'] ** 2\n",
    "    X_group_test['MI(v,Y)^2'] = X_group_test['MI(v,Y)'] ** 2\n",
    "    X_group_test['MI(X,Y)^2'] = X_group_test['MI(X,Y)'] ** 2\n",
    "    X_group_test['max(MI(v, others))^2'] = X_group_test['max(MI(v, others))'] ** 2\n",
    "    X_group_test['min(MI(v, others))^2'] = X_group_test['min(MI(v, others))'] ** 2\n",
    "\n",
    "    X_test = X_group_test.drop(columns=[\"dataset\", \"variable\"])\n",
    "    X_test = filter_features(X_test, model)\n",
    "    X_test = align_features(X_test, model)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    X_y_pred_test = X_group_test\n",
    "    X_y_pred_test[\"y_predicted\"] = y_predicted\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.classes_ = np.array([\n",
    "        'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
    "        'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
    "    ])\n",
    "\n",
    "    X_y_pred_test[\"label_predicted\"] = le.inverse_transform(y_predicted)\n",
    "\n",
    "    submission = create_submission(X_y_pred_test)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        submission.items(),\n",
    "        columns=[\n",
    "            id_column_name,\n",
    "            prediction_column_name\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ori\\AppData\\Local\\Temp\\ipykernel_3456\\3844714633.py:10: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  y_test = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy: 0.5563\n",
      "Sensitivity: 0.2080\n",
      "Specificity: 0.9047\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 读取预测结果\n",
    "prediction = pd.read_csv('./data/prediction.csv')\n",
    "\n",
    "# 读取真实标签数据\n",
    "with open('./data/y_test.pickle', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "# 将预测结果转换为字典格式，方便后续处理\n",
    "pred_dict = dict(zip(prediction['example_id'], prediction['prediction']))\n",
    "\n",
    "# 初始化计数器\n",
    "true_positives = 0\n",
    "true_negatives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "# 计算混淆矩阵的元素\n",
    "for dataset, adj_matrix in y_test.items():\n",
    "    for i in adj_matrix.index:\n",
    "        for j in adj_matrix.columns:\n",
    "            key = f\"{dataset}_{i}_{j}\"\n",
    "            if key in pred_dict:\n",
    "                true_value = adj_matrix.loc[i, j]\n",
    "                predicted_value = pred_dict[key]\n",
    "                \n",
    "                if true_value == 1 and predicted_value == 1:\n",
    "                    true_positives += 1\n",
    "                elif true_value == 0 and predicted_value == 0:\n",
    "                    true_negatives += 1\n",
    "                elif true_value == 0 and predicted_value == 1:\n",
    "                    false_positives += 1\n",
    "                elif true_value == 1 and predicted_value == 0:\n",
    "                    false_negatives += 1\n",
    "\n",
    "# 计算敏感度和特异度\n",
    "sensitivity = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "specificity = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n",
    "\n",
    "# 计算balanced accuracy\n",
    "balanced_accuracy = (sensitivity + specificity) / 2\n",
    "\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy:.4f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m13:16:11\u001b[0m \u001b[33mno forbidden library found\u001b[0m\n",
      "\u001b[32m13:16:11\u001b[0m \u001b[33m\u001b[0m\n",
      "\u001b[32m13:16:11\u001b[0m started\n",
      "\u001b[32m13:16:11\u001b[0m running local test\n",
      "\u001b[32m13:16:11\u001b[0m \u001b[33minternet access isn't restricted, no check will be done\u001b[0m\n",
      "\u001b[32m13:16:11\u001b[0m \n",
      "\u001b[32m13:16:19\u001b[0m starting dag process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download data\\X_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_train.pickle (1523944532 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_train.pickle (7017735 bytes)\n",
      "already exists: file length match\n",
      "download data\\X_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_test_reduced.pickle (122341879 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_test_reduced.pickle (562930 bytes)\n",
      "already exists: file length match\n",
      "download data\\example_prediction.parquet from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/example_prediction_reduced.parquet (668981 bytes)\n",
      "already exists: file length match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m13:16:47\u001b[0m \u001b[33mcall: train\u001b[0m\n",
      "\u001b[32m13:16:47\u001b[0m \u001b[33mcall: infer\u001b[0m\n",
      "100%|██████████| 1880/1880 [00:01<00:00, 1845.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: pearson_correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [00:06<00:00, 288.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: ttest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [00:09<00:00, 205.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: mutual_information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [01:43<00:00, 18.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: spearman_correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [00:08<00:00, 228.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: kendall_correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [00:24<00:00, 77.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: distance_correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1880/1880 [03:46<00:00,  8.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: conditional_mutual_information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [01:44<00:00, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: linear_regression_feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [00:09<00:00, 199.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: partial_correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [00:19<00:00, 95.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: ridge_regression_feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [01:21<00:00, 23.05it/s]\n",
      "\u001b[32m13:27:07\u001b[0m \u001b[33msave prediction - path=data\\prediction.csv\u001b[0m\n",
      "\u001b[32m13:27:08\u001b[0m check prediction - call=columns_name({})\n",
      "\u001b[32m13:27:08\u001b[0m check prediction - call=nans({})\n",
      "\u001b[32m13:27:08\u001b[0m check prediction - call=ids({})\n",
      "\u001b[32m13:27:09\u001b[0m check prediction - call=values_allowed({'values': [0, 1]}) column=`prediction`\n",
      "\u001b[32m13:27:09\u001b[0m \u001b[33mprediction is valid\u001b[0m\n",
      "\u001b[32m13:27:09\u001b[0m ended\n",
      "\u001b[32m13:27:09\u001b[0m \u001b[33mduration - time=00:10:57\u001b[0m\n",
      "\u001b[32m13:27:09\u001b[0m \u001b[33mmemory - before=\"1.02 GB\" after=\"846.05 MB\" consumed=\"-203,444,224.00 B\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download this notebook and submit it to the platform: https://hub.crunchdao.com/competitions/causality-discovery/submit/via/notebook\n"
     ]
    }
   ],
   "source": [
    "crunch.test(\n",
    "    no_determinism_check=True\n",
    ")\n",
    "\n",
    "print(\"Download this notebook and submit it to the platform: https://hub.crunchdao.com/competitions/causality-discovery/submit/via/notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
