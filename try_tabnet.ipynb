{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 不可用，使用 CPU 进行训练。\n",
      "CUDA 不可用，使用 CPU 进行训练。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        print(\"CUDA 可用，使用 GPU 进行训练。\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"CUDA 不可用，使用 CPU 进行训练。\")\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "def clean_feature_names(X):\n",
    "    # 函数用于清理特征名称\n",
    "    def clean_name(name):\n",
    "        # 移除或替换特殊字符\n",
    "        name = re.sub(r'[^\\w\\s-]', '_', name)\n",
    "        # 确保名称不以数字开头\n",
    "        if name[0].isdigit():\n",
    "            name = 'f_' + name\n",
    "        return name\n",
    "\n",
    "    X.columns = [clean_name(col) for col in X.columns]\n",
    "    return X\n",
    "\n",
    "# 定义函数以自动检测和处理类别特征\n",
    "def process_categorical_features(df, max_unique=10):\n",
    "    \"\"\"\n",
    "    自动检测和处理数据框中的类别变量。\n",
    "\n",
    "    参数：\n",
    "    - df (pd.DataFrame): 输入的数据框。\n",
    "    - max_unique (int): 判定为类别变量的最大唯一值数量。\n",
    "\n",
    "    返回：\n",
    "    - cat_idxs (list of int): 类别特征的索引。\n",
    "    - cat_dims (list of int): 每个类别特征的模态数。\n",
    "    - encoder_dict (dict): 存储每个类别特征的 LabelEncoder 实例。\n",
    "    \"\"\"\n",
    "    cat_cols = [col for col in df.columns if df[col].nunique() <= max_unique]\n",
    "    cat_dims = []\n",
    "    cat_idxs = []\n",
    "    encoder_dict = {}\n",
    "\n",
    "    for col in cat_cols:\n",
    "        print(f\"处理类别特征: {col}，唯一值数量: {df[col].nunique()}\")\n",
    "        # 使用 LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].values)\n",
    "        cat_dims.append(len(le.classes_))\n",
    "        cat_idxs.append(df.columns.get_loc(col))\n",
    "\n",
    "    return cat_idxs, cat_dims, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding numeric labels y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>variable</th>\n",
       "      <th>dimension</th>\n",
       "      <th>corr(v,X)</th>\n",
       "      <th>corr(v,Y)</th>\n",
       "      <th>max(corr(v, others))</th>\n",
       "      <th>min(corr(v, others))</th>\n",
       "      <th>mean(corr(v, others))</th>\n",
       "      <th>std(corr(v, others))</th>\n",
       "      <th>corr(X,Y)</th>\n",
       "      <th>...</th>\n",
       "      <th>v~X_piecewise_coef1</th>\n",
       "      <th>v~X_piecewise_coef2</th>\n",
       "      <th>v~Y_piecewise_coef1</th>\n",
       "      <th>v~Y_piecewise_coef2</th>\n",
       "      <th>X~v_piecewise_coef1</th>\n",
       "      <th>X~v_piecewise_coef2</th>\n",
       "      <th>Y~v_piecewise_coef1</th>\n",
       "      <th>Y~v_piecewise_coef2</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.169735</td>\n",
       "      <td>-0.113595</td>\n",
       "      <td>0.791467</td>\n",
       "      <td>0.027355</td>\n",
       "      <td>0.282376</td>\n",
       "      <td>0.298969</td>\n",
       "      <td>-0.771058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087250</td>\n",
       "      <td>0.164971</td>\n",
       "      <td>-0.112374</td>\n",
       "      <td>-0.002442</td>\n",
       "      <td>0.012587</td>\n",
       "      <td>0.314297</td>\n",
       "      <td>-0.312407</td>\n",
       "      <td>0.397623</td>\n",
       "      <td>Consequence of Y</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.072334</td>\n",
       "      <td>0.139419</td>\n",
       "      <td>0.230139</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.090511</td>\n",
       "      <td>0.082201</td>\n",
       "      <td>-0.771058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051409</td>\n",
       "      <td>-0.041849</td>\n",
       "      <td>0.163493</td>\n",
       "      <td>-0.048149</td>\n",
       "      <td>-0.056995</td>\n",
       "      <td>-0.030678</td>\n",
       "      <td>0.293044</td>\n",
       "      <td>-0.307249</td>\n",
       "      <td>Independent</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.123115</td>\n",
       "      <td>-0.238769</td>\n",
       "      <td>0.781051</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.257037</td>\n",
       "      <td>0.288165</td>\n",
       "      <td>-0.771058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208082</td>\n",
       "      <td>-0.169934</td>\n",
       "      <td>-0.288890</td>\n",
       "      <td>0.100243</td>\n",
       "      <td>0.213582</td>\n",
       "      <td>-0.180935</td>\n",
       "      <td>0.080068</td>\n",
       "      <td>-0.637675</td>\n",
       "      <td>Cause of Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.001935</td>\n",
       "      <td>0.013921</td>\n",
       "      <td>0.147408</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>0.052451</td>\n",
       "      <td>0.053457</td>\n",
       "      <td>-0.771058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>-0.036772</td>\n",
       "      <td>0.098802</td>\n",
       "      <td>-0.169762</td>\n",
       "      <td>-0.006993</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>-0.369786</td>\n",
       "      <td>0.767414</td>\n",
       "      <td>Cause of Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.284323</td>\n",
       "      <td>-0.466570</td>\n",
       "      <td>0.791467</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0.426306</td>\n",
       "      <td>0.250048</td>\n",
       "      <td>-0.771058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463463</td>\n",
       "      <td>-0.358279</td>\n",
       "      <td>-0.669777</td>\n",
       "      <td>0.406414</td>\n",
       "      <td>0.454525</td>\n",
       "      <td>-0.340405</td>\n",
       "      <td>-0.229665</td>\n",
       "      <td>-0.473811</td>\n",
       "      <td>Mediator</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142905</th>\n",
       "      <td>46997</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014740</td>\n",
       "      <td>0.697089</td>\n",
       "      <td>0.748635</td>\n",
       "      <td>0.013858</td>\n",
       "      <td>0.376630</td>\n",
       "      <td>0.352655</td>\n",
       "      <td>-0.089641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087745</td>\n",
       "      <td>-0.146010</td>\n",
       "      <td>0.575781</td>\n",
       "      <td>0.242615</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>-0.083877</td>\n",
       "      <td>0.726976</td>\n",
       "      <td>-0.059773</td>\n",
       "      <td>Cause of Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142906</th>\n",
       "      <td>46997</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.007397</td>\n",
       "      <td>-0.116481</td>\n",
       "      <td>0.998843</td>\n",
       "      <td>0.007397</td>\n",
       "      <td>0.180741</td>\n",
       "      <td>0.333308</td>\n",
       "      <td>-0.089641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123670</td>\n",
       "      <td>-0.262133</td>\n",
       "      <td>-0.110020</td>\n",
       "      <td>-0.012921</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>-0.018881</td>\n",
       "      <td>-0.397732</td>\n",
       "      <td>0.562503</td>\n",
       "      <td>Cause of Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142907</th>\n",
       "      <td>46997</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.011658</td>\n",
       "      <td>0.908029</td>\n",
       "      <td>0.970364</td>\n",
       "      <td>0.011658</td>\n",
       "      <td>0.469777</td>\n",
       "      <td>0.441105</td>\n",
       "      <td>-0.089641</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071578</td>\n",
       "      <td>0.119841</td>\n",
       "      <td>1.012838</td>\n",
       "      <td>-0.209618</td>\n",
       "      <td>0.122501</td>\n",
       "      <td>-0.268318</td>\n",
       "      <td>0.997623</td>\n",
       "      <td>-0.179187</td>\n",
       "      <td>Cause of Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142908</th>\n",
       "      <td>46998</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.083546</td>\n",
       "      <td>-0.019665</td>\n",
       "      <td>0.083546</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>0.040022</td>\n",
       "      <td>0.037719</td>\n",
       "      <td>0.036862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163719</td>\n",
       "      <td>-0.160346</td>\n",
       "      <td>0.014060</td>\n",
       "      <td>-0.067450</td>\n",
       "      <td>0.510875</td>\n",
       "      <td>-0.854658</td>\n",
       "      <td>-0.053777</td>\n",
       "      <td>0.068223</td>\n",
       "      <td>Consequence of X</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142909</th>\n",
       "      <td>46998</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.201922</td>\n",
       "      <td>0.666742</td>\n",
       "      <td>0.666742</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>0.295173</td>\n",
       "      <td>0.334828</td>\n",
       "      <td>0.036862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225463</td>\n",
       "      <td>-0.854768</td>\n",
       "      <td>0.529780</td>\n",
       "      <td>0.273924</td>\n",
       "      <td>0.064348</td>\n",
       "      <td>-0.532539</td>\n",
       "      <td>0.640283</td>\n",
       "      <td>0.052917</td>\n",
       "      <td>Collider</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142910 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset  variable  dimension  corr(v,X)  corr(v,Y)  \\\n",
       "0             0         0          8   0.169735  -0.113595   \n",
       "1             0         2          8  -0.072334   0.139419   \n",
       "2             0         3          8   0.123115  -0.238769   \n",
       "3             0         4          8  -0.001935   0.013921   \n",
       "4             0         5          8   0.284323  -0.466570   \n",
       "...         ...       ...        ...        ...        ...   \n",
       "142905    46997         6          7   0.014740   0.697089   \n",
       "142906    46997         7          7  -0.007397  -0.116481   \n",
       "142907    46997         8          7  -0.011658   0.908029   \n",
       "142908    46998         0          2   0.083546  -0.019665   \n",
       "142909    46998         3          2  -0.201922   0.666742   \n",
       "\n",
       "        max(corr(v, others))  min(corr(v, others))  mean(corr(v, others))  \\\n",
       "0                   0.791467              0.027355               0.282376   \n",
       "1                   0.230139              0.003233               0.090511   \n",
       "2                   0.781051              0.012225               0.257037   \n",
       "3                   0.147408              0.001935               0.052451   \n",
       "4                   0.791467              0.064815               0.426306   \n",
       "...                      ...                   ...                    ...   \n",
       "142905              0.748635              0.013858               0.376630   \n",
       "142906              0.998843              0.007397               0.180741   \n",
       "142907              0.970364              0.011658               0.469777   \n",
       "142908              0.083546              0.016856               0.040022   \n",
       "142909              0.666742              0.016856               0.295173   \n",
       "\n",
       "        std(corr(v, others))  corr(X,Y)  ...  v~X_piecewise_coef1  \\\n",
       "0                   0.298969  -0.771058  ...             0.087250   \n",
       "1                   0.082201  -0.771058  ...            -0.051409   \n",
       "2                   0.288165  -0.771058  ...             0.208082   \n",
       "3                   0.053457  -0.771058  ...             0.016450   \n",
       "4                   0.250048  -0.771058  ...             0.463463   \n",
       "...                      ...        ...  ...                  ...   \n",
       "142905              0.352655  -0.089641  ...             0.087745   \n",
       "142906              0.333308  -0.089641  ...             0.123670   \n",
       "142907              0.441105  -0.089641  ...            -0.071578   \n",
       "142908              0.037719   0.036862  ...             0.163719   \n",
       "142909              0.334828   0.036862  ...             0.225463   \n",
       "\n",
       "        v~X_piecewise_coef2  v~Y_piecewise_coef1  v~Y_piecewise_coef2  \\\n",
       "0                  0.164971            -0.112374            -0.002442   \n",
       "1                 -0.041849             0.163493            -0.048149   \n",
       "2                 -0.169934            -0.288890             0.100243   \n",
       "3                 -0.036772             0.098802            -0.169762   \n",
       "4                 -0.358279            -0.669777             0.406414   \n",
       "...                     ...                  ...                  ...   \n",
       "142905            -0.146010             0.575781             0.242615   \n",
       "142906            -0.262133            -0.110020            -0.012921   \n",
       "142907             0.119841             1.012838            -0.209618   \n",
       "142908            -0.160346             0.014060            -0.067450   \n",
       "142909            -0.854768             0.529780             0.273924   \n",
       "\n",
       "        X~v_piecewise_coef1  X~v_piecewise_coef2  Y~v_piecewise_coef1  \\\n",
       "0                  0.012587             0.314297            -0.312407   \n",
       "1                 -0.056995            -0.030678             0.293044   \n",
       "2                  0.213582            -0.180935             0.080068   \n",
       "3                 -0.006993             0.010116            -0.369786   \n",
       "4                  0.454525            -0.340405            -0.229665   \n",
       "...                     ...                  ...                  ...   \n",
       "142905             0.056678            -0.083877             0.726976   \n",
       "142906             0.002044            -0.018881            -0.397732   \n",
       "142907             0.122501            -0.268318             0.997623   \n",
       "142908             0.510875            -0.854658            -0.053777   \n",
       "142909             0.064348            -0.532539             0.640283   \n",
       "\n",
       "        Y~v_piecewise_coef2             label  y  \n",
       "0                  0.397623  Consequence of Y  5  \n",
       "1                 -0.307249       Independent  6  \n",
       "2                 -0.637675        Cause of Y  1  \n",
       "3                  0.767414        Cause of Y  1  \n",
       "4                 -0.473811          Mediator  7  \n",
       "...                     ...               ... ..  \n",
       "142905            -0.059773        Cause of Y  1  \n",
       "142906             0.562503        Cause of Y  1  \n",
       "142907            -0.179187        Cause of Y  1  \n",
       "142908             0.068223  Consequence of X  4  \n",
       "142909             0.052917          Collider  2  \n",
       "\n",
       "[142910 rows x 123 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting X_train, y_train, and group\n",
      "处理类别特征: dimension，唯一值数量: 8\n",
      "处理类别特征: pvalue_ttest_X_Y____0_05，唯一值数量: 2\n",
      "处理类别特征: square_dimension，唯一值数量: 8\n",
      "处理类别特征: ExactSearch_v_X_，唯一值数量: 2\n",
      "处理类别特征: ExactSearch_X_v_，唯一值数量: 2\n",
      "处理类别特征: ExactSearch_v_Y_，唯一值数量: 2\n",
      "处理类别特征: ExactSearch_Y_v_，唯一值数量: 2\n",
      "处理类别特征: ExactSearch_X_Y_，唯一值数量: 2\n",
      "处理类别特征: PC_v_X_，唯一值数量: 2\n",
      "处理类别特征: PC_X_v_，唯一值数量: 2\n",
      "处理类别特征: PC_v_Y_，唯一值数量: 2\n",
      "处理类别特征: PC_Y_v_，唯一值数量: 2\n",
      "处理类别特征: PC_X_Y_，唯一值数量: 2\n",
      "处理类别特征: FCI_v_X_，唯一值数量: 4\n",
      "处理类别特征: FCI_X_v_，唯一值数量: 4\n",
      "处理类别特征: FCI_v_Y_，唯一值数量: 4\n",
      "处理类别特征: FCI_Y_v_，唯一值数量: 4\n",
      "处理类别特征: FCI_X_Y_，唯一值数量: 4\n",
      "类别特征索引 (cat_idxs): [0, 8, 33, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]\n",
      "类别特征模态数 (cat_dims): [8, 2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "X_y_group_train = pd.read_csv('./mid_data/X_y_group_train_updated_v12.2_piecewise.csv')\n",
    "\n",
    "print(\"Adding numeric labels y\")\n",
    "le = LabelEncoder()\n",
    "X_y_group_train[\"y\"] = le.fit_transform(X_y_group_train[\"label\"])\n",
    "# reordering columns:\n",
    "X_y_group_train = X_y_group_train[[\"dataset\", \"variable\"] + X_y_group_train.columns.drop([\"dataset\", \"variable\", \"label\", \"y\"]).tolist() + [\"label\", \"y\"]]\n",
    "\n",
    "blacklist = [\"ttest(v,X)\", \"pvalue(ttest(v,X))<=0.05\", \"ttest(v,Y)\", \"pvalue(ttest(v,Y))<=0.05\", \"ttest(X,Y)\", \"pvalue(ttest(X,Y))<=0.05\"\\\n",
    "    \"square_dimension\", \"max(PPS(v,others))\"]\n",
    "columns_to_drop = [col for col in blacklist if col in X_y_group_train.columns]\n",
    "X_y_group_train = X_y_group_train.drop(columns=columns_to_drop)\n",
    "\n",
    "numeric_columns = X_y_group_train.select_dtypes(include=[np.number]).columns\n",
    "X_y_group_train[numeric_columns] = X_y_group_train[numeric_columns].fillna(X_y_group_train[numeric_columns].mean())\n",
    "\n",
    "display(X_y_group_train)\n",
    "\n",
    "# 清理特征名称\n",
    "X_y_group_train = clean_feature_names(X_y_group_train)\n",
    "\n",
    "print(\"Extracting X_train, y_train, and group\")\n",
    "# 分离数据集ID、特征和标签\n",
    "group_train = X_y_group_train[\"dataset\"]\n",
    "X = X_y_group_train.drop([\"variable\", \"dataset\", \"label\", \"y\"], axis=\"columns\")\n",
    "y = X_y_group_train[\"y\"]\n",
    "\n",
    "# 处理类别特征\n",
    "cat_idxs, cat_dims, X = process_categorical_features(X)\n",
    "print(f\"类别特征索引 (cat_idxs): {cat_idxs}\")\n",
    "print(f\"类别特征模态数 (cat_dims): {cat_dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train 唯一值: [0 1 2 3 4 5 6 7]\n",
      "y_test 唯一值: [0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "# 分割数据集为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(\"y_train 唯一值:\", np.unique(y_train))\n",
    "print(\"y_test 唯一值:\", np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.51206 | train_accuracy: 0.56376 | train_balanced_accuracy: 0.29578 | valid_accuracy: 0.56091 | valid_balanced_accuracy: 0.29552 |  0:01:09s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m\n\u001b[0;32m      2\u001b[0m clf \u001b[38;5;241m=\u001b[39m TabNetClassifier(\n\u001b[0;32m      3\u001b[0m     n_d\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,                  \u001b[38;5;66;03m# 决策层的宽度（小心过拟合）\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     n_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,                  \u001b[38;5;66;03m# 注意力嵌入的宽度（一般与n_d一致）\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     device_name\u001b[38;5;241m=\u001b[39mdevice,       \u001b[38;5;66;03m# 使用 GPU\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 训练集的特征矩阵（np.array）\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 训练集的目标标签（np.array，对于多分类任务，标签应为整数编码）\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# 验证集列表\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# 验证集的名称\u001b[39;49;00m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbalanced_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# 评估指标列表\u001b[39;49;00m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# 最大训练轮数\u001b[39;49;00m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# 早停的耐心轮数\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# 批量大小\u001b[39;49;00m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 用于 Ghost Batch Normalization 的虚拟批次大小（应能被 batch_size 整除）\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# 用于 torch.utils.data.DataLoader 的工作线程数\u001b[39;49;00m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# 是否在训练过程中丢弃最后一个不完整的批次\u001b[39;49;00m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# 回调函数列表\u001b[39;49;00m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_importance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# 是否计算特征重要性\u001b[39;49;00m\n\u001b[0;32m     42\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 预测\u001b[39;00m\n\u001b[0;32m     45\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_train\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:258\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[0;32m    254\u001b[0m \n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, valid_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:489\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[1;32m--> 489\u001b[0m     batch_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[0;32m    493\u001b[0m epoch_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:534\u001b[0m, in \u001b[0;36mTabModel._train_batch\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    531\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_sparse \u001b[38;5;241m*\u001b[39m M_loss\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# Perform backward pass and optimization\u001b[39;00m\n\u001b[1;32m--> 534\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_value:\n\u001b[0;32m    536\u001b[0m     clip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_value)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\adia_py311\\Lib\\site-packages\\torch\\autograd\\function.py:291\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    292\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定义 TabNetClassifier\n",
    "clf = TabNetClassifier(\n",
    "    n_d=64,                  # 决策层的宽度（小心过拟合）\n",
    "    n_a=64,                  # 注意力嵌入的宽度（一般与n_d一致）\n",
    "    n_steps=5,               # 决策步骤数（3-10）\n",
    "    gamma=1.5,               # 特征重用系数（值接近 1 会减少层间的特征选择相关性，1.0-2.0）\n",
    "    cat_idxs=cat_idxs,       # 类别特征的索引列表\n",
    "    cat_dims=cat_dims,       # 每个类别特征的模态数（即类别数量）\n",
    "    cat_emb_dim=1,           # 类别特征的嵌入维度\n",
    "    n_independent=2,         # 每个步骤中独立的 Gated Linear Units (GLU) 层的数量（1-5）\n",
    "    n_shared=2,              # 每个步骤中共享的 GLU 层的数量（1-5）\n",
    "    epsilon=1e-5,            # 防止除以零的常数\n",
    "    seed=42,                 # 随机种子\n",
    "    momentum=0.02,           # 批量归一化的动量参数（0.01-0.4）\n",
    "    clip_value=None,         # 如果设置为浮点数，将梯度剪裁到该值\n",
    "    lambda_sparse=1e-3,      # 额外的稀疏性损失系数，值越大，模型在特征选择上越稀疏（1e-3-1e-1）\n",
    "    optimizer_fn=torch.optim.Adam,                      # 优化器\n",
    "    optimizer_params=dict(lr=2e-2),                     # 优化器的参数\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,       # 学习率调度器\n",
    "    scheduler_params=dict(step_size=50, gamma=0.9),     # 学习率调度器的参数\n",
    "    mask_type='sparsemax',   # 特征选择的掩码类型（'sparsemax' 或 'entmax'）\n",
    "    # grouped_features=None,   # 将特征分组，使模型在同一组内共享注意力。这在特征预处理生成相关或依赖特征时尤其有用，例如使用 TF-IDF 或 PCA。特征重要性在同组内将相同。\n",
    "    verbose=1,               # 是否打印训练过程中的信息（0 或 1）\n",
    "    device_name=device,       # 使用 GPU\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "clf.fit(\n",
    "    X_train=X_train.values,  # 训练集的特征矩阵（np.array）\n",
    "    y_train=y_train.values,  # 训练集的目标标签（np.array，对于多分类任务，标签应为整数编码）\n",
    "    eval_set=[(X_train.values, y_train.values), (X_test.values, y_test.values)],   # 验证集列表\n",
    "    eval_name=['train', 'valid'],                      # 验证集的名称\n",
    "    eval_metric=['accuracy', 'balanced_accuracy'],     # 评估指标列表\n",
    "    max_epochs=2000,         # 最大训练轮数\n",
    "    patience=10,             # 早停的耐心轮数\n",
    "    batch_size=512,         # 批量大小\n",
    "    virtual_batch_size=128,  # 用于 Ghost Batch Normalization 的虚拟批次大小（应能被 batch_size 整除）\n",
    "    num_workers=0,           # 用于 torch.utils.data.DataLoader 的工作线程数\n",
    "    drop_last=False,         # 是否在训练过程中丢弃最后一个不完整的批次\n",
    "    callbacks=None,          # 回调函数列表\n",
    "    compute_importance=True,   # 是否计算特征重要性\n",
    ")\n",
    "\n",
    "# 预测\n",
    "y_train_pred = clf.predict(X_train.values)\n",
    "y_test_pred = clf.predict(X_test.values)\n",
    "\n",
    "# 计算平衡准确率\n",
    "train_score = balanced_accuracy_score(y_train, y_train_pred)\n",
    "test_score = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"训练集平衡准确率: {train_score:.6f}\")\n",
    "print(f\"测试集平衡准确率: {test_score:.6f}\")\n",
    "\n",
    "# 输出详细的分类报告\n",
    "print(\"\\n测试集分类报告:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adia_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
