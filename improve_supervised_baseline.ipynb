{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-21T11:20:39.508775Z",
                    "start_time": "2024-09-21T11:20:32.677062Z"
                },
                "metadata": {}
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "2024-10-03 02:22:55,321\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
                    ]
                }
            ],
            "source": [
                "import functools\n",
                "import os\n",
                "import typing\n",
                "import random\n",
                "import joblib\n",
                "import networkx as nx\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import pingouin as pg\n",
                "import dcor\n",
                "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
                "from scipy.stats import pearsonr, ttest_rel\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.feature_selection import mutual_info_regression\n",
                "from sklearn.metrics import balanced_accuracy_score, silhouette_score\n",
                "from sklearn.model_selection import GridSearchCV, GroupKFold, cross_val_score\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import Ridge, LinearRegression, LassoCV\n",
                "from sklearn.cluster import DBSCAN\n",
                "from sklearn.feature_selection import mutual_info_regression\n",
                "from sklearn.metrics.pairwise import rbf_kernel\n",
                "from sklearn.utils import check_random_state\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.feature_selection import mutual_info_regression\n",
                "from sklearn.preprocessing import KBinsDiscretizer\n",
                "from sklearn.model_selection import KFold\n",
                "from lightgbm import LGBMRegressor\n",
                "from scipy.special import softmax\n",
                "from scipy.stats import chi2_contingency, ks_2samp\n",
                "from scipy.spatial.distance import pdist, squareform\n",
                "from tqdm.auto import tqdm\n",
                "from scipy import stats\n",
                "from scipy.stats import entropy\n",
                "import statsmodels.api as sm\n",
                "import pingouin as pg\n",
                "from pgmpy.estimators import CITests\n",
                "from scipy.spatial.distance import pdist, squareform\n",
                "from econml.dml import CausalForestDML\n",
                "import lightgbm as lgb\n",
                "from sklearn.base import BaseEstimator, RegressorMixin\n",
                "from sklearn.exceptions import NotFittedError\n",
                "from sklearn.model_selection import KFold\n",
                "from lightgbm import LGBMRegressor\n",
                "from causallearn.graph.GraphNode import GraphNode\n",
                "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge\n",
                "from causallearn.utils.GraphUtils import GraphUtils\n",
                "from causallearn.search.ConstraintBased.PC import pc\n",
                "from causallearn.search.ScoreBased.ExactSearch import bic_exact_search\n",
                "from causallearn.search.FCMBased import lingam\n",
                "from causallearn.search.ConstraintBased.FCI import fci\n",
                "from causallearn.utils.GraphUtils import GraphUtils\n",
                "from causallearn.utils.cit import fisherz\n",
                "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge\n",
                "from causallearn.graph.Node import Node\n",
                "from causallearn.graph.GraphNode import GraphNode"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-20T02:42:13.047359Z",
                    "start_time": "2024-09-20T02:42:12.325055Z"
                },
                "metadata": {}
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "loaded inline runner with module: <module '__main__'>\n"
                    ]
                }
            ],
            "source": [
                "import crunch\n",
                "crunch = crunch.load_notebook()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:30:54.182971Z",
                    "start_time": "2024-09-18T09:30:54.145678Z"
                }
            },
            "outputs": [],
            "source": [
                "def graph_nodes_representation(graph, nodelist):\n",
                "    \"\"\"\n",
                "    Create an alternative representation of a graph which is hashable\n",
                "    and equivalent graphs have the same hash.\n",
                "\n",
                "    Python cannot PROPERLY use nx.Graph/DiGraph as key for\n",
                "    dictionaries, because two equivalent graphs with just different\n",
                "    order of the nodes would result in different keys. This is\n",
                "    undesirable here.\n",
                "\n",
                "    So here we transform the graph into an equivalent form that is\n",
                "    based on a specific nodelist and that is hashable. In this way,\n",
                "    two equivalent graphs, once transformed, will result in identical\n",
                "    keys.\n",
                "\n",
                "    So we use the following trick: extract the adjacency matrix\n",
                "    (with nodes in a fixed order) and then make a hashable thing out\n",
                "    of it, through tuple(array.flatten()):\n",
                "    \"\"\"\n",
                "\n",
                "    # This get the adjacency matrix with nodes in a given order, as\n",
                "    # numpy array (which is not hashable):\n",
                "    adjacency_matrix = nx.adjacency_matrix(graph, nodelist=nodelist).todense()\n",
                "\n",
                "    # This transforms the numpy array into a hashable object:\n",
                "    hashable = tuple(adjacency_matrix.flatten())\n",
                "\n",
                "    return hashable\n",
                "\n",
                "def create_graph_label():\n",
                "    \"\"\"\n",
                "    Create a dictionary from graphs to labels, in two formats.\n",
                "    \"\"\"\n",
                "    graph_label = {\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]):             \"Cause of X\",\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]):             \"Cause of Y\",\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]):             \"Consequence of X\",\n",
                "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]):             \"Consequence of Y\",\n",
                "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}):                \"Independent\",\n",
                "    }\n",
                "\n",
                "    nodelist = [\"v\", \"X\", \"Y\"]\n",
                "\n",
                "    # This is an equivalent alternative to graph_label but in a form\n",
                "    # for which two equivalent graphs have the same key:\n",
                "    adjacency_label = {\n",
                "        graph_nodes_representation(graph, nodelist): label\n",
                "        for graph, label in graph_label.items()\n",
                "    }\n",
                "\n",
                "    return graph_label, adjacency_label\n",
                "\n",
                "def get_labels(adjacency_matrix, adjacency_label):\n",
                "    \"\"\"\n",
                "    Transform an adjacency_matrix (as pd.DataFrame) into a dictionary of variable:label\n",
                "    \"\"\"\n",
                "\n",
                "    result = {}\n",
                "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
                "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]  # this is not hashable\n",
                "        key = tuple(submatrix.values.flatten())  # this is hashable and a compatible with adjacency_label\n",
                "    \n",
                "        result[variable] = adjacency_label[key]\n",
                "\n",
                "    return result\n",
                "\n",
                "graph_label, adjacency_label = create_graph_label()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def apply_mapping(df, mapping):\n",
                "    df_new = df.copy()\n",
                "    # 创建临时映射以避免冲突\n",
                "    temp_mapping = {k: f'_temp_{k}' for k in mapping.keys()}\n",
                "    df_new.rename(columns=temp_mapping, inplace=True)\n",
                "    if df_new.shape[0] == df_new.shape[1]:  # 如果是方阵，如标签矩阵\n",
                "        df_new.rename(index=temp_mapping, inplace=True)\n",
                "    # 应用最终映射\n",
                "    final_mapping = {f'_temp_{k}': v for k, v in mapping.items()}\n",
                "    df_new.rename(columns=final_mapping, inplace=True)\n",
                "    if df_new.shape[0] == df_new.shape[1]:\n",
                "        df_new.rename(index=final_mapping, inplace=True)\n",
                "    return df_new\n",
                "\n",
                "def check_duplicate_columns(df):\n",
                "    \"\"\"检查是否存在重复的列名\"\"\"\n",
                "    return df.columns.duplicated().any()\n",
                "\n",
                "def augment_data(X_train, y_train):\n",
                "    new_X_train = X_train.copy()\n",
                "    new_y_train = y_train.copy()\n",
                "    for sample_id in X_train.keys():\n",
                "        X = X_train[sample_id]\n",
                "        y = y_train[sample_id]\n",
                "        variables = list(X.columns)\n",
                "        dim = len(variables)\n",
                "        # 提取因果关系对\n",
                "        edges = []\n",
                "        for u in y.index:\n",
                "            for v in y.columns:\n",
                "                if y.loc[u, v] == 1:\n",
                "                    edges.append((u, v))\n",
                "        # 排除涉及 X 和 Y 的边\n",
                "        edges_no_XY = [(u, v) for (u, v) in edges if u not in ['X', 'Y'] and v not in ['X', 'Y']]\n",
                "        if dim >= 4:\n",
                "            edges_to_use = edges_no_XY\n",
                "            attempts = 0\n",
                "            success = False\n",
                "            while attempts < 3 and not success:\n",
                "                if not edges_to_use:\n",
                "                    break  # 没有合适的边，跳出循环\n",
                "                u, v = random.choice(edges_to_use)\n",
                "                mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
                "                # 应用映射到特征矩阵和标签矩阵\n",
                "                X_new = apply_mapping(X, mapping)\n",
                "                y_new = apply_mapping(y, mapping)\n",
                "                # 检查特征矩阵是否有重复列\n",
                "                if check_duplicate_columns(X_new):\n",
                "                    attempts += 1\n",
                "                    continue  # 重试\n",
                "                else:\n",
                "                    # 没有重复列，存储新的数据\n",
                "                    new_sample_id = '0' + sample_id\n",
                "                    new_X_train[new_sample_id] = X_new\n",
                "                    new_y_train[new_sample_id] = y_new\n",
                "                    success = True\n",
                "            if not success:\n",
                "                # 没有找到合适的映射，复制原始数据\n",
                "                new_sample_id = '0' + sample_id\n",
                "                new_X_train[new_sample_id] = X.copy()\n",
                "                new_y_train[new_sample_id] = y.copy()\n",
                "        else:\n",
                "            # 对于维度较低的数据，允许涉及 X 和 Y 的边\n",
                "            edges_to_use = edges\n",
                "            if not edges_to_use:\n",
                "                # 没有边，复制原始数据\n",
                "                new_sample_id = '0' + sample_id\n",
                "                new_X_train[new_sample_id] = X.copy()\n",
                "                new_y_train[new_sample_id] = y.copy()\n",
                "                continue\n",
                "            u, v = random.choice(edges_to_use)\n",
                "            mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
                "            # 应用映射到特征矩阵和标签矩阵\n",
                "            X_new = apply_mapping(X, mapping)\n",
                "            y_new = apply_mapping(y, mapping)\n",
                "            # 检查特征矩阵是否有重复列\n",
                "            if check_duplicate_columns(X_new):\n",
                "                # 如果有重复列，复制原始数据\n",
                "                new_sample_id = '0' + sample_id\n",
                "                new_X_train[new_sample_id] = X.copy()\n",
                "                new_y_train[new_sample_id] = y.copy()\n",
                "            else:\n",
                "                # 没有重复列，存储新的数据\n",
                "                new_sample_id = '0' + sample_id\n",
                "                new_X_train[new_sample_id] = X_new\n",
                "                new_y_train[new_sample_id] = y_new\n",
                "    return new_X_train, new_y_train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-20T02:43:04.532840Z",
                    "start_time": "2024-09-20T02:42:51.402011Z"
                },
                "metadata": {}
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "download data\\X_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_train.pickle (1523944532 bytes)\n",
                        "already exists: file length match\n",
                        "download data\\y_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_train.pickle (7017735 bytes)\n",
                        "already exists: file length match\n",
                        "download data\\X_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_test_reduced.pickle (122341879 bytes)\n",
                        "already exists: file length match\n",
                        "download data\\y_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_test_reduced.pickle (562930 bytes)\n",
                        "already exists: file length match\n",
                        "download data\\example_prediction.parquet from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/example_prediction_reduced.parquet (668981 bytes)\n",
                        "already exists: file length match\n",
                        "23500 23500 1880\n"
                    ]
                }
            ],
            "source": [
                "X_train, y_train, X_test = crunch.load_data()\n",
                "print(len(X_train), len(y_train), len(X_test))\n",
                "# X_train, y_train = augment_data(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:31:23.461861Z",
                    "start_time": "2024-09-18T09:31:23.368017Z"
                },
                "metadata": {}
            },
            "outputs": [],
            "source": [
                "def pearson_correlation(dataset):\n",
                "    \"\"\"\n",
                "    Given a dataset, we compute the correlation-based features for each\n",
                "    varibale, which are the correlation between that variable with X and Y,\n",
                "    as well as summary statistics (max, min, mean, std) of all pairs\n",
                "    of correlations.\n",
                "    \"\"\"\n",
                "\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        tmp = dataset.corr().drop([variable], axis=\"columns\").loc[variable].abs()\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"corr(v,X)\": dataset[[variable, \"X\"]].corr().loc[variable, \"X\"],\n",
                "            \"corr(v,Y)\": dataset[[variable, \"Y\"]].corr().loc[variable, \"Y\"],\n",
                "            \"max(corr(v, others))\": tmp.max(),\n",
                "            \"min(corr(v, others))\": tmp.min(),\n",
                "            \"mean(corr(v, others))\": tmp.mean(),\n",
                "            \"std(corr(v, others))\": tmp.std(),\n",
                "            \"25%(corr(v, others))\": tmp.quantile(0.25), \n",
                "            \"75%(corr(v, others))\": tmp.quantile(0.75), \n",
                "        })\n",
                "\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    df[\"corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr().loc[\"X\", \"Y\"]\n",
                "\n",
                "    # pearsonr is NaN when the variance is 0, so we fill with 0\n",
                "    df.fillna(0, inplace=True)\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def mutual_information(dataset):\n",
                "    \"\"\"\n",
                "    Given a dataset, we compute the mutual-information-based features\n",
                "    for each variable, which are the MI between that variable\n",
                "    and X and Y, as well as summary statistics (max, min, mean, std) of\n",
                "    all pairs of MI.\n",
                "    \"\"\"\n",
                "\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        tmp = mutual_info_regression(dataset.drop(columns=[variable]), dataset[variable])\n",
                "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"MI(v,X)\": mutual_info_regression(dataset[[variable]], dataset[\"X\"], discrete_features=False)[0],\n",
                "            \"MI(v,Y)\": mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0],\n",
                "            \"max(MI(v, others))\": tmp.max(),\n",
                "            \"min(MI(v, others))\": tmp.min(),\n",
                "            \"mean(MI(v, others))\": tmp.mean(),\n",
                "            \"std(MI(v, others))\": tmp.std(),\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    df[\"MI(X,Y)\"] = mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def add_dimension_feature(dataset):\n",
                "    \"\"\"\n",
                "    Add a dimension feature to the dataset.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "    dimension = len(variables)\n",
                "    square_dimension = dimension * dimension\n",
                "    df = pd.DataFrame({\n",
                "        \"variable\": variables,\n",
                "        \"dimension\": dimension,\n",
                "        \"square_dimension\": square_dimension\n",
                "    })\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    return df\n",
                "\n",
                "def spearman_correlation(dataset):\n",
                "    \"\"\"\n",
                "    Given a dataset, we compute the Spearman rank correlation-based features for each\n",
                "    variable, which are the Spearman correlation between that variable with X and Y,\n",
                "    as well as summary statistics (max, min, mean, std) of all pairs of Spearman correlations.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        tmp = dataset.corr(method='spearman').drop([variable], axis=\"columns\").loc[variable].abs()\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"spearman_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='spearman').loc[variable, \"X\"],\n",
                "            \"spearman_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='spearman').loc[variable, \"Y\"],\n",
                "            \"max(spearman_corr(v, others))\": tmp.max(),\n",
                "            \"min(spearman_corr(v, others))\": tmp.min(),\n",
                "            \"mean(spearman_corr(v, others))\": tmp.mean(),\n",
                "            \"std(spearman_corr(v, others))\": tmp.std(),\n",
                "            #TODO 分位数\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    df[\"spearman_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='spearman').loc[\"X\", \"Y\"]\n",
                "\n",
                "    # Spearman correlation is NaN when there are ties in rank, so we fill with 0\n",
                "    df.fillna(0, inplace=True)\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def kendall_correlation(dataset):\n",
                "    \"\"\"\n",
                "    Given a dataset, we compute the Kendall's tau correlation-based features for each\n",
                "    variable, which are the Kendall's tau correlation between that variable with X and Y,\n",
                "    as well as summary statistics (max, min, mean, std) of all pairs of Kendall's tau correlations.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        tmp = dataset.corr(method='kendall').drop([variable], axis=\"columns\").loc[variable].abs()\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"kendall_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='kendall').loc[variable, \"X\"],\n",
                "            \"kendall_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='kendall').loc[variable, \"Y\"],\n",
                "            \"max(kendall_corr(v, others))\": tmp.max(),\n",
                "            \"min(kendall_corr(v, others))\": tmp.min(),\n",
                "            \"mean(kendall_corr(v, others))\": tmp.mean(),\n",
                "            \"std(kendall_corr(v, others))\": tmp.std(),\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    df[\"kendall_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='kendall').loc[\"X\", \"Y\"]\n",
                "\n",
                "    # Kendall's tau correlation can be NaN in some cases, so we fill with 0\n",
                "    df.fillna(0, inplace=True)\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def distance_correlation(dataset):\n",
                "    \"\"\"\n",
                "    Given a dataset, we compute the distance correlation-based features for each\n",
                "    variable, which are the distance correlation between that variable with X and Y,\n",
                "    as well as summary statistics (max, min, mean, std) of all pairs of distance correlations.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        tmp = []\n",
                "        # Compute distance correlation between 'variable' and all other variables (excluding itself)\n",
                "        other_variables = dataset.columns.drop([variable])\n",
                "        for other_var in other_variables:\n",
                "            corr = dcor.distance_correlation(dataset[variable], dataset[other_var])\n",
                "            tmp.append(corr)\n",
                "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
                "\n",
                "        distance_correlation_v_X = dcor.distance_correlation(dataset[variable], dataset[\"X\"])\n",
                "        distance_correlation_v_Y = dcor.distance_correlation(dataset[variable], dataset[\"Y\"])\n",
                "        distance_correlation_X_Y = dcor.distance_correlation(dataset[\"X\"], dataset[\"Y\"])\n",
                "        distance_correlation_v_X_square = distance_correlation_v_X ** 2\n",
                "        distance_correlation_v_Y_square = distance_correlation_v_Y ** 2\n",
                "        distance_correlation_X_Y_square = distance_correlation_X_Y ** 2\n",
                "        \n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"dcor(v,X)\": distance_correlation_v_X,\n",
                "            \"dcor(v,Y)\": distance_correlation_v_Y,\n",
                "            \"dcor(v,X)^2\": distance_correlation_v_X_square,\n",
                "            \"dcor(v,Y)^2\": distance_correlation_v_Y_square,\n",
                "            \"max(dcor(v, others))\": tmp.max(),\n",
                "            \"min(dcor(v, others))\": tmp.min(),\n",
                "            \"mean(dcor(v, others))\": tmp.mean(),\n",
                "            \"std(dcor(v, others))\": tmp.std(),\n",
                "            \"25%(dcor(v, others))\": tmp.quantile(0.25),# Success\n",
                "            \"75%(dcor(v, others))\": tmp.quantile(0.75),\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    df[\"dcor(X,Y)\"] = distance_correlation_X_Y\n",
                "    df[\"dcor(X,Y)^2\"] = distance_correlation_X_Y_square\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def double_centered_distance_matrix(X):\n",
                "    \"\"\"Compute the double centered distance matrix for X.\"\"\"\n",
                "    n = X.shape[0]\n",
                "    dist_matrix = squareform(pdist(X[:, None]))  # Compute pairwise distances\n",
                "    row_mean = np.mean(dist_matrix, axis=1, keepdims=True)\n",
                "    col_mean = np.mean(dist_matrix, axis=0, keepdims=True)\n",
                "    total_mean = np.mean(dist_matrix)\n",
                "\n",
                "    # Double-centering the matrix\n",
                "    A = dist_matrix - row_mean - col_mean + total_mean\n",
                "    return A\n",
                "\n",
                "def distance_covariance(X, Y):\n",
                "    \"\"\"Compute distance covariance between X and Y.\"\"\"\n",
                "    A = double_centered_distance_matrix(X)\n",
                "    B = double_centered_distance_matrix(Y)\n",
                "    dcov = np.mean(A * B)\n",
                "    return dcov\n",
                "\n",
                "def partial_distance_covariance(X, Y, Z):\n",
                "    \"\"\"Compute partial distance covariance between X and Y given Z.\"\"\"\n",
                "    dcov_XY = distance_covariance(X, Y)\n",
                "    dcov_XZ = distance_covariance(X, Z)\n",
                "    dcov_YZ = distance_covariance(Y, Z)\n",
                "    dcov_ZZ = distance_covariance(Z, Z)\n",
                "\n",
                "    # Partial distance covariance formula\n",
                "    pdcov = dcov_XY - (dcov_XZ * dcov_YZ) / dcov_ZZ\n",
                "    return pdcov\n",
                "\n",
                "def partial_distance_covariance_dataset(dataset):\n",
                "    \"\"\"\n",
                "    Given a dataset, we compute the partial distance covariance-based features for each\n",
                "    variable, which are the partial distance covariance between that variable with X and Y,\n",
                "    using each as the conditioning variable.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        pdcov_v_X_given_Y = partial_distance_covariance(dataset[variable].values, dataset[\"X\"].values, dataset[\"Y\"].values)\n",
                "        pdcov_v_Y_given_X = partial_distance_covariance(dataset[variable].values, dataset[\"Y\"].values, dataset[\"X\"].values)\n",
                "        pdcov_X_Y_given_v = partial_distance_covariance(dataset[\"X\"].values, dataset[\"Y\"].values, dataset[variable].values)\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"pdcov(v,X|Y)\": pdcov_v_X_given_Y,\n",
                "            \"pdcov(v,Y|X)\": pdcov_v_Y_given_X,\n",
                "            \"pdcov(X,Y|v)\": pdcov_X_Y_given_v,\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\", \"variable\", \"pdcov(v,X|Y)\", \"pdcov(v,Y|X)\", \"pdcov(X,Y|v)\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def Squared_term(dataset, variables):\n",
                "    for var in variables:\n",
                "        dataset[f'{var}_squared_term'] = dataset[var] ** 2\n",
                "    return dataset\n",
                "    \n",
                "def Interaction_term(dataset, variables):\n",
                "    for i in range(len(variables)):\n",
                "        for j in range(i + 1, len(variables)):\n",
                "            dataset[f'{variables[i]}_{variables[j]}'] = dataset[variables[i]] * dataset[variables[j]]\n",
                "    return dataset\n",
                "\n",
                "def Cos_Sin_term(dataset, variables):\n",
                "    for var in variables:\n",
                "        dataset[f'{var}_cos_term'] = np.cos(dataset[var])\n",
                "        dataset[f'{var}_sin_term'] = np.sin(dataset[var])\n",
                "    return dataset\n",
                "\n",
                "def linear_regression_feature(dataset):\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                "\n",
                "    # model1: Fit X, v, v^2, v_i*v_j, v_i*X ~ Y\n",
                "    model1_features = [\"X\"] + variables\n",
                "    d1 = Squared_term(dataset[model1_features], model1_features)\n",
                "    d1 = Interaction_term(d1, model1_features)  # 0.4600-0.4648\n",
                "    # d1 = Cos_Sin_term(d1, variables)\n",
                "    model1_features = d1.columns.tolist()\n",
                "    scaler = StandardScaler()\n",
                "    d1_scaled = scaler.fit_transform(d1)\n",
                "    model1 = LinearRegression().fit(d1_scaled, dataset[[\"Y\"]])\n",
                "    model1_coefs = model1.coef_[0].tolist()\n",
                "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
                "    \n",
                "    # model2: Fit v, v^2, cos(v)/sin(v) ~ X\n",
                "    model2_features = variables\n",
                "    d2 = Squared_term(dataset[model2_features], model2_features)\n",
                "    d2 = Interaction_term(d2, model2_features)  # 0.4648-0.4643\n",
                "    d2 = Cos_Sin_term(d2, model2_features)\n",
                "    model2_features = d2.columns.tolist()\n",
                "    scaler = StandardScaler()\n",
                "    d2_scaled = scaler.fit_transform(d2)\n",
                "    model2 = LinearRegression().fit(d2_scaled, dataset[[\"X\"]])\n",
                "    model2_coefs = model2.coef_[0].tolist()\n",
                "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
                "    \n",
                "    df = []\n",
                "    for i, variable in enumerate(variables):\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"v~Y_coefficient\": model1_dict[variable],\n",
                "            \"v_squared~Y_coefficient\": model1_dict[f\"{variable}_squared_term\"],\n",
                "            # \"v_cos~Y_coefficient\": model1_dict[f\"{variable}_cos_term\"],  # 掉分\n",
                "            # \"v_sin~Y_coefficient\": model1_dict[f\"{variable}_sin_term\"],\n",
                "            \"v~X_coefficient\": model2_dict[variable],\n",
                "            \"v_squared~X_coefficient\": model2_dict[f\"{variable}_squared_term\"],\n",
                "            \"v_cos~X_coefficient\": model2_dict[f\"{variable}_cos_term\"], \n",
                "            \"v_sin~X_coefficient\": model2_dict[f\"{variable}_sin_term\"],\n",
                "        })\n",
                "        \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    df[\"X~Y_coefficient\"] = model1_dict[\"X\"]\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def ridge_regression_feature(dataset):\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                "    # 使用GridSearchCV来选择最佳的alpha值\n",
                "    param_grid = {'alpha': np.logspace(-6, 6, 13)}\n",
                "    \n",
                "    # model1: Fit X, v ~ Y\n",
                "    model1_features = [\"X\"] + variables\n",
                "    d1 = Squared_term(dataset[model1_features], model1_features)\n",
                "    # d1 = Interaction_term(d1, model1_features) # 掉分\n",
                "    model1_features = d1.columns.tolist()\n",
                "    scaler1 = StandardScaler()\n",
                "    d1_scaled = scaler1.fit_transform(d1)\n",
                "    model1 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
                "    model1.fit(d1_scaled, dataset[\"Y\"])\n",
                "    model1_coefs = model1.best_estimator_.coef_.tolist()\n",
                "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
                "    \n",
                "    # model2: Fit v ~ X\n",
                "    model2_features = variables\n",
                "    d2 = Squared_term(dataset[model2_features], model2_features)\n",
                "    d2 = Interaction_term(d2, model2_features)\n",
                "    # d2 = Cos_Sin_term(d2, model2_features)  # 掉分\n",
                "    model2_features = d2.columns.tolist()\n",
                "    scaler2 = StandardScaler()\n",
                "    d2_scaled = scaler2.fit_transform(d2)\n",
                "    model2 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
                "    model2.fit(d2_scaled, dataset[\"X\"])\n",
                "    model2_coefs = model2.best_estimator_.coef_.tolist()\n",
                "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
                "\n",
                "    # # 获取最优的 alpha 值\n",
                "    # best_alpha_model1 = model1.best_params_['alpha']   # 0.4730-0.4727\n",
                "    # best_alpha_model2 = model2.best_params_['alpha']\n",
                "    \n",
                "    df = []\n",
                "    for i, variable in enumerate(variables):\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"v~Y_ridge_coefficient\": model1_dict[variable],\n",
                "            # \"v_squared~Y_ridge_coefficient\": model1_dict[f\"{variable}_squared_term\"],  # 掉分\n",
                "            \"v~X_ridge_coefficient\": model2_dict[variable],\n",
                "            # \"v_squared~X_ridge_coefficient\": model2_dict[f\"{variable}_squared_term\"],  # 掉分\n",
                "            # \"v_cos~X_ridge_coefficient\": model2_dict[f\"{variable}_cos_term\"],  # 掉分\n",
                "            # \"v_sin~X_ridge_coefficient\": model2_dict[f\"{variable}_sin_term\"],  # 掉分\n",
                "            # \"v~Y_ridge_alpha\": best_alpha_model1,\n",
                "            # \"v~X_ridge_alpha\": best_alpha_model2\n",
                "        })\n",
                "        \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    df[\"X~Y_ridge_coefficient\"] = model1_dict[\"X\"]\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def conditional_independence_tests(dataset):  # 太慢了，得三个小时\n",
                "    \"\"\"\n",
                "    A mixed-data residualization based conditional independence test[1].\n",
                "    Uses XGBoost estimator to compute LS residuals[2], and then does an association test (Pillai’s Trace) on the residuals.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        # v-X\n",
                "        coef1, p_value1 = CITests.ci_pillai(X=variable, Y=\"X\", Z=dataset.columns.drop([\"X\", variable]).tolist(), data=dataset, boolean=False)\n",
                "        # v-Y\n",
                "        coef2, p_value2 = CITests.ci_pillai(X=variable, Y=\"Y\", Z=dataset.columns.drop([\"Y\", variable]).tolist(), data=dataset, boolean=False)\n",
                "        # X-v\n",
                "        coef3, p_value3 = CITests.ci_pillai(X=\"X\", Y=variable, Z=dataset.columns.drop([\"X\", variable]).tolist(), data=dataset, boolean=False)\n",
                "        # Y-v\n",
                "        coef4, p_value4 = CITests.ci_pillai(X=\"Y\", Y=variable, Z=dataset.columns.drop([\"Y\", variable]).tolist(), data=dataset, boolean=False)\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"v~X_ci_pillai_coef\": coef1,\n",
                "            \"v~X_ci_pillai_p_value\": p_value1,\n",
                "            \"v~Y_ci_pillai_coef\": coef2,\n",
                "            \"v~Y_ci_pillai_p_value\": p_value2,\n",
                "            \"X~v_ci_pillai_coef\": coef3,\n",
                "            \"X~v_ci_pillai_p_value\": p_value3,\n",
                "            \"Y~v_ci_pillai_coef\": coef4,\n",
                "            \"Y~v_ci_pillai_p_value\": p_value4\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def conditional_mutual_information(dataset):\n",
                "    \"\"\"\n",
                "    Calculate conditional mutual information for each variable with X and Y.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "    \n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        # Calculate conditional MI(v, X | Y)\n",
                "        mi_vx_given_y = mutual_info_regression(dataset[[variable, \"Y\"]], dataset[\"X\"], discrete_features=False)[0] - \\\n",
                "                        mutual_info_regression(dataset[[\"Y\"]], dataset[\"X\"], discrete_features=False)[0]\n",
                "        \n",
                "        # Calculate conditional MI(v, Y | X)\n",
                "        mi_vy_given_x = mutual_info_regression(dataset[[variable, \"X\"]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
                "                        mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
                "        \n",
                "        # Calculate conditional MI(X, Y | v)\n",
                "        mi_xy_given_v = mutual_info_regression(dataset[[\"X\", variable]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
                "                        mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0]\n",
                "        \n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"conditional_MI(v,X|Y)\": mi_vx_given_y,\n",
                "            \"conditional_MI(v,Y|X)\": mi_vy_given_x,\n",
                "            \"conditional_MI(X,Y|v)\": mi_xy_given_v,\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "    \n",
                "    return df\n",
                "\n",
                "def partial_correlation(dataset):\n",
                "    \"\"\"\n",
                "    Compute partial correlation coefficients for each variable with X and Y,\n",
                "    controlling for the other variable, as well as the partial correlation\n",
                "    between X and Y controlling for each variable.\n",
                "    \"\"\"\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "    \n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        # Compute partial correlations\n",
                "        pcorr_vX_Y = pg.partial_corr(data=dataset, x=variable, y='X', covar='Y')['r'].iloc[0]\n",
                "        pcorr_vY_X = pg.partial_corr(data=dataset, x=variable, y='Y', covar='X')['r'].iloc[0]\n",
                "        pcorr_XY_v = pg.partial_corr(data=dataset, x='X', y='Y', covar=variable)['r'].iloc[0]\n",
                "        \n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"partial_corr(v,X|Y)\": pcorr_vX_Y,\n",
                "            \"partial_corr(v,Y|X)\": pcorr_vY_X,\n",
                "            \"partial_corr(X,Y|v)\": pcorr_XY_v,\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "    \n",
                "    return df\n",
                "\n",
                "\"\"\"因果发现算法的工具函数\"\"\"\n",
                "def handle_multicollinearity(data, epsilon=1e-7, corr_threshold=0.99):\n",
                "    \"\"\"\n",
                "    检查数据中的多重共线性，并在需要时向存在多重共线性的列添加随机扰动。\n",
                "    \n",
                "    参数:\n",
                "    - data (np.ndarray): 输入的数据矩阵，形状为 (样本数, 特征数)。\n",
                "    - epsilon (float): 添加的随机扰动的尺度，默认值为1e-10。\n",
                "    - corr_threshold (float): 判断高相关性的阈值，默认值为0.95。\n",
                "    \n",
                "    返回:\n",
                "    - data (np.ndarray): 处理后的数据矩阵。\n",
                "    \"\"\"\n",
                "    # 计算相关系数矩阵\n",
                "    corr_matrix = np.corrcoef(data, rowvar=False)\n",
                "    n_cols = corr_matrix.shape[0]\n",
                "\n",
                "    # 使用集合存储所有涉及多重共线性的列索引，避免重复\n",
                "    high_corr_indices = set()\n",
                "    \n",
                "    # 遍历相关系数矩阵的上三角部分，寻找高相关的列对\n",
                "    for i in range(n_cols):\n",
                "        for j in range(i+1, n_cols):\n",
                "            if np.abs(corr_matrix[i, j]) > corr_threshold:\n",
                "                high_corr_indices.add(i)\n",
                "                high_corr_indices.add(j)\n",
                "    \n",
                "    if high_corr_indices:\n",
                "        sorted_indices = sorted(high_corr_indices)\n",
                "        # print(f\"检测到多重共线性，涉及的列索引: {sorted_indices}。正在添加随机扰动...\")\n",
                "        \n",
                "        # 生成与高相关性列对应的随机扰动\n",
                "        noise = np.random.normal(0, epsilon, (data.shape[0], len(sorted_indices)))\n",
                "        \n",
                "        # 将扰动添加到相应的列\n",
                "        data[:, sorted_indices] += noise\n",
                "    \n",
                "    return data\n",
                "\n",
                "def convert_bidirectional_to_dag(bidirectional_adj: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    将双向编码的邻接矩阵转换为单向编码的邻接矩阵（DAG表示）。\n",
                "    \n",
                "    在双向编码的邻接矩阵中：\n",
                "    - [j, i] = 1 且 [i, j] = -1 表示 i → j\n",
                "    - [j, i] = 1 且 [i, j] = 1 表示 i ↔ j（互为因果）\n",
                "    - 其他情况 [i, j] = 0 表示无边\n",
                "    \n",
                "    转换后，单向编码的邻接矩阵将：\n",
                "    - [i, j] = 1 表示 i → j\n",
                "    - [i, j] = 1 且 [j, i] = 1 表示 i ↔ j\n",
                "    - [i, j] = 0 表示无边\n",
                "    \n",
                "    参数:\n",
                "    - bidirectional_adj (pd.DataFrame): 双向编码的邻接矩阵，使用 1 和 -1 表示有向边。\n",
                "    \n",
                "    返回:\n",
                "    - pd.DataFrame: 单向编码的邻接矩阵（DAG表示）。\n",
                "    \"\"\"\n",
                "    # 确保输入是一个方阵\n",
                "    if bidirectional_adj.shape[0] != bidirectional_adj.shape[1]:\n",
                "        raise ValueError(\"输入的邻接矩阵必须是方阵（行数等于列数）。\")\n",
                "    \n",
                "    # 确保行列索引一致\n",
                "    if not bidirectional_adj.index.equals(bidirectional_adj.columns):\n",
                "        raise ValueError(\"邻接矩阵的行索引和列索引必须相同。\")\n",
                "    \n",
                "    # 初始化一个全零的邻接矩阵\n",
                "    single_direction_adj = pd.DataFrame(0, index=bidirectional_adj.index, columns=bidirectional_adj.columns)\n",
                "    \n",
                "    # 遍历每一对变量，确定有向边\n",
                "    for source in bidirectional_adj.columns:\n",
                "        for target in bidirectional_adj.index:\n",
                "            if source == target:\n",
                "                # 根据DAG定义，通常不允许自环，因此设置为0\n",
                "                single_direction_adj.at[source, target] = 0\n",
                "                continue\n",
                "            \n",
                "            # 检查双向编码的邻接矩阵来确定有向边\n",
                "            if bidirectional_adj.at[target, source] == 1:\n",
                "                if bidirectional_adj.at[source, target] == -1:\n",
                "                    # 表示 source → target\n",
                "                    single_direction_adj.at[source, target] = 1\n",
                "                elif bidirectional_adj.at[source, target] == 1:\n",
                "                    # 表示 source ↔ target（互为因果）\n",
                "                    single_direction_adj.at[source, target] = 1\n",
                "                    single_direction_adj.at[target, source] = 1\n",
                "                # 如果 [source, target] == 0 或其他情况，不设置边\n",
                "    return single_direction_adj\n",
                "\n",
                "def convert_dag_with_strength_to_dag(adj: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    将表示因果影响强度的邻接矩阵数据框转换为二元邻接矩阵数据框。\n",
                "    无论存在正向影响还是负向影响，都将其转换为1；不存在影响则为0。\n",
                "    \n",
                "    参数:\n",
                "    - adj (pd.DataFrame): 原始的因果影响强度邻接矩阵，元素可以是正数、负数或0。\n",
                "    \n",
                "    返回:\n",
                "    - pd.DataFrame: 二元邻接矩阵，元素为0或1。\n",
                "    \"\"\"\n",
                "    # 确保输入是一个方阵\n",
                "    if adj.shape[0] != adj.shape[1]:\n",
                "        raise ValueError(\"输入的邻接矩阵必须是方阵（行数等于列数）。\")\n",
                "    \n",
                "    # 确保行列索引一致\n",
                "    if not adj.index.equals(adj.columns):\n",
                "        raise ValueError(\"邻接矩阵的行索引和列索引必须相同。\")\n",
                "    \n",
                "    # 将所有非零元素转换为1，零元素保持为0\n",
                "    return adj.ne(0).astype(int)\n",
                "\n",
                "\"\"\"因果发现算法\"\"\"\n",
                "def PC_estimate(dataset, alpha=0.05, indep_test='fisherz', kernel=None, stable=True, \n",
                "               uc_rule=0, uc_priority=2, verbose=False, show_progress=False):\n",
                "    \"\"\"\n",
                "    使用PC算法进行因果发现，并应用先验知识。\n",
                "\n",
                "    参数:\n",
                "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
                "    - alpha (float): 显著性水平，默认值为0.05。\n",
                "    - indep_test (str): 独立性检验方法，默认值为'fisherz'。\n",
                "    - kernel (str): 核函数类型，默认值为'linear'。\n",
                "    - stable (bool): 是否运行稳定的骨架发现，默认值为True。\n",
                "    - uc_rule (int): 未屏蔽碰撞点的定向规则，默认值为0。\n",
                "    - uc_priority (int): 解决未屏蔽碰撞点冲突的优先规则，默认值为2。\n",
                "    - verbose (bool): 是否打印详细输出，默认值为False。\n",
                "    - show_progress (bool): 是否显示算法进度，默认值为False。\n",
                "\n",
                "    返回:\n",
                "    - adj_df (pd.DataFrame): 因果图的邻接矩阵，格式为pivot_table。\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. 将数据框转换为numpy.ndarray\n",
                "    data = dataset.values\n",
                "\n",
                "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
                "    data = handle_multicollinearity(data)\n",
                "    \n",
                "    # 2. 定义先验知识：'X' → 'Y'\n",
                "    # 创建GraphNode对象\n",
                "    try:\n",
                "        node_X = GraphNode('X')\n",
                "        node_Y = GraphNode('Y')\n",
                "    except Exception as e:\n",
                "        raise ValueError(\"确保数据框中包含名为'X'和'Y'的列。\") from e\n",
                "    \n",
                "    # 初始化BackgroundKnowledge对象并添加先验知识\n",
                "    bk = BackgroundKnowledge().add_required_by_node(node_X, node_Y)\n",
                "\n",
                "    # 3. 配置核参数\n",
                "    if indep_test == 'kci':\n",
                "        if kernel is None:\n",
                "            kernel = 'linear'\n",
                "        if kernel == 'linear':\n",
                "            kernel_kwargs = {\n",
                "                'kernelX': 'Linear', \n",
                "                'kernelY': 'Linear', \n",
                "                'kernelZ': 'Linear', \n",
                "                'approx': True,           # 使用伽玛近似\n",
                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                "            }\n",
                "        elif kernel == 'polynomial':\n",
                "            kernel_kwargs = {\n",
                "                'kernelX': 'Polynomial', \n",
                "                'kernelY': 'Polynomial', \n",
                "                'kernelZ': 'Polynomial', \n",
                "                'polyd': 3,               # 多项式次数设置为3\n",
                "                'approx': True,           # 使用伽玛近似\n",
                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                "            }\n",
                "        elif kernel == 'gaussian':\n",
                "            kernel_kwargs = {\n",
                "                'kernelX': 'Gaussian', \n",
                "                'kernelY': 'Gaussian', \n",
                "                'kernelZ': 'Gaussian', \n",
                "                'est_width': 'empirical', # 使用经验宽度\n",
                "                'approx': True,           # 使用伽玛近似\n",
                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                "            }\n",
                "        elif kernel == 'mix':\n",
                "            kernel_kwargs = {\n",
                "                'kernelX': 'Polynomial', \n",
                "                'kernelY': 'Polynomial', \n",
                "                'kernelZ': 'Gaussian',     # Z使用高斯核\n",
                "                'polyd': 3,                # 多项式次数设置为3\n",
                "                'est_width': 'median',     # Z的高斯核带宽使用中位数技巧\n",
                "                'approx': True,            # 使用伽玛近似\n",
                "                'nullss': 1000,           # 原假设下模拟的样本量\n",
                "            }\n",
                "        else:\n",
                "            raise ValueError(f'Unknown kernel: {kernel}')\n",
                "    else:\n",
                "        kernel_kwargs = {}\n",
                "\n",
                "    # 4. 运行PC算法，传入先验知识\n",
                "    cg = pc(data, alpha=alpha, indep_test=indep_test, stable=stable, uc_rule=uc_rule, uc_priority=uc_priority, \n",
                "            background_knowledge=bk, verbose=verbose, show_progress=show_progress, **kernel_kwargs)\n",
                "    \n",
                "    # 5. 提取邻接矩阵\n",
                "    adj_matrix = cg.G.graph\n",
                "    # 6. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
                "    adj_df = pd.DataFrame(adj_matrix, index=dataset.columns, columns=dataset.columns)\n",
                "    \n",
                "    return adj_df\n",
                "\n",
                "def PC_feature(dataset):\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                "\n",
                "    estimate_adj_df_bidirectional = PC_estimate(dataset)  # 双向的估计因果图\n",
                "    estimate_adj_df_dag = convert_bidirectional_to_dag(estimate_adj_df_bidirectional)  # 将双向图转换为有向图\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        # 检查变量与'X'和'Y'之间的边\n",
                "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
                "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
                "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
                "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
                "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"PC(v,X)\": v_to_X,\n",
                "            \"PC(X,v)\": X_to_v,\n",
                "            \"PC(v,Y)\": v_to_Y,\n",
                "            \"PC(Y,v)\": Y_to_v,\n",
                "            \"PC(X,Y)\": X_to_Y\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def ExactSearch_estimate(dataset, search_method='astar', use_path_extension=True, \n",
                "                        use_k_cycle_heuristic=False, k=3, max_parents=None):\n",
                "    \"\"\"\n",
                "    使用Exact Search算法进行因果发现。\n",
                "    \n",
                "    参数:\n",
                "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
                "    - search_method (str): Exact Search方法，'astar'或'dp'。默认值为'astar'。\n",
                "    - use_path_extension (bool): 是否使用路径扩展。默认值为True。\n",
                "    - use_k_cycle_heuristic (bool): 是否使用k-cycle冲突启发式。仅适用于'astar'方法。默认值为False。\n",
                "    - k (int): k-cycle启发式的参数。默认值为3。\n",
                "    - max_parents (int or None): 节点的最大父节点数量。默认值为None。\n",
                "    \n",
                "    返回:\n",
                "    - adj_df (pd.DataFrame): 因果图的邻接矩阵，格式为DataFrame。\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. 将数据框转换为numpy.ndarray\n",
                "    data = dataset.values\n",
                "\n",
                "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
                "    data = handle_multicollinearity(data)\n",
                "\n",
                "    # 2. 定义X→Y的超级图\n",
                "    d = dataset.shape[1]   \n",
                "    super_graph = np.ones((d, d), dtype=int)  # 初始化为全1，表示所有边默认允许\n",
                "    columns = dataset.columns.tolist()\n",
                "    x_idx = columns.index('X')\n",
                "    y_idx = columns.index('Y')\n",
                "    super_graph[y_idx][x_idx] = 0     # 禁止'Y'→'X'\n",
                "    np.fill_diagonal(super_graph, 0)  # 禁止自环：确保对角线为0\n",
                "    \n",
                "    # 3. 运行Exact Search算法，使用指定的参数\n",
                "    dag_est, search_stats = bic_exact_search(X=data, super_graph=super_graph, search_method=search_method,\n",
                "        use_path_extension=use_path_extension, use_k_cycle_heuristic=use_k_cycle_heuristic,\n",
                "        k=k, verbose=False, max_parents=max_parents)\n",
                "    \n",
                "    # 4. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
                "    adj_df = pd.DataFrame(dag_est, index=dataset.columns, columns=dataset.columns)\n",
                "    \n",
                "    return adj_df\n",
                "\n",
                "def ExactSearch_feature(dataset):\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                "\n",
                "    estimate_adj_df = ExactSearch_estimate(dataset)\n",
                "    estimate_adj_df_dag = estimate_adj_df.astype(int)  # 将双向图转换为有向图\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        # 检查变量与'X'和'Y'之间的边\n",
                "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
                "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
                "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
                "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
                "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
                "\n",
                "        # v_to_others = sum(estimate_adj_df_dag.loc[variable, variables])\n",
                "        # others_to_v = sum(estimate_adj_df_dag.loc[variables, variable])\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"ExactSearch(v,X)\": v_to_X,\n",
                "            \"ExactSearch(X,v)\": X_to_v,\n",
                "            \"ExactSearch(v,Y)\": v_to_Y,\n",
                "            \"ExactSearch(Y,v)\": Y_to_v,\n",
                "            \"ExactSearch(X,Y)\": X_to_Y\n",
                "            # \"ExactSearch(v,others)\": v_to_others,\n",
                "            # \"ExactSearch(others,v)\": others_to_v\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "def FCI_estimate(dataset, alpha=0.05, indep_test='fisherz', kernel=None, \n",
                "               depth=-1, max_path_length=-1, verbose=False, show_progress=False):\n",
                "    \"\"\"\n",
                "    使用FCI算法进行因果发现，并应用先验知识。\n",
                "\n",
                "    参数:\n",
                "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
                "    - alpha (float): 显著性水平，默认值为0.05。\n",
                "    - indep_test (str): 独立性检验方法，默认值为'fisherz'。\n",
                "    - kernel (str): 核函数类型，默认值为'linear'。\n",
                "    - verbose (bool): 是否打印详细输出，默认值为False。\n",
                "    - show_progress (bool): 是否显示算法进度，默认值为False。\n",
                "\n",
                "    返回:\n",
                "    - adj_df (pd.DataFrame): 因果图的邻接矩阵，格式为pivot_table。\n",
                "    \"\"\"\n",
                "    \n",
                "\n",
                "    # 1. 将数据框转换为numpy.ndarray\n",
                "    data = dataset.values\n",
                "\n",
                "    # 2. 直接给整个数据集添加小的随机噪声\n",
                "    epsilon = 5e-7  # 可以根据需要调整这个值\n",
                "    noise = np.random.normal(0, epsilon, data.shape)\n",
                "    data = data + noise\n",
                "\n",
                "\n",
                "    # 2. 定义先验知识：'X' → 'Y'\n",
                "    # 创建GraphNode对象\n",
                "    try:\n",
                "        node_X = GraphNode('X')\n",
                "        node_Y = GraphNode('Y')\n",
                "    except Exception as e:\n",
                "        raise ValueError(\"确保数据框中包含名为'X'和'Y'的列。\") from e\n",
                "    \n",
                "    # 初始化BackgroundKnowledge对象并添加先验知识\n",
                "    bk = BackgroundKnowledge().add_required_by_node(node_X, node_Y)\n",
                "\n",
                "    # 3. 配置核参数\n",
                "    if indep_test == 'kci':\n",
                "        if kernel is None:\n",
                "            kernel = 'linear'\n",
                "        if kernel == 'linear':\n",
                "            kernel_kwargs = {\n",
                "                'kernelX': 'Linear', \n",
                "                'kernelY': 'Linear', \n",
                "                'kernelZ': 'Linear', \n",
                "                'approx': True,           # 使用伽玛近似\n",
                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                "            }\n",
                "        elif kernel == 'polynomial':\n",
                "            kernel_kwargs = {\n",
                "                'kernelX': 'Polynomial', \n",
                "                'kernelY': 'Polynomial', \n",
                "                'kernelZ': 'Polynomial', \n",
                "                'polyd': 3,               # 多项式次数设置为3\n",
                "                'approx': True,           # 使用伽玛近似\n",
                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                "            }\n",
                "        elif kernel == 'gaussian':\n",
                "            kernel_kwargs = {\n",
                "                'kernelX': 'Gaussian', \n",
                "                'kernelY': 'Gaussian', \n",
                "                'kernelZ': 'Gaussian', \n",
                "                'est_width': 'empirical', # 使用经验宽度\n",
                "                'approx': True,           # 使用伽玛近似\n",
                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                "            }\n",
                "        elif kernel == 'mix':\n",
                "            kernel_kwargs = {\n",
                "                'kernelX': 'Polynomial', \n",
                "                'kernelY': 'Polynomial', \n",
                "                'kernelZ': 'Gaussian',     # Z使用高斯核\n",
                "                'polyd': 3,                # 多项式次数设置为3\n",
                "                'est_width': 'median',     # Z的高斯核带宽使用中位数技巧\n",
                "                'approx': True,            # 使用伽玛近似\n",
                "                'nullss': 1000,           # 原假设下模拟的样本量\n",
                "            }\n",
                "        else:\n",
                "            raise ValueError(f'Unknown kernel: {kernel}')\n",
                "    else:\n",
                "        kernel_kwargs = {}\n",
                "\n",
                "    # 4. 运行FCI算法，传入先验知识\n",
                "    g, edges = fci(data, \n",
                "            alpha=alpha, \n",
                "            independence_test_method=indep_test, \n",
                "            depth=depth,\n",
                "            max_path_length=max_path_length,\n",
                "            background_knowledge=bk, \n",
                "            verbose=verbose, \n",
                "            show_progress=show_progress,\n",
                "            **kernel_kwargs\n",
                "    )\n",
                "    \n",
                "    # 5. 提取邻接矩阵\n",
                "    adj_matrix = g.graph\n",
                "    \n",
                "    # 6. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
                "    adj_df = pd.DataFrame(adj_matrix, index=dataset.columns, columns=dataset.columns)\n",
                "    \n",
                "    return adj_df\n",
                "\n",
                "def FCI_feature(dataset):\n",
                "    \"\"\"\n",
                "    使用FCI_estimate函数进行因果发现，并从adj_df中提取特征。\n",
                "    \n",
                "    参数:\n",
                "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
                "    \n",
                "    返回:\n",
                "    - df (pd.DataFrame): 包含提取的特征，格式为DataFrame。\n",
                "    \"\"\"\n",
                "    adj_df = FCI_estimate(dataset)\n",
                "    \n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                "    \n",
                "    # 提取特征\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        v_to_X = adj_df.loc[variable, 'X']\n",
                "        X_to_v = adj_df.loc['X', variable]\n",
                "        v_to_Y = adj_df.loc[variable, 'Y']\n",
                "        Y_to_v = adj_df.loc['Y', variable]\n",
                "        X_to_Y = adj_df.loc['X', 'Y']\n",
                "        \n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"FCI(v,X)\": v_to_X,\n",
                "            \"FCI(X,v)\": X_to_v,\n",
                "            \"FCI(v,Y)\": v_to_Y,\n",
                "            \"FCI(Y,v)\": Y_to_v,\n",
                "            \"FCI(X,Y)\": X_to_Y\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    # 调整列的顺序\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "    \n",
                "    return df\n",
                "\n",
                "\n",
                "\n",
                "\"\"\"DML-有内存泄漏\"\"\"\n",
                "def DML_estimate(T_, Y_, X_, data):\n",
                "    # 设置处理变量、结果变量和控制变量\n",
                "    T = data[T_].values.reshape(-1, 1)\n",
                "    Y = data[Y_].values.reshape(-1, 1)\n",
                "    X = data[X_].values\n",
                "\n",
                "    # 初始化 CausalForestDML 使用自定义的估计器，并设置 random_state\n",
                "    model = CausalForestDML(\n",
                "        model_t=\"forest\",\n",
                "        model_y=\"forest\",\n",
                "        cv=2,\n",
                "        n_estimators=36,\n",
                "        n_jobs=1,\n",
                "        random_state=42,  # 确保随机性的一致性\n",
                "        inference=True\n",
                "    )\n",
                "    model.fit(Y, T, X=X)\n",
                "\n",
                "    # 平均边际处理效应的推断结果\n",
                "    amte_inference = model.ate_inference(X=X)\n",
                "\n",
                "    return amte_inference\n",
                "\n",
                "def DML_feature(dataset):\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        # 判断v-X的因果效应，设置variables中的其他v和Y为控制变量\n",
                "        amte_inference1 = DML_estimate(variable, \"X\", [\"Y\"] + list(variables.drop(variable)), dataset)\n",
                "        # 判断v-Y的因果效应，设置variables中的其他v和X为控制变量\n",
                "        amte_inference2 = DML_estimate(variable, \"Y\", [\"X\"] + list(variables.drop(variable)), dataset)\n",
                "        # 判断X-v的因果效应，设置variables中的其他v和Y为控制变量\n",
                "        amte_inference3 = DML_estimate(\"X\", variable, [\"Y\"] + list(variables.drop(variable)), dataset)\n",
                "        # 判断Y-v的因果效应，设置variables中的其他v和X为控制变量\n",
                "        amte_inference4 = DML_estimate(\"Y\", variable, [\"X\"] + list(variables.drop(variable)), dataset)\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"v~X_DML_AMTE\": amte_inference1.mean_point,  \n",
                "            \"v~X_DML_AMTE_zstat\": amte_inference1.zstat(),\n",
                "            \"v~X_DML_AMTE_pvalue\": amte_inference1.pvalue(),\n",
                "            \"v~X_DML_std_point\": amte_inference1.std_point,\n",
                "            \"v~X_DML_stderr_point\": amte_inference1.stderr_point,\n",
                "\n",
                "            \"v~Y_DML_AMTE\": amte_inference2.mean_point,\n",
                "            \"v~Y_DML_AMTE_zstat\": amte_inference2.zstat(),\n",
                "            \"v~Y_DML_AMTE_pvalue\": amte_inference2.pvalue(),\n",
                "            \"v~Y_DML_std_point\": amte_inference2.std_point,\n",
                "            \"v~Y_DML_stderr_point\": amte_inference2.stderr_point,\n",
                "\n",
                "            \"X~v_DML_AMTE\": amte_inference3.mean_point,\n",
                "            \"X~v_DML_AMTE_zstat\": amte_inference3.zstat(),\n",
                "            \"X~v_DML_AMTE_pvalue\": amte_inference3.pvalue(),\n",
                "            \"X~v_DML_std_point\": amte_inference3.std_point,\n",
                "            \"X~v_DML_stderr_point\": amte_inference3.stderr_point,\n",
                "\n",
                "            \"Y~v_DML_AMTE\": amte_inference4.mean_point,\n",
                "            \"Y~v_DML_AMTE_zstat\": amte_inference4.zstat(),\n",
                "            \"Y~v_DML_AMTE_pvalue\": amte_inference4.pvalue(),\n",
                "            \"Y~v_DML_std_point\": amte_inference4.std_point,\n",
                "            \"Y~v_DML_stderr_point\": amte_inference4.stderr_point,\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n",
                "\n",
                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = []\n",
                "    for variable in variables:\n",
                "        # 判断Y-v的因果效应，设置variables中的其他v和X为控制变量\n",
                "        amte_inference4 = DML_estimate(\"Y\", variable, [\"X\"] + list(variables.drop(variable)), dataset)\n",
                "\n",
                "        df.append({\n",
                "            \"variable\": variable,\n",
                "            \"Y~v_DML_AMTE\": amte_inference4.mean_point,\n",
                "            \"Y~v_DML_AMTE_zstat\": amte_inference4.zstat(),\n",
                "            \"Y~v_DML_AMTE_pvalue\": amte_inference4.pvalue(),\n",
                "            \"Y~v_DML_std_point\": amte_inference4.std_point,\n",
                "            \"Y~v_DML_stderr_point\": amte_inference4.stderr_point,\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(df)\n",
                "    df[\"dataset\"] = dataset.name\n",
                "    \n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:31:25.402707Z",
                    "start_time": "2024-09-18T09:31:25.388081Z"
                },
                "metadata": {}
            },
            "outputs": [],
            "source": [
                "def label(adjacency_matrix):\n",
                "    \"\"\"\n",
                "    Given a graph as adjacency_matrix, create the class labels of each variable.\n",
                "    \"\"\"\n",
                "\n",
                "    adjacency_graph, adjacency_label = create_graph_label()\n",
                "    labels = get_labels(adjacency_matrix, adjacency_label)\n",
                "    variables = adjacency_matrix.columns.drop([\"X\", \"Y\"])\n",
                "\n",
                "    df = pd.DataFrame({\n",
                "        \"variable\": variables,\n",
                "        \"label\": [labels[variable] for variable in variables],\n",
                "    })\n",
                "    df[\"dataset\"] = adjacency_matrix.name\n",
                "\n",
                "    # Reorder columns:\n",
                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                "\n",
                "    return df\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:31:26.636102Z",
                    "start_time": "2024-09-18T09:31:26.629666Z"
                },
                "metadata": {}
            },
            "outputs": [],
            "source": [
                "def create_some_columns(names_datasets, function):\n",
                "    \"\"\"\n",
                "    Apply an embedding function to a list of datasets.\n",
                "    \"\"\"\n",
                "\n",
                "    df = []\n",
                "    for name, dataset in tqdm(names_datasets.items()):\n",
                "        dataset = names_datasets[name]\n",
                "        dataset.name = name\n",
                "    \n",
                "        try:\n",
                "            df_dataset = function(dataset)\n",
                "        except ValueError as e:\n",
                "            print(name, e)\n",
                "            raise NotImplementedError\n",
                "\n",
                "        df_dataset[\"dataset\"] = name\n",
                "        df.append(df_dataset)\n",
                "\n",
                "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:31:27.391364Z",
                    "start_time": "2024-09-18T09:31:27.380030Z"
                },
                "metadata": {}
            },
            "outputs": [],
            "source": [
                "def create_some_columns_parallel(names_datasets, function, n_jobs=-1):\n",
                "    \"\"\"\n",
                "    Apply an embedding function to a list of datasets.\n",
                "\n",
                "    Parallel version.\n",
                "    \"\"\"\n",
                "\n",
                "    def f(name, dataset, function):\n",
                "        dataset.name = name\n",
                "        df_dataset = function(dataset)\n",
                "        df_dataset[\"dataset\"] = name\n",
                "        return df_dataset\n",
                "\n",
                "    df = joblib.Parallel(n_jobs=n_jobs)(\n",
                "        joblib.delayed(f)(name, dataset, function)\n",
                "        for name, dataset in tqdm(names_datasets.items())\n",
                "    )\n",
                "\n",
                "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:31:28.411964Z",
                    "start_time": "2024-09-18T09:31:28.402880Z"
                },
                "metadata": {}
            },
            "outputs": [],
            "source": [
                "def create_all_columns(functions_names_datasets, n_jobs=-1, create_dimension_feature = False):\n",
                "    \"\"\"\n",
                "    given a dictionary of {function1:names, function2:names,...} apply\n",
                "    the desired functions to the list of datasets and merge all of them\n",
                "    in a single X_y_group dataframe.\n",
                "    \"\"\"\n",
                "\n",
                "    columns = []\n",
                "    if create_dimension_feature:\n",
                "        dimension_feature = create_some_columns(functions_names_datasets[list(functions_names_datasets.keys())[0]], add_dimension_feature)\n",
                "        columns.append(dimension_feature)\n",
                "    \n",
                "    for function, names_datasets in functions_names_datasets.items():\n",
                "        print(f\"set: {function.__name__}\")\n",
                "\n",
                "        if n_jobs != 1:\n",
                "            feature_set = create_some_columns_parallel(names_datasets, function, n_jobs=n_jobs)\n",
                "        else:\n",
                "            feature_set = create_some_columns(names_datasets, function)\n",
                "\n",
                "        columns.append(feature_set)\n",
                "\n",
                "    # Merge all feature sets into a single dataframe:\n",
                "    columns = functools.reduce(\n",
                "        lambda left, right: pd.merge(left, right, on=[\"dataset\", \"variable\"]),\n",
                "        columns,\n",
                "    )\n",
                "\n",
                "    return columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating X_y_group_train from 23500 datasets and graphs\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 12%|█▏        | 2881/23500 [00:01<00:10, 1895.94it/s]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m names_graphs_train \u001b[38;5;241m=\u001b[39m y_train\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating X_y_group_train from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(names_datasets_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m datasets and graphs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m X_y_group_train \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_all_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpearson_correlation\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutual_information\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# uncomment this line to add features but at high computational cost\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_graphs_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_dimension_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
                        "Cell \u001b[1;32mIn[17], line 10\u001b[0m, in \u001b[0;36mcreate_all_columns\u001b[1;34m(functions_names_datasets, n_jobs, create_dimension_feature)\u001b[0m\n\u001b[0;32m      8\u001b[0m columns \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_dimension_feature:\n\u001b[1;32m---> 10\u001b[0m     dimension_feature \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_some_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunctions_names_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunctions_names_datasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_dimension_feature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     columns\u001b[38;5;241m.\u001b[39mappend(dimension_feature)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m function, names_datasets \u001b[38;5;129;01min\u001b[39;00m functions_names_datasets\u001b[38;5;241m.\u001b[39mitems():\n",
                        "Cell \u001b[1;32mIn[15], line 12\u001b[0m, in \u001b[0;36mcreate_some_columns\u001b[1;34m(names_datasets, function)\u001b[0m\n\u001b[0;32m      9\u001b[0m dataset\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     df_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name, e)\n",
                        "Cell \u001b[1;32mIn[13], line 83\u001b[0m, in \u001b[0;36madd_dimension_feature\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     81\u001b[0m dimension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(variables)\n\u001b[0;32m     82\u001b[0m square_dimension \u001b[38;5;241m=\u001b[39m dimension \u001b[38;5;241m*\u001b[39m dimension\n\u001b[1;32m---> 83\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvariable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdimension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msquare_dimension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msquare_dimension\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
                        "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
                        "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
                        "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    149\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
                        "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2139\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_block_manager_from_column_arrays\u001b[39m(\n\u001b[0;32m   2122\u001b[0m     arrays: \u001b[38;5;28mlist\u001b[39m[ArrayLike],\n\u001b[0;32m   2123\u001b[0m     axes: \u001b[38;5;28mlist\u001b[39m[Index],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2135\u001b[0m     \u001b[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m     \u001b[38;5;66;03m#  verify_integrity=False below.\u001b[39;00m\n\u001b[0;32m   2138\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2139\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m \u001b[43m_form_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2140\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m BlockManager(blocks, axes, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2141\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2212\u001b[0m, in \u001b[0;36m_form_blocks\u001b[1;34m(arrays, consolidate, refs)\u001b[0m\n\u001b[0;32m   2209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype\u001b[38;5;241m.\u001b[39mtype, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m   2210\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m-> 2212\u001b[0m values, placement \u001b[38;5;241m=\u001b[39m \u001b[43m_stack_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtup_block\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dtlike:\n\u001b[0;32m   2214\u001b[0m     values \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n",
                        "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2254\u001b[0m, in \u001b[0;36m_stack_arrays\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   2252\u001b[0m stacked \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   2253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[1;32m-> 2254\u001b[0m     stacked[i] \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m   2256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stacked, placement\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "names_datasets_train = X_train\n",
                "names_graphs_train = y_train\n",
                "print(f\"Creating X_y_group_train from {len(names_datasets_train)} datasets and graphs\")\n",
                "X_y_group_train = create_all_columns(\n",
                "    {\n",
                "        pearson_correlation: names_datasets_train,\n",
                "        mutual_information: names_datasets_train,  # uncomment this line to add features but at high computational cost\n",
                "        label: names_graphs_train,\n",
                "    },\n",
                "    n_jobs=-1,\n",
                "    create_dimension_feature=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "names_datasets_train = X_train\n",
                "names_graphs_train = y_train\n",
                "X_y_group_train_additional = create_all_columns(\n",
                "    {\n",
                "        spearman_correlation: names_datasets_train,\n",
                "        kendall_correlation: names_datasets_train,\n",
                "        distance_correlation: names_datasets_train,\n",
                "        conditional_mutual_information: names_datasets_train,\n",
                "        linear_regression_feature: names_datasets_train,\n",
                "    },\n",
                "    n_jobs=-1,\n",
                ")\n",
                "\n",
                "\n",
                "X_y_group_train = pd.concat([X_y_group_train, X_y_group_train_additional], axis=1)\n",
                "# 去掉重复的列\n",
                "X_y_group_train = X_y_group_train.loc[:,~X_y_group_train.columns.duplicated()]\n",
                "X_y_group_train['MI(v,X)^2'] = X_y_group_train['MI(v,X)'] ** 2\n",
                "X_y_group_train['MI(v,Y)^2'] = X_y_group_train['MI(v,Y)'] ** 2\n",
                "X_y_group_train['MI(X,Y)^2'] = X_y_group_train['MI(X,Y)'] ** 2\n",
                "X_y_group_train['max(MI(v, others))^2'] = X_y_group_train['max(MI(v, others))'] ** 2\n",
                "X_y_group_train['min(MI(v, others))^2'] = X_y_group_train['min(MI(v, others))'] ** 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:44:41.738202Z",
                    "start_time": "2024-09-18T09:44:40.143933Z"
                },
                "metadata": {}
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(142910, 74)\n",
                        "Index(['dataset', 'variable', 'dimension', 'corr(v,X)', 'corr(v,Y)',\n",
                        "       'max(corr(v, others))', 'min(corr(v, others))', 'mean(corr(v, others))',\n",
                        "       'std(corr(v, others))', 'corr(X,Y)', 'ttest(v,X)',\n",
                        "       'pvalue(ttest(v,X))<=0.05', 'ttest(v,Y)', 'pvalue(ttest(v,Y))<=0.05',\n",
                        "       'ttest(X,Y)', 'pvalue(ttest(X,Y))<=0.05', 'MI(v,X)', 'MI(v,Y)',\n",
                        "       'max(MI(v, others))', 'min(MI(v, others))', 'mean(MI(v, others))',\n",
                        "       'std(MI(v, others))', 'MI(X,Y)', 'label', 'y', 'spearman_corr(v,X)',\n",
                        "       'spearman_corr(v,Y)', 'spearman_corr(X,Y)', 'kendall_corr(v,X)',\n",
                        "       'kendall_corr(v,Y)', 'kendall_corr(X,Y)', 'dcor(v,X)', 'dcor(v,Y)',\n",
                        "       'dcor(X,Y)', 'MI(v,X)^2', 'MI(v,Y)^2', 'MI(X,Y)^2',\n",
                        "       'max(MI(v, others))^2', 'min(MI(v, others))^2', 'conditional_MI(v,X|Y)',\n",
                        "       'conditional_MI(v,Y|X)', 'conditional_MI(X,Y|v)', 'square_dimension',\n",
                        "       'max(spearman_corr(v, others))', 'min(spearman_corr(v, others))',\n",
                        "       'mean(spearman_corr(v, others))', 'std(spearman_corr(v, others))',\n",
                        "       'max(kendall_corr(v, others))', 'min(kendall_corr(v, others))',\n",
                        "       'mean(kendall_corr(v, others))', 'std(kendall_corr(v, others))',\n",
                        "       'partial_corr(v,X|Y)', 'partial_corr(v,Y|X)', 'partial_corr(X,Y|v)',\n",
                        "       'v~Y_coefficient', 'v_squared~Y_coefficient', 'v~X_coefficient',\n",
                        "       'v_squared~X_coefficient', 'v_cos~X_coefficient', 'v_sin~X_coefficient',\n",
                        "       'X~Y_coefficient', 'v~Y_ridge_coefficient', 'v~X_ridge_coefficient',\n",
                        "       'X~Y_ridge_coefficient', 'PC(v,X)', 'PC(X,v)', 'PC(v,Y)', 'PC(Y,v)',\n",
                        "       'PC(X,Y)', 'ExactSearch(v,X)', 'ExactSearch(X,v)', 'ExactSearch(v,Y)',\n",
                        "       'ExactSearch(Y,v)', 'ExactSearch(X,Y)'],\n",
                        "      dtype='object')\n"
                    ]
                }
            ],
            "source": [
                "X_y_group_train = pd.read_csv('mid_data/X_y_group_train_updated_v9.2_PC+ES.csv')\n",
                "print(X_y_group_train.shape)\n",
                "print(X_y_group_train.columns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "set: FCI_feature\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": []
                },
                {
                    "ename": "KeyError",
                    "evalue": "(2, 5)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
                        "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Ori\\AppData\\Local\\Temp\\ipykernel_11272\\1759249529.py\", line 10, in f\n  File \"C:\\Users\\Ori\\AppData\\Local\\Temp\\ipykernel_11272\\3448764000.py\", line 930, in FCI_feature\n  File \"C:\\Users\\Ori\\AppData\\Local\\Temp\\ipykernel_11272\\3448764000.py\", line 901, in FCI_estimate\n  File \"e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\causallearn\\search\\ConstraintBased\\FCI.py\", line 775, in fci\n    change_flag = ruleR3(graph, sep_sets, background_knowledge, change_flag, verbose)\n  File \"e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\causallearn\\search\\ConstraintBased\\FCI.py\", line 351, in ruleR3\n    if not isNoncollider(graph, sep_sets, node_A, node_D, node_C):\n  File \"e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\causallearn\\search\\ConstraintBased\\FCI.py\", line 323, in isNoncollider\n    sep_set = sep_sets[(graph.get_node_map()[node_i], graph.get_node_map()[node_k])]\nKeyError: (2, 5)\n\"\"\"",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m names_datasets_train \u001b[38;5;241m=\u001b[39m X_train\n\u001b[0;32m      2\u001b[0m names_graphs_train \u001b[38;5;241m=\u001b[39m y_train\n\u001b[1;32m----> 3\u001b[0m X_y_group_train_additional \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_all_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# conditional_independence_tests: names_datasets_train,\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# partial_distance_covariance_dataset: names_datasets_train,\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# PC_feature: names_datasets_train,\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mFCI_feature\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_datasets_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m X_y_group_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_y_group_train, X_y_group_train_additional], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_y_group_train.shape\u001b[39m\u001b[38;5;124m'\u001b[39m, X_y_group_train\u001b[38;5;241m.\u001b[39mshape)\n",
                        "Cell \u001b[1;32mIn[17], line 17\u001b[0m, in \u001b[0;36mcreate_all_columns\u001b[1;34m(functions_names_datasets, n_jobs, create_dimension_feature)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 17\u001b[0m     feature_set \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_some_columns_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     feature_set \u001b[38;5;241m=\u001b[39m create_some_columns(names_datasets, function)\n",
                        "Cell \u001b[1;32mIn[16], line 14\u001b[0m, in \u001b[0;36mcreate_some_columns_parallel\u001b[1;34m(names_datasets, function, n_jobs)\u001b[0m\n\u001b[0;32m     11\u001b[0m     df_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_dataset\n\u001b[1;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames_datasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(df, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
                        "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
                        "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
                        "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
                        "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
                        "\u001b[1;31mKeyError\u001b[0m: (2, 5)"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": []
                }
            ],
            "source": [
                "names_datasets_train = X_train\n",
                "names_graphs_train = y_train\n",
                "X_y_group_train_additional = create_all_columns(\n",
                "    {\n",
                "        # conditional_independence_tests: names_datasets_train,\n",
                "        # partial_distance_covariance_dataset: names_datasets_train,\n",
                "        # PC_feature: names_datasets_train,\n",
                "        FCI_feature: names_datasets_train,\n",
                "    },\n",
                "    n_jobs=-1,\n",
                ")\n",
                "X_y_group_train = pd.concat([X_y_group_train, X_y_group_train_additional], axis=1)\n",
                "print('X_y_group_train.shape', X_y_group_train.shape)\n",
                "# 去掉重复的列\n",
                "X_y_group_train = X_y_group_train.loc[:,~X_y_group_train.columns.duplicated()]\n",
                "print('去重后X_y_group_train.shape', X_y_group_train.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "set: double_machine_learning\n",
                "  1%|          | 288/23500 [02:17<2:48:55,  2.29it/s]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:34:49.212104Z",
                    "start_time": "2024-09-18T09:34:48.478623Z"
                },
                "metadata": {}
            },
            "outputs": [],
            "source": [
                "print(\"Adding numeric labels y\")\n",
                "le = LabelEncoder()\n",
                "X_y_group_train[\"y\"] = le.fit_transform(X_y_group_train[\"label\"])\n",
                "# reordering columns:\n",
                "X_y_group_train = X_y_group_train[[\"dataset\", \"variable\"] + X_y_group_train.columns.drop([\"dataset\", \"variable\", \"label\", \"y\"]).tolist() + [\"label\", \"y\"]]\n",
                "\n",
                "numeric_columns = X_y_group_train.select_dtypes(include=[np.number]).columns\n",
                "X_y_group_train[numeric_columns] = X_y_group_train[numeric_columns].fillna(X_y_group_train[numeric_columns].mean())\n",
                "\n",
                "blacklist = [\"ttest(v,X)\", \"pvalue(ttest(v,X))<=0.05\", \"ttest(v,Y)\", \"pvalue(ttest(v,Y))<=0.05\", \"ttest(X,Y)\", \"pvalue(ttest(X,Y))<=0.05\", \"max(energy_dist(v, others))\", \"min(energy_dist(v, others))\",\"mean(energy_dist(v, others))\", \"std(energy_dist(v, others))\", \"25%(energy_dist(v, others))\", \"75%(energy_dist(v, others))\"]\n",
                "columns_to_drop = [col for col in blacklist if col in X_y_group_train.columns]\n",
                "X_y_group_train = X_y_group_train.drop(columns=columns_to_drop)\n",
                "\n",
                "display(X_y_group_train)\n",
                "\n",
                "print(\"Extracting X_train, y_train, and group\")\n",
                "X_train = X_y_group_train.drop([\"variable\", \"dataset\", \"label\", \"y\"], axis=\"columns\")\n",
                "y_train = X_y_group_train[\"y\"]\n",
                "group_train = X_y_group_train[\"dataset\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-09-18T09:40:09.797137Z",
                    "start_time": "2024-09-18T09:39:33.165697Z"
                },
                "metadata": {}
            },
            "outputs": [],
            "source": [
                "model = RandomForestClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=13,\n",
                "    n_jobs=-1,\n",
                "    class_weight=\"balanced\",\n",
                "    random_state=42\n",
                ")\n",
                "display(model)\n",
                "\n",
                "cv = GroupKFold(n_splits=4)\n",
                "results = cross_val_score(\n",
                "    model,\n",
                "    X_train,\n",
                "    y_train,\n",
                "    groups=group_train,\n",
                "    cv=cv,\n",
                "    verbose=True,\n",
                "    scoring=\"balanced_accuracy\"\n",
                ")\n",
                "\n",
                "print(f\"multiclass balanced accuracy: mean={results.mean()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "before:0.489\n",
                "after: 0.488, 差别不大...\n",
                "\n",
                "0.488120992009518"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import cross_val_predict\n",
                "from sklearn.metrics import balanced_accuracy_score\n",
                "\n",
                "\n",
                "cv = GroupKFold(n_splits=4)\n",
                "\n",
                "# 使用cross_val_predict获取每个折叠的预测结果\n",
                "y_pred = cross_val_predict(model, X_train, y_train, groups=group_train, cv=cv, n_jobs=-1)\n",
                "\n",
                "# 计算每个折叠的平衡准确率\n",
                "fold_scores = []\n",
                "for train_index, val_index in cv.split(X_train, y_train, groups=group_train):\n",
                "    y_true_fold = y_train[val_index]\n",
                "    y_pred_fold = y_pred[val_index]\n",
                "    fold_score = balanced_accuracy_score(y_true_fold, y_pred_fold)\n",
                "    fold_scores.append(fold_score)\n",
                "\n",
                "# 找出最佳折叠的索引\n",
                "best_fold_index = fold_scores.index(max(fold_scores))\n",
                "\n",
                "# 在最佳折叠上训练模型\n",
                "train_index, val_index = list(cv.split(X_train, y_train, groups=group_train))[best_fold_index]\n",
                "X_train_best, X_val_best = X_train[train_index], X_train[val_index]\n",
                "y_train_best, y_val_best = y_train[train_index], y_train[val_index]\n",
                "\n",
                "best_model = RandomForestClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=13,\n",
                "    n_jobs=-1,\n",
                "    class_weight=\"balanced\",\n",
                "    random_state=42\n",
                ")\n",
                "best_model.fit(X_train_best, y_train_best)\n",
                "\n",
                "# 保存最佳模型\n",
                "import joblib\n",
                "joblib.dump(best_model, 'best_random_forest_model.joblib')\n",
                "\n",
                "print(f\"Best fold balanced accuracy: {max(fold_scores)}\")\n",
                "print(f\"Average balanced accuracy: {sum(fold_scores) / len(fold_scores)}\")\n",
                "print(\"Best model saved as 'best_random_forest_model.joblib'\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "\n",
                "X = X_train\n",
                "y = y_train\n",
                "# Assuming X and y are your features and target variables\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Fit the model\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# Get predictions\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "\n",
                "# Calculate accuracy for each dimension\n",
                "dimensions = sorted(X_test['dimension'].unique())\n",
                "accuracies = []\n",
                "\n",
                "for dim in dimensions:\n",
                "    mask = X_test['dimension'] == dim\n",
                "    y_true_dim = y_test[mask]\n",
                "    y_pred_dim = y_pred[mask]\n",
                "    acc = accuracy_score(y_true_dim, y_pred_dim)\n",
                "    accuracies.append(acc)\n",
                "\n",
                "# Visualize the results\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(dimensions, accuracies, marker='o')\n",
                "plt.xlabel('Dimension')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.title('Model Accuracy vs Dimension')\n",
                "plt.grid(True)\n",
                "plt.xticks(dimensions)  # Ensure all dimensions are shown on x-axis\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print accuracies\n",
                "print(\"Accuracies for each dimension:\")\n",
                "for dim, acc in zip(dimensions, accuracies):\n",
                "    print(f\"Dimension {dim}: Accuracy = {acc:.4f}\")\n",
                "\n",
                "# Calculate and print overall accuracy\n",
                "overall_accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")\n",
                "\n",
                "# Get feature importances\n",
                "importances = model.feature_importances_\n",
                "feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n",
                "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
                "\n",
                "# Display top 20 most important features\n",
                "print(\"\\nTop 20 most important features:\")\n",
                "print(feature_importances)\n",
                "\n",
                "# Save the model\n",
                "import joblib\n",
                "joblib.dump(model, './resources/random_forest_model_v1.joblib')\n",
                "\n",
                "print(\"\\nModel and feature importances have been saved.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import balanced_accuracy_score\n",
                "from pytorch_tabnet.tab_model import TabNetClassifier\n",
                "import torch\n",
                "import re\n",
                "# 假设 X_train, y_train 已经准备好\n",
                "\n",
                "# 清理特征名称\n",
                "def clean_feature_names(X):\n",
                "    def clean_name(name):\n",
                "        name = re.sub(r'[^\\w\\s-]', '_', name)\n",
                "        if name[0].isdigit():\n",
                "            name = 'f_' + name\n",
                "        return name\n",
                "    \n",
                "    X.columns = [clean_name(col) for col in X.columns]\n",
                "    return X\n",
                "\n",
                "X_train = clean_feature_names(X_train)\n",
                "\n",
                "# 划分训练集和验证集\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
                "\n",
                "# 创建TabNet模型\n",
                "tabnet_model = TabNetClassifier(\n",
                "    n_d=64,  # 决策步骤的维度\n",
                "    n_a=64,  # 注意力步骤的维度\n",
                "    n_steps=5,  # 总共的决策步骤数\n",
                "    gamma=1.5,  # 控制稀疏性的参数\n",
                "    n_independent=2,  # 每个步骤的独立层数\n",
                "    n_shared=2,  # 共享层数\n",
                "    cat_idxs=[],  # 分类特征的索引\n",
                "    cat_dims=[],  # 每个分类特征的唯一值数量\n",
                "    cat_emb_dim=[],  # 每个分类特征的嵌入维度\n",
                "    lambda_sparse=1e-3,  # L1正则化参数\n",
                "    optimizer_fn=torch.optim.Adam,\n",
                "    optimizer_params=dict(lr=2e-2),\n",
                "    scheduler_params=dict(\n",
                "        mode=\"min\", patience=5, min_lr=1e-5, factor=0.5\n",
                "    ),\n",
                "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
                "    mask_type=\"entmax\",  # \"sparsemax\" or \"entmax\"\n",
                "    device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
                ")\n",
                "\n",
                "# 训练模型\n",
                "tabnet_model.fit(\n",
                "    X_train=X_train.values, \n",
                "    y_train=y_train.values,\n",
                "    eval_set=[(X_val.values, y_val.values)],\n",
                "    eval_name=['val'],\n",
                "    eval_metric=['balanced_accuracy'],\n",
                "    max_epochs=100,\n",
                "    patience=10,\n",
                "    batch_size=1024,\n",
                "    virtual_batch_size=256,\n",
                "    num_workers=0,\n",
                "    weights=1,\n",
                "    drop_last=False\n",
                ")\n",
                "\n",
                "# 打印训练结果\n",
                "print(f\"Best epoch: {tabnet_model.best_epoch}\")\n",
                "print(f\"Best validation balanced accuracy: {tabnet_model.best_cost}\")\n",
                "\n",
                "# 在训练集和验证集上评估模型\n",
                "y_train_pred = tabnet_model.predict(X_train.values)\n",
                "y_val_pred = tabnet_model.predict(X_val.values)\n",
                "\n",
                "train_accuracy = balanced_accuracy_score(y_train, y_train_pred)\n",
                "val_accuracy = balanced_accuracy_score(y_val, y_val_pred)\n",
                "\n",
                "print(f\"Train balanced accuracy: {train_accuracy:.4f}\")\n",
                "print(f\"Validation balanced accuracy: {val_accuracy:.4f}\")\n",
                "\n",
                "# 保存模型\n",
                "import joblib\n",
                "# joblib.dump(tabnet_model, './resources/tabnet_model_v0.joblib')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "def clean_feature_names(X):\n",
                "    # 函数用于清理特征名称\n",
                "    def clean_name(name):\n",
                "        # 移除或替换特殊字符\n",
                "        name = re.sub(r'[^\\w\\s-]', '_', name)\n",
                "        # 确保名称不以数字开头\n",
                "        if name[0].isdigit():\n",
                "            name = 'f_' + name\n",
                "        return name\n",
                "\n",
                "    X.columns = [clean_name(col) for col in X.columns]\n",
                "    return X\n",
                "\n",
                "# 清理特征名称\n",
                "X_train = clean_feature_names(X_train)\n",
                "\n",
                "\n",
                "from lightgbm import LGBMClassifier\n",
                "import lightgbm as lgb\n",
                "model = LGBMClassifier(\n",
                "    n_estimators=2000,\n",
                "    learning_rate=0.03,\n",
                "    max_depth=6,\n",
                "    num_leaves=29,\n",
                "    min_child_samples=20,\n",
                "    subsample=0.7,\n",
                "    colsample_bytree=0.7,\n",
                "    reg_alpha=0.1,\n",
                "    reg_lambda=0.1,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    class_weight='balanced',\n",
                "    device='gpu',\n",
                "    gpu_platform_id=1,\n",
                "    gpu_device_id=0,\n",
                ")\n",
                "display(model)\n",
                "\n",
                "from sklearn.model_selection import GroupKFold\n",
                "from sklearn.metrics import balanced_accuracy_score\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "\n",
                "cv = GroupKFold(n_splits=4)\n",
                "train_scores = []\n",
                "val_scores = []\n",
                "\n",
                "for train_idx, val_idx in cv.split(X_train, y_train, groups=group_train):\n",
                "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
                "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
                "    \n",
                "    callbacks = [lgb.log_evaluation(period=1), lgb.early_stopping(stopping_rounds=10)]\n",
                "    \n",
                "    model.fit(X_train_fold, y_train_fold, callbacks=callbacks, eval_set=[(X_val_fold, y_val_fold)])\n",
                "\n",
                "    \n",
                "    y_train_pred = model.predict(X_train_fold)\n",
                "    y_val_pred = model.predict(X_val_fold)\n",
                "    \n",
                "    train_score = balanced_accuracy_score(y_train_fold, y_train_pred)\n",
                "    val_score = balanced_accuracy_score(y_val_fold, y_val_pred)\n",
                "    \n",
                "    train_scores.append(train_score)\n",
                "    val_scores.append(val_score)\n",
                "    \n",
                "    print(f\"Fold train balanced accuracy: {train_score:.4f}\")\n",
                "    print(f\"Fold validation balanced accuracy: {val_score:.4f}\")\n",
                "    print(\"---\")\n",
                "\n",
                "print(f\"Average train balanced accuracy: {np.mean(train_scores):.4f} ± {np.std(train_scores):.4f}\")\n",
                "print(f\"Average validation balanced accuracy: {np.mean(val_scores):.4f} ± {np.std(val_scores):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment what you need!\n",
                "def train(\n",
                "    X_train: typing.Dict[str, pd.DataFrame],\n",
                "    y_train: typing.Dict[str, pd.DataFrame],\n",
                "    # number_of_features: int,\n",
                "    model_directory_path: str,\n",
                "    # id_column_name: str,\n",
                "    # prediction_column_name: str,\n",
                "    # has_gpu: bool,\n",
                ") -> None:\n",
                "    # 1) Create the variable embedding:\n",
                "    return\n",
                "    X_y_group_train_pathname = \"/tmp/X_y_group_train.parquet\"\n",
                "    try:\n",
                "        print(f\"Loading {X_y_group_train_pathname}\")\n",
                "        X_y_group_train = pd.read_parquet(X_y_group_train_pathname)\n",
                "    except FileNotFoundError:\n",
                "        print(\"Creating X_y_group_train\")\n",
                "        names_datasets_train = X_train\n",
                "        names_graphs_train = y_train\n",
                "        X_y_group_train = create_all_columns(\n",
                "            {\n",
                "                pearson_correlation: names_datasets_train,\n",
                "                ttest: names_datasets_train,\n",
                "                mutual_information: names_datasets_train,  # comment this line to greatly reduce computation\n",
                "                label: names_graphs_train,\n",
                "                spearman_correlation: names_datasets_train,\n",
                "                kendall_correlation: names_datasets_train,\n",
                "                distance_correlation: names_datasets_train,\n",
                "                conditional_mutual_information: names_datasets_train,\n",
                "            },\n",
                "            n_jobs=-1,\n",
                "        )\n",
                "        X_y_group_train = remove_outliers(X_y_group_train, ['corr(v,X)', 'corr(v,Y)', 'ttest(v,X)', 'ttest(v,Y)'])\n",
                "        # 2) Massage X_y_group_train to prepare what is needed by the model:\n",
                "        print(\"Adding numeric labels y\") # sklearn wants numeric labels\n",
                "        le = LabelEncoder()\n",
                "        le.classes_ = np.array([\n",
                "            'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
                "            'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
                "        ])\n",
                "        X_y_group_train[\"y\"] = le.transform(X_y_group_train[\"label\"])\n",
                "\n",
                "        # reordering columns:\n",
                "        X_y_group_train = X_y_group_train[[\"dataset\", \"variable\"] + X_y_group_train.columns.drop([\"dataset\", \"variable\", \"label\", \"y\"]).tolist() + [\"label\", \"y\"]]\n",
                "        display(X_y_group_train)\n",
                "\n",
                "        print(\"Saving X_y_group_train\")\n",
                "        os.makedirs(os.path.dirname(X_y_group_train_pathname), exist_ok=True)\n",
                "        X_y_group_train.to_parquet(X_y_group_train_pathname)\n",
                "\n",
                "    print(\"Extracting X_train, y_train, and group\")\n",
                "    X_train = X_y_group_train.drop([\"variable\", \"dataset\", \"label\", \"y\"], axis=\"columns\")\n",
                "    y_train = X_y_group_train[\"y\"]\n",
                "    group_train = X_y_group_train[\"dataset\"]\n",
                "\n",
                "    # Model:\n",
                "    model = RandomForestClassifier(n_estimators=100, max_depth=11, n_jobs=-1, class_weight=\"balanced\")\n",
                "    model.fit(X_train, y_train)\n",
                "\n",
                "    joblib.dump(\n",
                "        model,\n",
                "        os.path.join(model_directory_path, \"model.joblib\")\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_submission(X_y_pred_test):\n",
                "    \"\"\"\n",
                "    From the predicted test set, for each dataset, take predicted\n",
                "    classes of all variables, create the adjacency matrix, then create\n",
                "    the submission in the requested format.\n",
                "    \"\"\"\n",
                "\n",
                "    submission = {}\n",
                "    for name, prediction in tqdm(X_y_pred_test.groupby(\"dataset\"), delay=10):\n",
                "        variables_labels = prediction[[\"variable\", \"label_predicted\"]].set_index(\"variable\")\n",
                "        variables = variables_labels.index.tolist()\n",
                "        variables_all = [\"X\", \"Y\"] + variables\n",
                "\n",
                "        adjacency_matrix = pd.DataFrame(index=variables_all, columns=variables_all)\n",
                "        adjacency_matrix.index.name = \"parent\"\n",
                "        adjacency_matrix[:] = 0\n",
                "        adjacency_matrix.loc[\"X\", \"Y\"] = 1\n",
                "\n",
                "        for v in variables:\n",
                "            l = variables_labels.loc[v].item()\n",
                "            if l == \"Cause of X\":\n",
                "                adjacency_matrix.loc[v, \"X\"] = 1\n",
                "            elif l == \"Cause of Y\":\n",
                "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
                "            elif l == \"Consequence of X\":\n",
                "                adjacency_matrix.loc[\"X\", v] = 1\n",
                "            elif l == \"Consequence of Y\":\n",
                "                adjacency_matrix.loc[\"Y\", v] = 1\n",
                "            elif l == \"Confounder\":\n",
                "                adjacency_matrix.loc[v, \"X\"] = 1\n",
                "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
                "            elif l == \"Collider\":\n",
                "                adjacency_matrix.loc[\"X\", v] = 1\n",
                "                adjacency_matrix.loc[\"Y\", v] = 1\n",
                "            elif l == \"Mediator\":\n",
                "                adjacency_matrix.loc[\"X\", v] = 1\n",
                "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
                "            elif l == \"Confounder\":\n",
                "                pass\n",
                "\n",
                "        for i in variables_all:\n",
                "            for j in variables_all:\n",
                "                submission[f'{name}_{i}_{j}'] = int(adjacency_matrix.loc[i, j])\n",
                "\n",
                "    return submission\n",
                "\n",
                "\n",
                "# Uncomment what you need!\n",
                "def infer(\n",
                "    X_test: typing.Dict[str, pd.DataFrame],\n",
                "    # number_of_features: int,\n",
                "    model_directory_path: str,\n",
                "    id_column_name: str,\n",
                "    prediction_column_name: str,\n",
                "    # has_gpu: bool,\n",
                "    # has_trained: bool,\n",
                ") -> pd.DataFrame:\n",
                "    model = joblib.load(os.path.join(model_directory_path, \"random_forest_model_v0.joblib\"))\n",
                "\n",
                "    names_datasets_test = X_test\n",
                "    X_group_test = create_all_columns(\n",
                "        {\n",
                "                pearson_correlation: names_datasets_train,\n",
                "                ttest: names_datasets_train,\n",
                "                mutual_information: names_datasets_train,  # comment this line to greatly reduce computation\n",
                "                label: names_graphs_train,\n",
                "                spearman_correlation: names_datasets_train,\n",
                "                kendall_correlation: names_datasets_train,\n",
                "                distance_correlation: names_datasets_train,\n",
                "                conditional_mutual_information: names_datasets_train,\n",
                "        },\n",
                "        n_jobs=-1,\n",
                "    )\n",
                "\n",
                "    X_test = X_group_test.drop(columns=[\"dataset\", \"variable\"])\n",
                "    y_predicted = model.predict(X_test)\n",
                "    X_y_pred_test = X_group_test\n",
                "    X_y_pred_test[\"y_predicted\"] = y_predicted\n",
                "\n",
                "    le = LabelEncoder()\n",
                "    le.classes_ = np.array([\n",
                "        'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
                "        'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
                "    ])\n",
                "\n",
                "    X_y_pred_test[\"label_predicted\"] = le.inverse_transform(y_predicted)\n",
                "\n",
                "    submission = create_submission(X_y_pred_test)\n",
                "\n",
                "    return pd.DataFrame(\n",
                "        submission.items(),\n",
                "        columns=[\n",
                "            id_column_name,\n",
                "            prediction_column_name\n",
                "        ]\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "crunch.test(\n",
                "    no_determinism_check=True\n",
                ")\n",
                "\n",
                "print(\"Download this notebook and submit it to the platform: https://hub.crunchdao.com/competitions/causality-discovery/submit/via/notebook\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "casual",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
