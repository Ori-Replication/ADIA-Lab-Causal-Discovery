{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import sys\n",
    "import typing\n",
    "import random\n",
    "import joblib\n",
    "import dcor\n",
    "import math\n",
    "from math import ceil, log, sqrt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "from copy import deepcopy\n",
    "import scipy.stats as stats\n",
    "import scipy.special as special\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LassoCV\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from collections import Counter, defaultdict\n",
    "from econml.dml import CausalForestDML, LinearDML\n",
    "from causallearn.graph.GraphNode import GraphNode\n",
    "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.search.ScoreBased.ExactSearch import bic_exact_search\n",
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "from causallearn.search.PermutationBased.GRaSP import grasp\n",
    "import semopy\n",
    "from semopy import Model\n",
    "from semopy.inspector import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_nodes_representation(graph, nodelist):\n",
    "    \"\"\"\n",
    "    Create an alternative representation of a graph which is hashable\n",
    "    and equivalent graphs have the same hash.\n",
    "\n",
    "    Python cannot PROPERLY use nx.Graph/DiGraph as key for\n",
    "    dictionaries, because two equivalent graphs with just different\n",
    "    order of the nodes would result in different keys. This is\n",
    "    undesirable here.\n",
    "\n",
    "    So here we transform the graph into an equivalent form that is\n",
    "    based on a specific nodelist and that is hashable. In this way,\n",
    "    two equivalent graphs, once transformed, will result in identical\n",
    "    keys.\n",
    "\n",
    "    So we use the following trick: extract the adjacency matrix\n",
    "    (with nodes in a fixed order) and then make a hashable thing out\n",
    "    of it, through tuple(array.flatten()):\n",
    "    \"\"\"\n",
    "\n",
    "    # This get the adjacency matrix with nodes in a given order, as\n",
    "    # numpy array (which is not hashable):\n",
    "    adjacency_matrix = nx.adjacency_matrix(graph, nodelist=nodelist).todense()\n",
    "\n",
    "    # This transforms the numpy array into a hashable object:\n",
    "    hashable = tuple(adjacency_matrix.flatten())\n",
    "\n",
    "    return hashable\n",
    "\n",
    "def create_graph_label():\n",
    "    \"\"\"\n",
    "    Create a dictionary from graphs to labels, in two formats.\n",
    "    \"\"\"\n",
    "    graph_label = {\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]):             \"Cause of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]):             \"Cause of Y\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]):             \"Consequence of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]):             \"Consequence of Y\",\n",
    "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}):                \"Independent\",\n",
    "    }\n",
    "\n",
    "    nodelist = [\"v\", \"X\", \"Y\"]\n",
    "\n",
    "    # This is an equivalent alternative to graph_label but in a form\n",
    "    # for which two equivalent graphs have the same key:\n",
    "    adjacency_label = {\n",
    "        graph_nodes_representation(graph, nodelist): label\n",
    "        for graph, label in graph_label.items()\n",
    "    }\n",
    "\n",
    "    return graph_label, adjacency_label\n",
    "\n",
    "def get_labels(adjacency_matrix, adjacency_label):\n",
    "    \"\"\"\n",
    "    Transform an adjacency_matrix (as pd.DataFrame) into a dictionary of variable:label\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
    "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]  # this is not hashable\n",
    "        key = tuple(submatrix.values.flatten())  # this is hashable and a compatible with adjacency_label\n",
    "    \n",
    "        result[variable] = adjacency_label[key]\n",
    "\n",
    "    return result\n",
    "\n",
    "graph_label, adjacency_label = create_graph_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mapping(df, mapping):\n",
    "    df_new = df.copy()\n",
    "    # 创建临时映射以避免冲突\n",
    "    temp_mapping = {k: f'_temp_{k}' for k in mapping.keys()}\n",
    "    df_new.rename(columns=temp_mapping, inplace=True)\n",
    "    if df_new.shape[0] == df_new.shape[1]:  # 如果是方阵，如标签矩阵\n",
    "        df_new.rename(index=temp_mapping, inplace=True)\n",
    "    # 应用最终映射\n",
    "    final_mapping = {f'_temp_{k}': v for k, v in mapping.items()}\n",
    "    df_new.rename(columns=final_mapping, inplace=True)\n",
    "    if df_new.shape[0] == df_new.shape[1]:\n",
    "        df_new.rename(index=final_mapping, inplace=True)\n",
    "    return df_new\n",
    "\n",
    "def check_duplicate_columns(df):\n",
    "    \"\"\"检查是否存在重复的列名\"\"\"\n",
    "    return df.columns.duplicated().any()\n",
    "\n",
    "def augment_data(X_train, y_train):\n",
    "    new_X_train = X_train.copy()\n",
    "    new_y_train = y_train.copy()\n",
    "    for sample_id in X_train.keys():\n",
    "        X = X_train[sample_id]\n",
    "        y = y_train[sample_id]\n",
    "        variables = list(X.columns)\n",
    "        dim = len(variables)\n",
    "        # 提取因果关系对\n",
    "        edges = []\n",
    "        for u in y.index:\n",
    "            for v in y.columns:\n",
    "                if y.loc[u, v] == 1:\n",
    "                    edges.append((u, v))\n",
    "        # 排除涉及 X 和 Y 的边\n",
    "        edges_no_XY = [(u, v) for (u, v) in edges if u not in ['X', 'Y'] and v not in ['X', 'Y']]\n",
    "        if dim >= 4:\n",
    "            edges_to_use = edges_no_XY\n",
    "            attempts = 0\n",
    "            success = False\n",
    "            while attempts < 3 and not success:\n",
    "                if not edges_to_use:\n",
    "                    break  # 没有合适的边，跳出循环\n",
    "                u, v = random.choice(edges_to_use)\n",
    "                mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
    "                # 应用映射到特征矩阵和标签矩阵\n",
    "                X_new = apply_mapping(X, mapping)\n",
    "                y_new = apply_mapping(y, mapping)\n",
    "                # 检查特征矩阵是否有重复列\n",
    "                if check_duplicate_columns(X_new):\n",
    "                    attempts += 1\n",
    "                    continue  # 重试\n",
    "                else:\n",
    "                    # 没有重复列，存储新的数据\n",
    "                    new_sample_id = '0' + sample_id\n",
    "                    new_X_train[new_sample_id] = X_new\n",
    "                    new_y_train[new_sample_id] = y_new\n",
    "                    success = True\n",
    "            if not success:\n",
    "                # 没有找到合适的映射，复制原始数据\n",
    "                new_sample_id = '0' + sample_id\n",
    "                new_X_train[new_sample_id] = X.copy()\n",
    "                new_y_train[new_sample_id] = y.copy()\n",
    "        else:\n",
    "            # 对于维度较低的数据，允许涉及 X 和 Y 的边\n",
    "            edges_to_use = edges\n",
    "            if not edges_to_use:\n",
    "                # 没有边，复制原始数据\n",
    "                new_sample_id = '0' + sample_id\n",
    "                new_X_train[new_sample_id] = X.copy()\n",
    "                new_y_train[new_sample_id] = y.copy()\n",
    "                continue\n",
    "            u, v = random.choice(edges_to_use)\n",
    "            mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
    "            # 应用映射到特征矩阵和标签矩阵\n",
    "            X_new = apply_mapping(X, mapping)\n",
    "            y_new = apply_mapping(y, mapping)\n",
    "            # 检查特征矩阵是否有重复列\n",
    "            if check_duplicate_columns(X_new):\n",
    "                # 如果有重复列，复制原始数据\n",
    "                new_sample_id = '0' + sample_id\n",
    "                new_X_train[new_sample_id] = X.copy()\n",
    "                new_y_train[new_sample_id] = y.copy()\n",
    "            else:\n",
    "                # 没有重复列，存储新的数据\n",
    "                new_sample_id = '0' + sample_id\n",
    "                new_X_train[new_sample_id] = X_new\n",
    "                new_y_train[new_sample_id] = y_new\n",
    "    return new_X_train, new_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('./data/X_train.pickle')\n",
    "y_train = pd.read_pickle('./data/y_train.pickle')\n",
    "## 需要修改为增强后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"离散化统计指标的工具函数\"\"\"\n",
    "def discretize_sequence(x, ffactor=10):\n",
    "    \"\"\"\n",
    "    将连续序列离散化。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 输入的连续变量，范围在 [-1, 1] 之间。\n",
    "    ffactor (int): 离散化因子，用于缩放和离散化。\n",
    "\n",
    "    返回:\n",
    "    np.ndarray: 离散化后的序列（整数类型）。\n",
    "    \"\"\"\n",
    "    # 确保输入在 [-1, 1] 范围内\n",
    "    x = np.clip(x, -1, 1)\n",
    "    # 缩放并四舍五入\n",
    "    x = np.round(x * ffactor).astype(int)\n",
    "    return x\n",
    "\n",
    "def compute_discrete_probability(x):\n",
    "    \"\"\"\n",
    "    计算离散概率分布。\n",
    "\n",
    "    参数:\n",
    "    x (iterable): 输入的离散化后的序列，可以是单变量或联合变量。\n",
    "\n",
    "    返回:\n",
    "    Counter: 元素及其计数。\n",
    "    \"\"\"\n",
    "    return Counter(x)\n",
    "\n",
    "def discrete_entropy(x, bias_factor=0.7):\n",
    "    \"\"\"\n",
    "    计算离散熵。\n",
    "\n",
    "    参数:\n",
    "    x (iterable): 输入的离散化后的序列，可以是单变量或联合变量。\n",
    "    bias_factor (float): 偏差因子，用于修正有限样本的熵估计。\n",
    "\n",
    "    返回:\n",
    "    float: 计算得到的熵值。\n",
    "    \"\"\"\n",
    "    c = compute_discrete_probability(x)\n",
    "    pk = np.array(list(c.values()), dtype=float)\n",
    "    pk_sum = pk.sum()\n",
    "    if pk_sum == 0:\n",
    "        return 0.0\n",
    "    pk /= pk_sum\n",
    "    # 避免 log(0) 问题，添加一个很小的常数\n",
    "    vec = pk * np.log(pk + 1e-12)\n",
    "    S = -np.sum(vec)\n",
    "    # 添加偏差项\n",
    "    bias = bias_factor * (len(pk) - 1) / (2.0 * len(x))\n",
    "    return S + bias\n",
    "\n",
    "def discrete_joint_entropy(x, y):\n",
    "    \"\"\"\n",
    "    计算两个离散序列的联合熵 H(X, Y)。\n",
    "\n",
    "    参数:\n",
    "    x, y (array-like): 输入的离散化后的序列。\n",
    "\n",
    "    返回:\n",
    "    float: 联合熵 H(X, Y)。\n",
    "    \"\"\"\n",
    "    joint = list(zip(x, y))  # 将 x 和 y 配对\n",
    "    return discrete_entropy(joint)\n",
    "\n",
    "def normalized_error_probability(x, y):\n",
    "    \"\"\"\n",
    "    计算归一化的错误概率。\n",
    "\n",
    "    该函数通过构建联合概率矩阵，计算分类错误的概率，并将其归一化。\n",
    "\n",
    "    参数:\n",
    "    x, y (array-like): 输入的离散化后的序列。\n",
    "\n",
    "    返回:\n",
    "    float: 归一化的错误概率。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    cy = Counter(y)\n",
    "\n",
    "    sorted_cx = sorted(cx.keys())\n",
    "    sorted_cy = sorted(cy.keys())\n",
    "\n",
    "    # 统计联合频数\n",
    "    pxy = defaultdict(int)\n",
    "    for a, b in zip(x, y):\n",
    "        pxy[(a, b)] += 1\n",
    "\n",
    "    total = sum(pxy.values())\n",
    "    if total == 0:\n",
    "        return 0.0  # 或者根据需求返回其他值\n",
    "\n",
    "    # 构建联合概率矩阵\n",
    "    pxy_matrix = np.array([\n",
    "        [pxy.get((a, b), 0) for b in sorted_cy]\n",
    "        for a in sorted_cx\n",
    "    ], dtype=float)\n",
    "\n",
    "    # 归一化为概率\n",
    "    pxy_matrix /= total\n",
    "\n",
    "    # 计算每行的最大概率\n",
    "    max_per_row = pxy_matrix.max(axis=1)\n",
    "    perr = 1 - np.sum(max_per_row)\n",
    "\n",
    "    # 计算每列的概率和的最大值\n",
    "    sum_per_column = pxy_matrix.sum(axis=0)\n",
    "    max_perr = 1 - np.max(sum_per_column)\n",
    "\n",
    "    # 归一化错误概率\n",
    "    pnorm = perr / max_perr if max_perr > 0 else perr\n",
    "    return pnorm\n",
    "\n",
    "def discrete_divergence(cx, cy):\n",
    "    \"\"\"\n",
    "    计算两个离散分布之间的KL散度（Kullback-Leibler Divergence）。\n",
    "\n",
    "    KL散度衡量了分布 cx 相对于分布 cy 的差异，是信息论中的一个重要概念。\n",
    "\n",
    "    参数:\n",
    "    cx (Counter): 第一个离散分布的元素计数。\n",
    "    cy (Counter): 第二个离散分布的元素计数。\n",
    "\n",
    "    返回:\n",
    "    float: KL散度 D_KL(cx || cy)。\n",
    "    \"\"\"\n",
    "    # 创建 cy 的副本，避免修改原始对象\n",
    "    cy = cy.copy()\n",
    "\n",
    "    # 为了避免 cy 中某些元素的概率为零，将它们的计数设为 1\n",
    "    for a in cx:\n",
    "        if cy[a] == 0:\n",
    "            cy[a] = 1\n",
    "\n",
    "    # 计算概率\n",
    "    nx = float(sum(cx.values()))\n",
    "    ny = float(sum(cy.values()))\n",
    "\n",
    "    kl_div = 0.0\n",
    "    for a, v in cx.items():\n",
    "        px = v / nx\n",
    "        py = cy[a] / ny\n",
    "        kl_div += px * np.log(px / py)\n",
    "    return kl_div\n",
    "\n",
    "def discrete_conditional_entropy(x, y):\n",
    "    \"\"\"\n",
    "    计算两个离散序列的条件熵 H(X|Y)。\n",
    "\n",
    "    条件熵衡量了在已知 Y 的情况下，X 的不确定性。\n",
    "\n",
    "    参数:\n",
    "    x, y (array-like): 输入的离散化后的序列。\n",
    "\n",
    "    返回:\n",
    "    float: 条件熵 H(X|Y)。\n",
    "    \"\"\"\n",
    "    joint_entropy = discrete_joint_entropy(x, y)\n",
    "    entropy_y = discrete_entropy(y)\n",
    "    return joint_entropy - entropy_y\n",
    "\n",
    "def adjusted_mutual_information_score(x, y):\n",
    "    \"\"\"\n",
    "    计算两个离散序列的调整互信息（Adjusted Mutual Information, AMI）。\n",
    "\n",
    "    AMI 是互信息的一种调整版本，考虑了随机期望的互信息，通常用于聚类评估。\n",
    "\n",
    "    参数:\n",
    "    x, y (array-like): 输入的离散化后的序列。\n",
    "\n",
    "    返回:\n",
    "    float: 调整后的互信息。\n",
    "    \"\"\"\n",
    "    return adjusted_mutual_info_score(x, y)\n",
    "\n",
    "def discrete_mutual_information(x, y):\n",
    "    \"\"\"\n",
    "    计算两个离散序列的互信息 I(X; Y)。\n",
    "\n",
    "    互信息衡量了两个变量之间共享的信息量，是信息论中的一个基本概念。\n",
    "\n",
    "    参数:\n",
    "    x, y (array-like): 输入的离散化后的序列。\n",
    "\n",
    "    返回:\n",
    "    float: 互信息 I(X; Y)。\n",
    "    \"\"\"\n",
    "    entropy_x = discrete_entropy(x)\n",
    "    entropy_y = discrete_entropy(y)\n",
    "    joint_entropy = discrete_joint_entropy(x, y)\n",
    "    mutual_info = entropy_x + entropy_y - joint_entropy\n",
    "    # 避免由于数值误差导致的负值\n",
    "    mutual_info = max(mutual_info, 0)\n",
    "    return mutual_info\n",
    "\n",
    "# -------------------\n",
    "# Helper Functions\n",
    "# -------------------\n",
    "\n",
    "def normalize_discrete(x):\n",
    "    \"\"\"\n",
    "    对离散化后的序列进行标准化处理。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散化后的x序列。\n",
    "\n",
    "    返回:\n",
    "    np.ndarray: 标准化后的x序列。\n",
    "    \"\"\"\n",
    "    if len(set(x)) < 2:\n",
    "        return np.zeros_like(x, dtype=float)\n",
    "    x_mean = np.mean(x)\n",
    "    x_std = np.std(x)\n",
    "    if x_std > 0:\n",
    "        return (x - x_mean) / x_std\n",
    "    else:\n",
    "        return x - x_mean\n",
    "\n",
    "def to_numerical(x_discrete, y_continuous):\n",
    "    \"\"\"\n",
    "    将类别型的离散x转换为数值型，通过将每个唯一的x值替换为对应y的平均值。\n",
    "\n",
    "    参数:\n",
    "    x_discrete (array-like): 离散化后的x数组。\n",
    "    y_continuous (array-like): 与x对应的连续y数组。\n",
    "\n",
    "    返回:\n",
    "    np.ndarray: 数值型的x数组，每个x值被替换为对应的y平均值。\n",
    "    \"\"\"\n",
    "    dx = defaultdict(lambda: [0.0, 0])\n",
    "    for a, b in zip(x_discrete, y_continuous):\n",
    "        dx[a][0] += b\n",
    "        dx[a][1] += 1\n",
    "    for a in dx:\n",
    "        dx[a][0] /= dx[a][1] if dx[a][1] > 0 else 1e-12\n",
    "    x_numerical = np.array([dx[a][0] for a in x_discrete], dtype=float)\n",
    "    x_numerical = (x_numerical - np.mean(x_numerical)) / np.std(x_numerical) if np.std(x_numerical) > 0 else x_numerical\n",
    "    return x_numerical\n",
    "\n",
    "def count_unique(x):\n",
    "    \"\"\"\n",
    "    计算数组中唯一元素的数量。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 输入数组。\n",
    "\n",
    "    返回:\n",
    "    int: 唯一元素的数量。\n",
    "    \"\"\"\n",
    "    return len(set(x))\n",
    "\n",
    "# -------------------\n",
    "# Feature Engineering Functions\n",
    "# -------------------\n",
    "\n",
    "def normalized_entropy_baseline(x):\n",
    "    \"\"\"\n",
    "    计算给定归一化x的标准化熵基线。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "\n",
    "    返回:\n",
    "    float: 标准化熵基线值。\n",
    "    \"\"\"\n",
    "    if len(set(x)) < 2:\n",
    "        return 0.0\n",
    "    xs = np.sort(x)\n",
    "    delta = xs[1:] - xs[:-1]\n",
    "    delta = delta[delta != 0]\n",
    "    if len(delta) == 0:\n",
    "        return 0.0\n",
    "    hx = np.mean(np.log(delta))\n",
    "    hx += special.psi(len(delta))\n",
    "    hx -= special.psi(1)\n",
    "    return hx\n",
    "\n",
    "def normalized_entropy(x, m=2):\n",
    "    \"\"\"\n",
    "    计算标准化熵。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    m (int): delta计算的参数。\n",
    "\n",
    "    返回:\n",
    "    float: 标准化熵值。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    if len(cx) < 2:\n",
    "        return 0.0\n",
    "    xk = np.array(list(cx.keys()), dtype=float)\n",
    "    xk.sort()\n",
    "    if len(xk) < 2:\n",
    "        return 0.0\n",
    "    delta = (xk[1:] - xk[:-1]) / m\n",
    "    counter = np.array([cx[i] for i in xk], dtype=float)\n",
    "    hx = np.sum(counter[1:] * np.log(delta / counter[1:])) / len(x)\n",
    "    hx += (special.psi(len(delta)) - np.log(len(delta)))\n",
    "    hx += np.log(len(x))\n",
    "    hx -= (special.psi(m) - np.log(m))\n",
    "    return hx\n",
    "\n",
    "def igci(x, y):\n",
    "    \"\"\"\n",
    "    计算IGCI（信息几何因果推断）度量。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "\n",
    "    返回:\n",
    "    float: IGCI度量值。\n",
    "    \"\"\"\n",
    "    # 检查是否有足够的唯一值\n",
    "    if len(set(x)) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # 判断是否有重复的x值\n",
    "    if len(x) != len(set(x)):\n",
    "        dx = defaultdict(lambda: [0.0, 0])\n",
    "        for a, b in zip(x, y):\n",
    "            dx[a][0] += b\n",
    "            dx[a][1] += 1\n",
    "        for a in dx:\n",
    "            dx[a][0] /= dx[a][1] if dx[a][1] > 0 else 1e-12\n",
    "        # 构建联合序列\n",
    "        xy = np.array([[a, dx[a][0]] for a in dx.keys()], dtype=float)\n",
    "        # 获取每个x的计数\n",
    "        counter = np.array([dx[a][1] for a in xy[:, 0]], dtype=float)\n",
    "    else:\n",
    "        # 如果x没有重复，直接排序\n",
    "        xy = np.array(sorted(zip(x, y)), dtype=float)\n",
    "        counter = np.ones(len(x))\n",
    "    \n",
    "    # 计算相邻差值\n",
    "    delta = xy[1:] - xy[:-1]\n",
    "    # 选择y差值不为0的样本\n",
    "    selec = delta[:, 1] != 0\n",
    "    delta = delta[selec]\n",
    "    counter = np.minimum(counter[1:], counter[:-1])[selec]\n",
    "    \n",
    "    if len(delta) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 添加一个极小值epsilon，避免log(0)\n",
    "    epsilon = 1e-12\n",
    "    ratio = (delta[:, 0] + epsilon) / np.abs(delta[:, 1])\n",
    "    ratio = np.where(ratio > 0, ratio, epsilon)\n",
    "    \n",
    "    # 计算 hxy，避免返回 NaN\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        hxy = np.sum(counter * np.log(ratio)) / len(x)\n",
    "    \n",
    "    # 检查 hxy 是否为有效数值\n",
    "    if np.isnan(hxy):\n",
    "        return 0.0\n",
    "    \n",
    "    return hxy\n",
    "\n",
    "def uniform_divergence(x, m=2):\n",
    "    \"\"\"\n",
    "    计算统一散度。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    m (int): delta计算的参数。\n",
    "\n",
    "    返回:\n",
    "    float: 统一散度值。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    xk = np.array(list(cx.keys()), dtype=float)\n",
    "    xk.sort()\n",
    "    delta = np.zeros(len(xk))\n",
    "    if len(xk) > 1:\n",
    "        delta[0] = xk[1] - xk[0]\n",
    "        if len(xk) > m:\n",
    "            delta[1:-1] = (xk[m:] - xk[:-m]) / m\n",
    "        else:\n",
    "            delta[1:-1] = (xk[-1] - xk[0]) / (len(xk) - 1)\n",
    "        delta[-1] = xk[-1] - xk[-2]\n",
    "    else:\n",
    "        delta = np.array([np.sqrt(12)], dtype=float)  # 假设均匀分布在[-1,1]\n",
    "\n",
    "    counter = np.array([cx[i] for i in xk], dtype=float)\n",
    "    delta_sum = np.sum(delta)\n",
    "    if delta_sum > 0:\n",
    "        delta = delta / delta_sum\n",
    "    else:\n",
    "        delta = delta\n",
    "    if len(xk) > 1:\n",
    "        hx = np.sum(counter * np.log(counter / delta)) / len(x)\n",
    "    else:\n",
    "        hx = 0.0\n",
    "    hx -= np.log(len(x))\n",
    "    hx += (special.psi(m) - np.log(m))\n",
    "    return hx\n",
    "\n",
    "def normalized_skewness(x):\n",
    "    \"\"\"\n",
    "    计算x的标准化偏度。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "\n",
    "    返回:\n",
    "    float: 标准化偏度值。\n",
    "    \"\"\"\n",
    "    return stats.skew(x)\n",
    "\n",
    "def normalized_kurtosis(x):\n",
    "    \"\"\"\n",
    "    计算x的标准化峰度。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "\n",
    "    返回:\n",
    "    float: 标准化峰度值。\n",
    "    \"\"\"\n",
    "    return stats.kurtosis(x)\n",
    "\n",
    "def normalized_moment(x, y, n, m):\n",
    "    \"\"\"\n",
    "    计算x和y的标准化联合矩。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "    n (int): x的幂次。\n",
    "    m (int): y的幂次。\n",
    "\n",
    "    返回:\n",
    "    float: 标准化的联合矩值。\n",
    "    \"\"\"\n",
    "    return np.mean((x ** n) * (y ** m))\n",
    "\n",
    "def moment21(x, y):\n",
    "    \"\"\"\n",
    "    计算标准化联合矩 I(X^2 * Y)。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "\n",
    "    返回:\n",
    "    float: 联合矩 I(X^2 * Y)。\n",
    "    \"\"\"\n",
    "    return normalized_moment(x, y, 2, 1)\n",
    "\n",
    "def moment22(x, y):\n",
    "    \"\"\"\n",
    "    计算标准化联合矩 I(X^2 * Y^2)。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "\n",
    "    返回:\n",
    "    float: 联合矩 I(X^2 * Y^2)。\n",
    "    \"\"\"\n",
    "    return normalized_moment(x, y, 2, 2)\n",
    "\n",
    "def moment31(x, y):\n",
    "    \"\"\"\n",
    "    计算标准化联合矩 I(X^3 * Y)。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "\n",
    "    返回:\n",
    "    float: 联合矩 I(X^3 * Y)。\n",
    "    \"\"\"\n",
    "    return normalized_moment(x, y, 3, 1)\n",
    "\n",
    "def fit_pairwise(x, y):\n",
    "    \"\"\"\n",
    "    拟合多项式到x和y，并基于系数计算一个复杂的度量值。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列（数值型）。\n",
    "    y (array-like): 离散且归一化的y序列（数值型）。\n",
    "\n",
    "    返回:\n",
    "    float: 拟合度量值。\n",
    "    \"\"\"\n",
    "    if count_unique(x) <= 2 or count_unique(y) <= 2:\n",
    "        return 0.0\n",
    "    x_std = x if np.std(x) == 1 else (x - np.mean(x)) / np.std(x) if np.std(x) > 0 else x\n",
    "    y_std = y if np.std(y) == 1 else (y - np.mean(y)) / np.std(y) if np.std(y) > 0 else y\n",
    "    try:\n",
    "        xy1 = np.polyfit(x_std, y_std, 1)\n",
    "        xy2 = np.polyfit(x_std, y_std, 2)\n",
    "        return abs(2 * xy2[0]) + abs(xy2[1] - xy1[0])\n",
    "    except np.RankWarning:\n",
    "        return 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def fit_error(x, y, m=2):\n",
    "    \"\"\"\n",
    "    计算x和y之间的拟合误差。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "    m (int): 拟合时使用的多项式的阶数。\n",
    "\n",
    "    返回:\n",
    "    float: 拟合误差。\n",
    "    \"\"\"\n",
    "    if count_unique(x) <= m or count_unique(y) <= m:\n",
    "        poly_degree = min(count_unique(x), count_unique(y)) - 1\n",
    "    else:\n",
    "        poly_degree = m\n",
    "\n",
    "    if poly_degree < 1:\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        poly = np.polyfit(x, y, poly_degree)\n",
    "        y_pred = np.polyval(poly, x)\n",
    "        return np.std(y - y_pred)\n",
    "    except np.RankWarning:\n",
    "        return 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def fit_noise_entropy(x, y, minc=10):\n",
    "    \"\"\"\n",
    "    计算拟合噪声熵。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "    minc (int): 计算熵的最小计数阈值。\n",
    "\n",
    "    返回:\n",
    "    float: 拟合噪声熵。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    entyx = []\n",
    "    for a in cx:\n",
    "        if cx[a] > minc:\n",
    "            y_subset = y[x == a]\n",
    "            entyx.append(discrete_entropy(y_subset))\n",
    "    if len(entyx) == 0:\n",
    "        return 0.0\n",
    "    n = count_unique(y)\n",
    "    return np.std(entyx) / np.log(n) if n > 0 else 0.0\n",
    "\n",
    "def fit_noise_skewness(x, y, minc=8):\n",
    "    \"\"\"\n",
    "    计算拟合噪声偏度的标准差。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "    minc (int): 计算偏度的最小计数阈值。\n",
    "\n",
    "    返回:\n",
    "    float: 拟合噪声偏度的标准差。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    skewyx = []\n",
    "    for a in cx:\n",
    "        if cx[a] >= minc:\n",
    "            y_subset = y[x == a]\n",
    "            skewyx.append(normalized_skewness(y_subset))\n",
    "    if len(skewyx) == 0:\n",
    "        return 0.0\n",
    "    return np.std(skewyx)\n",
    "\n",
    "def fit_noise_kurtosis(x, y, minc=8):\n",
    "    \"\"\"\n",
    "    计算拟合噪声峰度的标准差。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "    minc (int): 计算峰度的最小计数阈值。\n",
    "\n",
    "    返回:\n",
    "    float: 拟合噪声峰度的标准差。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    kurtyx = []\n",
    "    for a in cx:\n",
    "        if cx[a] >= minc:\n",
    "            y_subset = y[x == a]\n",
    "            kurtyx.append(normalized_kurtosis(y_subset))\n",
    "    if len(kurtyx) == 0:\n",
    "        return 0.0\n",
    "    return np.std(kurtyx)\n",
    "\n",
    "def conditional_distribution_similarity(x, y, minc=12):\n",
    "    \"\"\"\n",
    "    计算条件分布相似性。\n",
    "\n",
    "    参数:\n",
    "    x (array-like): 离散且归一化的x序列。\n",
    "    y (array-like): 离散且归一化的y序列。\n",
    "    minc (int): 计算条件分布的最小计数阈值。\n",
    "\n",
    "    返回:\n",
    "    float: 条件分布相似性度量。\n",
    "    \"\"\"\n",
    "    cx = Counter(x)\n",
    "    cy = Counter(y)\n",
    "    yrange = sorted(cy.keys())\n",
    "    ny = len(yrange)\n",
    "\n",
    "    py = np.array([cy[i] for i in yrange], dtype=float)\n",
    "    py = py / py.sum() if py.sum() > 0 else py\n",
    "\n",
    "    pyx = []\n",
    "    for a in cx:\n",
    "        if cx[a] > minc:\n",
    "            yx = y[x == a]\n",
    "            cyx = Counter(yx)\n",
    "            pyxa = np.array([cyx.get(i, 0.0) for i in yrange], dtype=float)\n",
    "            if pyxa.sum() == 0:\n",
    "                continue\n",
    "            pyxa = pyxa / pyxa.sum()\n",
    "            pyx.append(py * pyxa)  # 修正这里，将 pyx * pyxa 改为 py * pyxa\n",
    "\n",
    "    if len(pyx) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    pyx = np.array(pyx)\n",
    "    pyx = pyx - pyx.mean(axis=0)\n",
    "    return np.std(pyx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cloud的工具函数\"\"\"\n",
    "def log2(n):\n",
    "    return log(n or 1, 2)\n",
    "\n",
    "def C_MN(n: int, K: int):\n",
    "    \"\"\"Computes the normalizing term of NML distribution recursively. O(n+K)\n",
    "\n",
    "    For more detail, please refer to eq (19) (Theorem1) in\n",
    "    \"NML Computation Algorithms for Tree-Structured Multinomial Bayesian Networks\"\n",
    "    https://pubmed.ncbi.nlm.nih.gov/18382603/\n",
    "\n",
    "    and algorithm 2 in\n",
    "    \"Computing the Multinomial Stochastic Complexity in Sub-Linear Time\"\n",
    "    http://pgm08.cs.aau.dk/Papers/31_Paper.pdf\n",
    "\n",
    "\n",
    "    Args\n",
    "    ----------\n",
    "        n (int): sample size of a dataset\n",
    "        K (int): K-value multinomal distribution\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        float: (Approximated) Multinomal Normalizing Sum\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    total = 1\n",
    "    b = 1\n",
    "    d = 10 # 10 digit precision\n",
    "\n",
    "    #bound = int(ceil(2 + sqrt( -2 * n * np.log(2 * 10**(-d) - 100 ** (-d)))))\n",
    "    bound = int(ceil(2 + sqrt(2 * n * d * log(10))))  # using equation (38)\n",
    "\n",
    "    for k in range(1, bound + 1):\n",
    "        b = (n - k + 1) / n * b\n",
    "        total += b\n",
    "\n",
    "    log_old_sum = log2(1.0)\n",
    "    log_total = log2(total)\n",
    "    log_n = log2(n)\n",
    "    for j in range(3, K + 1):\n",
    "        log_x = log_n + log_old_sum - log_total - log2(j - 2)\n",
    "        x = 2 ** log_x\n",
    "        log_one_plus_x = log2(1 + x)\n",
    "        log_new_sum = log_total + log_one_plus_x\n",
    "        log_old_sum = log_total\n",
    "        log_total = log_new_sum\n",
    "\n",
    "    if K == 1:\n",
    "        log_total = log2(1.0)\n",
    "\n",
    "    return log_total\n",
    "\n",
    "def parametric_complexity(X, Y, model_type: str, X_ndistinct_vals=None, Y_ndistinct_vals=None):\n",
    "    \"\"\"Computes the Parametric Complexity of Multinomals.\n",
    "\n",
    "    Args\n",
    "    ----------\n",
    "        model_type (str): [\"to\", \"gets\", \"indep\", \"confounder\"]\n",
    "        X (sequence): sequence of discrete outcomes\n",
    "        Y (sequence): sequence of discrete outcomes\n",
    "        X_ndistinct_vals (int): number of distinct values of the multinomial r.v X.\n",
    "        Y_ndistinct_vals (int): number of distinct values of the multinomial r.v Y.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        float: Parametric Complexity of Multinomals\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(X)==len(Y)\n",
    "    n = len(X)\n",
    "    X_ndistinct_vals = X_ndistinct_vals or len(set(X))\n",
    "    Y_ndistinct_vals = Y_ndistinct_vals or len(set(Y))\n",
    "\n",
    "\n",
    "    if model_type == \"confounder\":\n",
    "        return  C_MN(n=n, K=X_ndistinct_vals * Y_ndistinct_vals)\n",
    "\n",
    "    else:\n",
    "        return  C_MN(n=n, K=X_ndistinct_vals) + C_MN(n=n, K=Y_ndistinct_vals)\n",
    "\n",
    "# ref: https://github.molgen.mpg.de/EDA/cisc/blob/master/formatter.py\n",
    "def stratify(X, Y):\n",
    "    \"\"\"Stratifies Y based on unique values of X.\n",
    "    Args:\n",
    "        X (sequence): sequence of discrete outcomes\n",
    "        Y (sequence): sequence of discrete outcomes\n",
    "    Returns:\n",
    "        (dict): list of Y-values for a X-value\n",
    "    \"\"\"\n",
    "    Y_grps = defaultdict(list)\n",
    "    for i, x in enumerate(X):\n",
    "        Y_grps[x].append(Y[i])\n",
    "    return Y_grps\n",
    "\n",
    "def map_to_majority(X, Y):\n",
    "    \"\"\"Creates a function that maps x to most frequent y.\n",
    "    Args:\n",
    "        X (sequence): sequence of discrete outcomes\n",
    "        Y (sequence): sequence of discrete outcomes\n",
    "    Returns:\n",
    "        (dict): map from Y-values to frequently co-occuring X-values\n",
    "    \"\"\"\n",
    "    f = dict()\n",
    "    Y_grps = stratify(X, Y)\n",
    "    for x, Ys in Y_grps.items():\n",
    "        frequent_y, _ = Counter(Ys).most_common(1)[0]\n",
    "        f[x] = frequent_y\n",
    "    return f\n",
    "\n",
    "def update_regression(C, E, f, max_niterations=1000):\n",
    "    \"\"\"Update discrete regression with C as a cause variable and Y as a effect variable\n",
    "    so that it maximize likelihood\n",
    "    Args\n",
    "    -------\n",
    "        C (sequence): sequence of discrete outcomes\n",
    "        E (sequence): sequence of discrete outcomes\n",
    "        f (dict): map from C to Y\n",
    "\n",
    "    \"\"\"\n",
    "    supp_C = list(set(C))\n",
    "    supp_E = list(set(E))\n",
    "    mod_E = len(supp_E)\n",
    "    n = len(C)\n",
    "\n",
    "    # N_E's log likelihood\n",
    "    # optimize f to minimize N_E's log likelihood\n",
    "    cur_likelihood = 0\n",
    "    res = [(e - f[c]) % mod_E for c, e in zip(C, E)]\n",
    "    for freq in Counter(res).values():\n",
    "        cur_likelihood += freq * (log2(n) - log2(freq))\n",
    "\n",
    "    j = 0\n",
    "    minimized = True\n",
    "    while j < max_niterations and minimized:\n",
    "        minimized = False\n",
    "\n",
    "        for c_to_map in supp_C:\n",
    "            best_likelihood = sys.float_info.max\n",
    "            best_e = None\n",
    "\n",
    "            for cand_e in supp_E:\n",
    "                if cand_e == f[c_to_map]:\n",
    "                    continue\n",
    "\n",
    "                f_ = f.copy()\n",
    "                f_[c_to_map] = cand_e\n",
    "\n",
    "                \"\"\"\n",
    "                if len(set(f_.values())) == 1:\n",
    "                    continue\n",
    "                \"\"\"\n",
    "\n",
    "                neglikelihood = 0\n",
    "                res = [(e - f_[c]) % mod_E for c, e in zip(C, E)]\n",
    "                for freq in Counter(res).values():\n",
    "                    neglikelihood += freq * (log2(n) - log2(freq))\n",
    "\n",
    "                if neglikelihood < best_likelihood:\n",
    "                    best_likelihood = neglikelihood\n",
    "                    best_e = cand_e\n",
    "\n",
    "            if best_likelihood < cur_likelihood:\n",
    "                cur_likelihood = best_likelihood\n",
    "                f[c_to_map] = best_e\n",
    "                minimized = True\n",
    "        j += 1\n",
    "\n",
    "    return f\n",
    "\n",
    "def cause_effect_negloglikelihood(C, E, func):\n",
    "    \"\"\"Compute negative log likelihood for cause & effect pair.\n",
    "    Model type : C→E\n",
    "\n",
    "    Args\n",
    "    -------\n",
    "        C (sequence): sequence of discrete outcomes (Cause)\n",
    "        E (sequence): sequence of discrete outcomes (Effect)\n",
    "        func (dict): map from C-value to E-value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        (float): maximum log likelihood\n",
    "    \"\"\"\n",
    "    mod_C = len(set(C))\n",
    "    mod_E = len(set(E))\n",
    "    supp_C = list(set(C))\n",
    "    supp_E = list(set(E))\n",
    "\n",
    "    C_freqs = Counter(C)\n",
    "    n = len(C)\n",
    "\n",
    "    pair_cnt = defaultdict(lambda: defaultdict(int))\n",
    "    for c, e in zip(C, E):\n",
    "        pair_cnt[c][e] += 1\n",
    "\n",
    "    loglikelihood = 0\n",
    "\n",
    "    for freq in C_freqs.values():\n",
    "        loglikelihood += freq * (log2(n) - log2(freq))\n",
    "\n",
    "    for e_E in supp_E:\n",
    "        freq = 0\n",
    "        for e in supp_E:\n",
    "            for c in supp_C:\n",
    "                if (func[c] + e_E) % mod_E == e:\n",
    "                    freq += pair_cnt[c][e]\n",
    "        loglikelihood += freq * (log2(n) - log2(freq))\n",
    "\n",
    "    return loglikelihood\n",
    "\n",
    "def neg_log_likelihood(X, Y, model_type):\n",
    "    \"\"\"Compute negative maximum log-likelihood of the model given observations z^n.\n",
    "\n",
    "    Args\n",
    "    ------\n",
    "        X (sequence): sequence of discrete outcomes\n",
    "        Y (sequence): sequence of discrete outcomes\n",
    "        model_type (str): one of [\"to\", \"gets\", \"indep\", \"confounder\"]\n",
    "        f (dict): map from Y-values to frequently co-occuring X-values\n",
    "    Returns\n",
    "    -----------\n",
    "        (float): (negative) maximum log likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(X)\n",
    "    loglikelihood = 0\n",
    "\n",
    "    if model_type == \"to\":\n",
    "        f = map_to_majority(X, Y)\n",
    "        f = update_regression(X, Y, f)\n",
    "        loglikelihood = cause_effect_negloglikelihood(X, Y, f)\n",
    "\n",
    "    elif model_type == \"gets\":\n",
    "        g = map_to_majority(Y, X)\n",
    "        g = update_regression(Y, X, g)\n",
    "        loglikelihood = cause_effect_negloglikelihood(Y, X, g)\n",
    "\n",
    "    elif model_type == \"indep\":\n",
    "        X_freqs = Counter(X)\n",
    "        Y_freqs = Counter(Y)\n",
    "        for freq in X_freqs.values():\n",
    "            loglikelihood += freq * (log2(n) - log2(freq))\n",
    "        for freq in Y_freqs.values():\n",
    "            loglikelihood += freq * (log2(n) - log2(freq))\n",
    "\n",
    "    elif model_type == \"confounder\":\n",
    "        pair_cnt = defaultdict(lambda: defaultdict(int))\n",
    "        for x, y in zip(X, Y):\n",
    "            pair_cnt[x][y] += 1\n",
    "\n",
    "        for x in list(set(X)):\n",
    "            for y in list(set(Y)):\n",
    "                loglikelihood += pair_cnt[x][y] * (log2(n) - log2(pair_cnt[x][y]))\n",
    "\n",
    "    return loglikelihood\n",
    "\n",
    "def sc(X, Y, model_type: str, X_ndistinct_vals=None, Y_ndistinct_vals=None):\n",
    "    \"\"\"Computes the stochastic complexity of z^n(two discrete sequences).\n",
    "\n",
    "    Args\n",
    "    ------\n",
    "        X (sequence): sequence of discrete outcomes\n",
    "        Y (sequence): sequence of discrete outcomes\n",
    "        model_type (str): [\"to\", \"gets\", \"indep\", \"confounder\"]\n",
    "        X_ndistinct_vals (int): number of distinct values of the multinomial r.v X.\n",
    "        Y_ndistinct_vals (int): number of distinct values of the multinomial r.v Y.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        float: Stochastic Complexity of a given dataset\n",
    "    \"\"\"\n",
    "    assert len(X)==len(Y)\n",
    "    X_ndistinct_vals = X_ndistinct_vals or len(set(X))\n",
    "    Y_ndistinct_vals = Y_ndistinct_vals or len(set(Y))\n",
    "\n",
    "    data_cost =  neg_log_likelihood(X, Y, model_type)\n",
    "    model_cost = parametric_complexity(X, Y, model_type, X_ndistinct_vals, Y_ndistinct_vals)\n",
    "\n",
    "    stochastic_complexity = data_cost + model_cost\n",
    "\n",
    "    # add function code length\n",
    "    if model_type == \"to\":\n",
    "        stochastic_complexity += log2(Y_ndistinct_vals**(X_ndistinct_vals - 1) - 1)\n",
    "    elif model_type == \"gets\":\n",
    "        stochastic_complexity += log2(X_ndistinct_vals**(Y_ndistinct_vals - 1) - 1)\n",
    "\n",
    "    return stochastic_complexity\n",
    "\n",
    "def Cloud_print(score, llabel=\"X\", rlabel=\"Y\"):\n",
    "    score.sort(key=lambda x: x[0])\n",
    "    pred = score[0][1]\n",
    "    if pred == \"to\":\n",
    "        arrow = \"⇒\"\n",
    "    elif pred == \"gets\":\n",
    "        arrow = \"⇐\"\n",
    "    elif pred == \"indep\":\n",
    "        arrow = \"⫫\"\n",
    "    elif pred == \"confounder\":\n",
    "        arrow = \"⇐  C ⇒\"\n",
    "    conf = abs(score[0][0] - score[1][0])\n",
    "    out_str = \"Cloud Inference Result:: %s %s %s\\t Δ=%.2f\" % \\\n",
    "                          (llabel, arrow, rlabel, conf)\n",
    "    print(out_str)\n",
    "\n",
    "def Cloud(X, Y, n_candidates=4, is_print=False, X_ndistinct_vals=None, Y_ndistinct_vals=None):\n",
    "    \"\"\"main function in our study.\n",
    "    Cloud (Code Length-based method for Unobserved factor in Discrete data)\n",
    "\n",
    "    Args\n",
    "    ----------\n",
    "        X (sequence): sequence of discrete outcomes\n",
    "        Y (sequence): sequence of discrete outcomes\n",
    "        n_candidates (int): the number of model candidates\n",
    "        is_print (bool): whether or not to print inference result\n",
    "        X_ndistinct_vals (int): number of distinct values of the multinomial r.v X.\n",
    "        Y_ndistinct_vals (int): number of distinct values of the multinomial r.v Y.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        (List) : each element is tuple that contains code length L(z^n, M) (float) and causal model' label (str)\n",
    "        \n",
    "    \"\"\"\n",
    "    if n_candidates == 4:\n",
    "        MODEL_CANDIDATES = [\"to\", \"gets\", \"indep\", \"confounder\"]\n",
    "    elif n_candidates == 2:\n",
    "        MODEL_CANDIDATES = [\"to\", \"gets\"]\n",
    "    elif n_candidates == 1:\n",
    "        MODEL_CANDIDATES = [\"confounder\"]\n",
    "    else:\n",
    "        MODEL_CANDIDATES = [\"to\", \"gets\", \"indep\"]\n",
    "\n",
    "    # prepare data\n",
    "    le_X = LabelEncoder()\n",
    "    X = le_X.fit_transform(X)\n",
    "    le_Y = LabelEncoder()\n",
    "    Y = le_Y.fit_transform(Y)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model_type in MODEL_CANDIDATES:\n",
    "        stochastic_complexity = sc(X, Y, model_type, X_ndistinct_vals, Y_ndistinct_vals)\n",
    "        results.append((stochastic_complexity, model_type))\n",
    "\n",
    "    if is_print:\n",
    "        Cloud_print(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "def Cloud_output(score):\n",
    "    score.sort(key=lambda x: x[0])\n",
    "    pred = score[0][1]\n",
    "    conf = abs(score[0][0] - score[1][0])\n",
    "    return pred, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cloud_feature(dataset):\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "    X = discretize_sequence(dataset['X'].values, ffactor=5)\n",
    "    Y = discretize_sequence(dataset['Y'].values, ffactor=5)\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        v = discretize_sequence(dataset[variable].values, ffactor=5)\n",
    "\n",
    "        result_vX = Cloud(\n",
    "            X=v, \n",
    "            Y=X,\n",
    "            n_candidates=2, # select a set of model candidates\n",
    "            is_print=False, # print out inferred causal direction \n",
    "            X_ndistinct_vals=11,\n",
    "            Y_ndistinct_vals=11,\n",
    "        )\n",
    "        pred_vX, conf_vX = Cloud_output(result_vX)\n",
    "        result_vY = Cloud(\n",
    "            X=v, \n",
    "            Y=Y,\n",
    "            n_candidates=2, # select a set of model candidates\n",
    "            is_print=False, # print out inferred causal direction \n",
    "            X_ndistinct_vals=11,\n",
    "            Y_ndistinct_vals=11,\n",
    "        )\n",
    "        pred_vY, conf_vY = Cloud_output(result_vY)\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"Cloud_to(v,X)\": result_vX[0][0],\n",
    "            \"Cloud_gets(v,X)\": result_vX[1][0],\n",
    "            # \"Cloud_indep(v,X)\": result_vX[2][0],\n",
    "            # \"Cloud_confounder(v,X)\": result_vX[0][0],\n",
    "            \"Cloud_pred(v,X)\": pred_vX,\n",
    "            \"Cloud_conf(v,X)\": conf_vX,\n",
    "            \"Cloud_to(v,Y)\": result_vY[0][0],\n",
    "            \"Cloud_gets(v,Y)\": result_vY[1][0],\n",
    "            # \"Cloud_indep(v,Y)\": result_vY[2][0],\n",
    "            # \"Cloud_confounder(v,Y)\": result_vY[0][0],\n",
    "            \"Cloud_pred(v,Y)\": pred_vY,\n",
    "            \"Cloud_conf(v,Y)\": conf_vY,\n",
    "        })\n",
    "\n",
    "    # result_XY = Cloud(\n",
    "    #     X=X, \n",
    "    #     Y=Y,\n",
    "    #     n_candidates=4, # select a set of model candidates\n",
    "    #     is_print=False, # print out inferred causal direction \n",
    "    #     X_ndistinct_vals=21,\n",
    "    #     Y_ndistinct_vals=21,\n",
    "    # )\n",
    "    # pred_XY, conf_XY = Cloud_output(result_XY)\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # df[\"Cloud_to(X,Y)\"] = result_XY[0][0]\n",
    "    # df[\"Cloud_gets(X,Y)\"] = result_XY[1][0]\n",
    "    # df[\"Cloud_indep(X,Y)\"] = result_XY[2][0]\n",
    "    # df[\"Cloud_confounder(X,Y)\"] = result_XY[3][0]\n",
    "    # df[\"Cloud_pred(X,Y)\"] = pred_XY\n",
    "    # df[\"Cloud_conf(X,Y)\"] = conf_XY\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Given a graph as adjacency_matrix, create the class labels of each variable.\n",
    "    \"\"\"\n",
    "\n",
    "    adjacency_graph, adjacency_label = create_graph_label()\n",
    "    labels = get_labels(adjacency_matrix, adjacency_label)\n",
    "    variables = adjacency_matrix.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"variable\": variables,\n",
    "        \"label\": [labels[variable] for variable in variables],\n",
    "    })\n",
    "    df[\"dataset\"] = adjacency_matrix.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_some_columns(names_datasets, function):\n",
    "    \"\"\"\n",
    "    Apply an embedding function to a list of datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    df = []\n",
    "    for name, dataset in tqdm(names_datasets.items()):\n",
    "        dataset = names_datasets[name]\n",
    "        dataset.name = name\n",
    "    \n",
    "        try:\n",
    "            df_dataset = function(dataset)\n",
    "        except ValueError as e:\n",
    "            print(name, e)\n",
    "            raise NotImplementedError\n",
    "\n",
    "        df_dataset[\"dataset\"] = name\n",
    "        df.append(df_dataset)\n",
    "\n",
    "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_some_columns_parallel(names_datasets, function, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Apply an embedding function to a list of datasets.\n",
    "\n",
    "    Parallel version.\n",
    "    \"\"\"\n",
    "\n",
    "    def f(name, dataset, function):\n",
    "        dataset.name = name\n",
    "        df_dataset = function(dataset)\n",
    "        df_dataset[\"dataset\"] = name\n",
    "        return df_dataset\n",
    "\n",
    "    df = joblib.Parallel(n_jobs=n_jobs)(\n",
    "        joblib.delayed(f)(name, dataset, function)\n",
    "        for name, dataset in tqdm(names_datasets.items())\n",
    "    )\n",
    "\n",
    "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_columns(functions_names_datasets, n_jobs=-1, create_dimension_feature = False):\n",
    "    \"\"\"\n",
    "    given a dictionary of {function1:names, function2:names,...} apply\n",
    "    the desired functions to the list of datasets and merge all of them\n",
    "    in a single X_y_group dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    columns = []\n",
    "    if create_dimension_feature:\n",
    "        dimension_feature = create_some_columns(functions_names_datasets[list(functions_names_datasets.keys())[0]], add_dimension_feature)\n",
    "        columns.append(dimension_feature)\n",
    "    \n",
    "    for function, names_datasets in functions_names_datasets.items():\n",
    "        print(f\"set: {function.__name__}\")\n",
    "\n",
    "        if n_jobs != 1:\n",
    "            feature_set = create_some_columns_parallel(names_datasets, function, n_jobs=n_jobs)\n",
    "        else:\n",
    "            feature_set = create_some_columns(names_datasets, function)\n",
    "\n",
    "        columns.append(feature_set)\n",
    "\n",
    "    # Merge all feature sets into a single dataframe:\n",
    "    columns = functools.reduce(\n",
    "        lambda left, right: pd.merge(left, right, on=[\"dataset\", \"variable\"]),\n",
    "        columns,\n",
    "    )\n",
    "\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_group_train = pd.read_csv('mid_data/X_y_group_train_updated_v13.8_cloudXY4.csv') #需要修改为增强后数据\n",
    "print(X_y_group_train.shape)\n",
    "print(X_y_group_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_datasets_train = X_train\n",
    "names_graphs_train = y_train\n",
    "X_y_group_train_additional = create_all_columns(\n",
    "    {\n",
    "        Cloud_feature: names_datasets_train,\n",
    "    },\n",
    "    n_jobs=-1,\n",
    ")\n",
    "X_y_group_train = pd.concat([X_y_group_train, X_y_group_train_additional], axis=1)\n",
    "print('X_y_group_train.shape', X_y_group_train.shape)\n",
    "# 去掉重复的列\n",
    "X_y_group_train = X_y_group_train.loc[:,~X_y_group_train.columns.duplicated()]\n",
    "print('去重后X_y_group_train.shape', X_y_group_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_group_train.to_csv('./mid_data/X_y_group_train_updated_v13.9_cloudvX2vY2.csv', index=False) ### 需要修改为新的文件名，提交版本需要对应更新Cloud_feature\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
