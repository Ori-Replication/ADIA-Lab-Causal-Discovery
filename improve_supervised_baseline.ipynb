{
        "cells": [
                {
                        "cell_type": "code",
                        "execution_count": 1,
                        "metadata": {},
                        "outputs": [
                                {
                                        "data": {
                                                "text/plain": [
                                                        "'\\n                       _oo0oo_\\n                      o8888888o\\n                      88\" . \"88\\n                      (| -_- |)\\n                      0\\\\  =  /0\\n                    ___/`---\\'\\\\___\\n                  .\\' \\\\|     |// \\'.\\n                 / \\\\|||  :  |||//                 / _||||| -:- |||||-                |   | \\\\\\\\  - /// |   |\\n               | \\\\_|  \\'\\'\\\\---/\\'\\'  |_/ |\\n               \\\\  .-\\\\__  \\'-\\'  ___/-. /\\n             ___\\'. .\\'  /--.--\\\\  `. .\\'___\\n          .\"\" \\'<  `.___\\\\_<|>_/___.\\' >\\' \"\".\\n         | | :  `- \\\\`.;`\\\\ _ /`;.`/ - ` : | |\\n         \\\\  \\\\ `_.   \\\\_ __\\\\ /__ _/   .-` /  /\\n     =====`-.____`.___ \\\\_____/___.-`___.-\\'=====\\n                       `=---=\\'\\n\\n\\n     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\n           佛祖保佑     永不宕机     永无BUG\\n'"
                                                ]
                                        },
                                        "execution_count": 1,
                                        "metadata": {},
                                        "output_type": "execute_result"
                                }
                        ],
                        "source": [
                                "'''\n",
                                "                       _oo0oo_\n",
                                "                      o8888888o\n",
                                "                      88\" . \"88\n",
                                "                      (| -_- |)\n",
                                "                      0\\  =  /0\n",
                                "                    ___/`---'\\___\n",
                                "                  .' \\\\|     |// '.\n",
                                "                 / \\\\|||  :  |||// \\\n",
                                "                / _||||| -:- |||||- \\\n",
                                "               |   | \\\\\\  - /// |   |\n",
                                "               | \\_|  ''\\---/''  |_/ |\n",
                                "               \\  .-\\__  '-'  ___/-. /\n",
                                "             ___'. .'  /--.--\\  `. .'___\n",
                                "          .\"\" '<  `.___\\_<|>_/___.' >' \"\".\n",
                                "         | | :  `- \\`.;`\\ _ /`;.`/ - ` : | |\n",
                                "         \\  \\ `_.   \\_ __\\ /__ _/   .-` /  /\n",
                                "     =====`-.____`.___ \\_____/___.-`___.-'=====\n",
                                "                       `=---='\n",
                                "\n",
                                "\n",
                                "     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
                                "\n",
                                "           佛祖保佑     永不宕机     永无BUG\n",
                                "'''"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 2,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-21T11:20:39.508775Z",
                                        "start_time": "2024-09-21T11:20:32.677062Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [
                                {
                                        "name": "stderr",
                                        "output_type": "stream",
                                        "text": [
                                                "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                                                "  from .autonotebook import tqdm as notebook_tqdm\n",
                                                "2024-10-05 18:14:52,962\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
                                        ]
                                }
                        ],
                        "source": [
                                "import functools\n",
                                "import os\n",
                                "import typing\n",
                                "import random\n",
                                "import joblib\n",
                                "import dcor\n",
                                "import networkx as nx\n",
                                "import numpy as np\n",
                                "import pandas as pd\n",
                                "import pingouin as pg\n",
                                "from tqdm.auto import tqdm\n",
                                "from sklearn.ensemble import RandomForestClassifier\n",
                                "from sklearn.feature_selection import mutual_info_regression\n",
                                "from sklearn.model_selection import GridSearchCV, GroupKFold, cross_val_score\n",
                                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                                "from sklearn.linear_model import Ridge, LinearRegression, LassoCV\n",
                                "from pgmpy.estimators import CITests\n",
                                "from scipy.spatial.distance import pdist, squareform\n",
                                "from econml.dml import CausalForestDML, LinearDML\n",
                                "from causallearn.graph.GraphNode import GraphNode\n",
                                "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge\n",
                                "from causallearn.search.ConstraintBased.PC import pc\n",
                                "from causallearn.search.ScoreBased.ExactSearch import bic_exact_search\n",
                                "from causallearn.search.ConstraintBased.FCI import fci"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 3,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "import warnings\n",
                                "warnings.filterwarnings('ignore', category=DeprecationWarning)   # 忽略PPS的警告"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 4,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-20T02:42:13.047359Z",
                                        "start_time": "2024-09-20T02:42:12.325055Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [
                                {
                                        "name": "stdout",
                                        "output_type": "stream",
                                        "text": [
                                                "loaded inline runner with module: <module '__main__'>\n"
                                        ]
                                }
                        ],
                        "source": [
                                "import crunch\n",
                                "crunch = crunch.load_notebook()"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 5,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:30:54.182971Z",
                                        "start_time": "2024-09-18T09:30:54.145678Z"
                                }
                        },
                        "outputs": [],
                        "source": [
                                "def graph_nodes_representation(graph, nodelist):\n",
                                "    \"\"\"\n",
                                "    Create an alternative representation of a graph which is hashable\n",
                                "    and equivalent graphs have the same hash.\n",
                                "\n",
                                "    Python cannot PROPERLY use nx.Graph/DiGraph as key for\n",
                                "    dictionaries, because two equivalent graphs with just different\n",
                                "    order of the nodes would result in different keys. This is\n",
                                "    undesirable here.\n",
                                "\n",
                                "    So here we transform the graph into an equivalent form that is\n",
                                "    based on a specific nodelist and that is hashable. In this way,\n",
                                "    two equivalent graphs, once transformed, will result in identical\n",
                                "    keys.\n",
                                "\n",
                                "    So we use the following trick: extract the adjacency matrix\n",
                                "    (with nodes in a fixed order) and then make a hashable thing out\n",
                                "    of it, through tuple(array.flatten()):\n",
                                "    \"\"\"\n",
                                "\n",
                                "    # This get the adjacency matrix with nodes in a given order, as\n",
                                "    # numpy array (which is not hashable):\n",
                                "    adjacency_matrix = nx.adjacency_matrix(graph, nodelist=nodelist).todense()\n",
                                "\n",
                                "    # This transforms the numpy array into a hashable object:\n",
                                "    hashable = tuple(adjacency_matrix.flatten())\n",
                                "\n",
                                "    return hashable\n",
                                "\n",
                                "def create_graph_label():\n",
                                "    \"\"\"\n",
                                "    Create a dictionary from graphs to labels, in two formats.\n",
                                "    \"\"\"\n",
                                "    graph_label = {\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]):             \"Cause of X\",\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]):             \"Cause of Y\",\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]):             \"Consequence of X\",\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]):             \"Consequence of Y\",\n",
                                "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}):                \"Independent\",\n",
                                "    }\n",
                                "\n",
                                "    nodelist = [\"v\", \"X\", \"Y\"]\n",
                                "\n",
                                "    # This is an equivalent alternative to graph_label but in a form\n",
                                "    # for which two equivalent graphs have the same key:\n",
                                "    adjacency_label = {\n",
                                "        graph_nodes_representation(graph, nodelist): label\n",
                                "        for graph, label in graph_label.items()\n",
                                "    }\n",
                                "\n",
                                "    return graph_label, adjacency_label\n",
                                "\n",
                                "def get_labels(adjacency_matrix, adjacency_label):\n",
                                "    \"\"\"\n",
                                "    Transform an adjacency_matrix (as pd.DataFrame) into a dictionary of variable:label\n",
                                "    \"\"\"\n",
                                "\n",
                                "    result = {}\n",
                                "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
                                "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]  # this is not hashable\n",
                                "        key = tuple(submatrix.values.flatten())  # this is hashable and a compatible with adjacency_label\n",
                                "    \n",
                                "        result[variable] = adjacency_label[key]\n",
                                "\n",
                                "    return result\n",
                                "\n",
                                "graph_label, adjacency_label = create_graph_label()"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 6,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "def apply_mapping(df, mapping):\n",
                                "    df_new = df.copy()\n",
                                "    # 创建临时映射以避免冲突\n",
                                "    temp_mapping = {k: f'_temp_{k}' for k in mapping.keys()}\n",
                                "    df_new.rename(columns=temp_mapping, inplace=True)\n",
                                "    if df_new.shape[0] == df_new.shape[1]:  # 如果是方阵，如标签矩阵\n",
                                "        df_new.rename(index=temp_mapping, inplace=True)\n",
                                "    # 应用最终映射\n",
                                "    final_mapping = {f'_temp_{k}': v for k, v in mapping.items()}\n",
                                "    df_new.rename(columns=final_mapping, inplace=True)\n",
                                "    if df_new.shape[0] == df_new.shape[1]:\n",
                                "        df_new.rename(index=final_mapping, inplace=True)\n",
                                "    return df_new\n",
                                "\n",
                                "def check_duplicate_columns(df):\n",
                                "    \"\"\"检查是否存在重复的列名\"\"\"\n",
                                "    return df.columns.duplicated().any()\n",
                                "\n",
                                "def augment_data(X_train, y_train):\n",
                                "    new_X_train = X_train.copy()\n",
                                "    new_y_train = y_train.copy()\n",
                                "    for sample_id in X_train.keys():\n",
                                "        X = X_train[sample_id]\n",
                                "        y = y_train[sample_id]\n",
                                "        variables = list(X.columns)\n",
                                "        dim = len(variables)\n",
                                "        # 提取因果关系对\n",
                                "        edges = []\n",
                                "        for u in y.index:\n",
                                "            for v in y.columns:\n",
                                "                if y.loc[u, v] == 1:\n",
                                "                    edges.append((u, v))\n",
                                "        # 排除涉及 X 和 Y 的边\n",
                                "        edges_no_XY = [(u, v) for (u, v) in edges if u not in ['X', 'Y'] and v not in ['X', 'Y']]\n",
                                "        if dim >= 4:\n",
                                "            edges_to_use = edges_no_XY\n",
                                "            attempts = 0\n",
                                "            success = False\n",
                                "            while attempts < 3 and not success:\n",
                                "                if not edges_to_use:\n",
                                "                    break  # 没有合适的边，跳出循环\n",
                                "                u, v = random.choice(edges_to_use)\n",
                                "                mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
                                "                # 应用映射到特征矩阵和标签矩阵\n",
                                "                X_new = apply_mapping(X, mapping)\n",
                                "                y_new = apply_mapping(y, mapping)\n",
                                "                # 检查特征矩阵是否有重复列\n",
                                "                if check_duplicate_columns(X_new):\n",
                                "                    attempts += 1\n",
                                "                    continue  # 重试\n",
                                "                else:\n",
                                "                    # 没有重复列，存储新的数据\n",
                                "                    new_sample_id = '0' + sample_id\n",
                                "                    new_X_train[new_sample_id] = X_new\n",
                                "                    new_y_train[new_sample_id] = y_new\n",
                                "                    success = True\n",
                                "            if not success:\n",
                                "                # 没有找到合适的映射，复制原始数据\n",
                                "                new_sample_id = '0' + sample_id\n",
                                "                new_X_train[new_sample_id] = X.copy()\n",
                                "                new_y_train[new_sample_id] = y.copy()\n",
                                "        else:\n",
                                "            # 对于维度较低的数据，允许涉及 X 和 Y 的边\n",
                                "            edges_to_use = edges\n",
                                "            if not edges_to_use:\n",
                                "                # 没有边，复制原始数据\n",
                                "                new_sample_id = '0' + sample_id\n",
                                "                new_X_train[new_sample_id] = X.copy()\n",
                                "                new_y_train[new_sample_id] = y.copy()\n",
                                "                continue\n",
                                "            u, v = random.choice(edges_to_use)\n",
                                "            mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
                                "            # 应用映射到特征矩阵和标签矩阵\n",
                                "            X_new = apply_mapping(X, mapping)\n",
                                "            y_new = apply_mapping(y, mapping)\n",
                                "            # 检查特征矩阵是否有重复列\n",
                                "            if check_duplicate_columns(X_new):\n",
                                "                # 如果有重复列，复制原始数据\n",
                                "                new_sample_id = '0' + sample_id\n",
                                "                new_X_train[new_sample_id] = X.copy()\n",
                                "                new_y_train[new_sample_id] = y.copy()\n",
                                "            else:\n",
                                "                # 没有重复列，存储新的数据\n",
                                "                new_sample_id = '0' + sample_id\n",
                                "                new_X_train[new_sample_id] = X_new\n",
                                "                new_y_train[new_sample_id] = y_new\n",
                                "    return new_X_train, new_y_train"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 7,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-20T02:43:04.532840Z",
                                        "start_time": "2024-09-20T02:42:51.402011Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [
                                {
                                        "name": "stdout",
                                        "output_type": "stream",
                                        "text": [
                                                "23500 23500\n"
                                        ]
                                }
                        ],
                        "source": [
                                "# X_train, y_train, X_test = crunch.load_data()\n",
                                "X_train = pd.read_pickle('./data/X_train.pickle')\n",
                                "y_train = pd.read_pickle('./data/y_train.pickle')\n",
                                "print(len(X_train), len(y_train))\n",
                                "# X_train, y_train = augment_data(X_train, y_train)"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 8,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "##################################################################################\n",
                                "###  Estimating Copula Entropy and Transfer Entropy \n",
                                "###  2024-10-02\n",
                                "###  by Ma Jian (Email: majian03@gmail.com)\n",
                                "###\n",
                                "###  Parameters\n",
                                "###\tx    \t: N * d data, N samples, d dimensions\n",
                                "###\tk    \t: kth nearest neighbour, parameter for kNN entropy estimation. default = 3\n",
                                "###\tdtype\t: distance type ['euclidean', 'chebychev' (i.e Maximum distance)]\n",
                                "###\tlag\t: time lag. default = 1\n",
                                "###\ts0,s1\t: two samples with same dimension\n",
                                "###\tn\t: repeat time of estimation. default = 12\n",
                                "###\tthd\t: threshold for the statistic of two-sample test\n",
                                "###\tmaxp\t: maximal number of change points\n",
                                "###\tminseglen : minimal length of binary segmentation\n",
                                "###\n",
                                "###  References\n",
                                "###  [1] Ma Jian, Sun Zengqi. Mutual information is copula entropy. \n",
                                "###      arXiv:0808.0845, 2008.\n",
                                "###  [2] Kraskov A, Stögbauer H, Grassberger P. Estimating mutual information. \n",
                                "###      Physical review E, 2004, 69(6): 066138.\n",
                                "###  [3] Ma, Jian. Estimating Transfer Entropy via Copula Entropy. \n",
                                "###      arXiv preprint arXiv:1910.04375, 2019.\n",
                                "###  [4] Ma, Jian. Multivariate Normality Test with Copula Entropy.\n",
                                "###      arXiv preprint arXiv:2206.05956, 2022.\n",
                                "###  [5] Ma, Jian. Two-Sample Test with Copula Entropy.\n",
                                "###      arXiv preprint arXiv:2307.07247, 2023.\n",
                                "###  [6] Ma, Jian. Change Point Detection with Copula Entropy based Two-Sample Test.\n",
                                "###      arXiv preprint arXiv:2403.07892, 2024.\n",
                                "##################################################################################\n",
                                "\n",
                                "from scipy.special import digamma\n",
                                "from scipy.stats import rankdata as rank \n",
                                "from scipy.spatial.distance import cdist\n",
                                "from math import gamma, log, pi\n",
                                "from numpy import array, abs, max, hstack, vstack, ones, zeros, cov, matrix, where\n",
                                "from numpy.random import uniform, normal as rnorm\n",
                                "from numpy.linalg import det\n",
                                "from multiprocessing.pool import Pool,ThreadPool\n",
                                "import sys\n",
                                "\n",
                                "##### constructing empirical copula density [1]\n",
                                "def construct_empirical_copula(x):\n",
                                "\t(N,d) = x.shape\t\n",
                                "\txc = zeros([N,d]) \n",
                                "\tfor i in range(0,d):\n",
                                "\t\txc[:,i] = rank(x[:,i]) / N\n",
                                "\t\n",
                                "\treturn xc\n",
                                "\n",
                                "##### Estimating entropy with kNN method [2]\n",
                                "def entknn(x, k = 3, dtype = 'chebychev'):\n",
                                "\t(N,d) = x.shape\n",
                                "\t\n",
                                "\tg1 = digamma(N) - digamma(k)\n",
                                "\t\n",
                                "\tif dtype == 'euclidean':\n",
                                "\t\tcd = pi**(d/2) / 2**d / gamma(1+d/2)\n",
                                "\telse:\t# (chebychev) maximum distance\n",
                                "\t\tcd = 1;\n",
                                "\n",
                                "\tlogd = 0\n",
                                "\tdists = cdist(x, x, dtype)\n",
                                "\tdists.sort()\n",
                                "\tfor i in range(0,N):\n",
                                "\t\tlogd = logd + log( 2 * dists[i,k] ) * d / N\n",
                                "\n",
                                "\treturn (g1 + log(cd) + logd)\n",
                                "\n",
                                "##### 2-step Nonparametric estimation of copula entropy [1]\n",
                                "def copent(x, k = 3, dtype = 'chebychev', log0 = False):\n",
                                "\txarray = array(x)\n",
                                "\n",
                                "\tif log0:\n",
                                "\t\t(N,d) = xarray.shape\n",
                                "\t\tmax1 = max(abs(xarray), axis = 0)\n",
                                "\t\tfor i in range(0,d):\n",
                                "\t\t\tif max1[i] == 0:\n",
                                "\t\t\t\txarray[:,i] = rnorm(0,1,N)\n",
                                "\t\t\telse:\n",
                                "\t\t\t\txarray[:,i] = xarray[:,i] + rnorm(0,1,N) * max1[i] * 0.000005\n",
                                "\n",
                                "\txc = construct_empirical_copula(xarray)\n",
                                "\n",
                                "\ttry:\n",
                                "\t\treturn -entknn(xc, k, dtype)\n",
                                "\texcept ValueError: # log0 error\n",
                                "\t\treturn copent(x, k, dtype, log0 = True)\n",
                                "\n",
                                "\n",
                                "##### conditional independence test [3]\n",
                                "##### to test independence of (x,y) conditioned on z\n",
                                "def ci(x, y, z, k = 3, dtype = 'chebychev'):\n",
                                "\txyz = vstack((x,y,z)).T\n",
                                "\tyz = vstack((y,z)).T\n",
                                "\txz = vstack((x,z)).T\n",
                                "\treturn copent(xyz,k,dtype) - copent(yz,k,dtype) - copent(xz,k,dtype)\n",
                                "\n",
                                "##### estimating transfer entropy from y to x with lag [3]\n",
                                "def transent(x, y, lag = 1, k = 3, dtype = 'chebychev'):\n",
                                "\txlen = len(x)\n",
                                "\tylen = len(y)\n",
                                "\tif (xlen > ylen):\n",
                                "\t\tl = ylen\n",
                                "\telse:\n",
                                "\t\tl = xlen\n",
                                "\tif (l < (lag + k + 1)):\n",
                                "\t\treturn 0\n",
                                "\tx1 = x[0:(l-lag)]\n",
                                "\tx2 = x[lag:l]\n",
                                "\ty = y[0:(l-lag)]\n",
                                "\treturn ci(x2,y,x1,k,dtype)\n",
                                "\n",
                                "##### multivariate normality test [4]\n",
                                "def mvnt(x, k = 3, dtype = 'chebychev'):\n",
                                "\treturn -0.5 * log(det(cov(x.T))) - copent(x,k,dtype)\n",
                                "\n",
                                "##### two-sample test [5]\n",
                                "def tst(s0,s1,n=12, k = 3, dtype = 'chebychev'):\n",
                                "\t(N0,d0) = s0.shape\n",
                                "\t(N1,d1) = s1.shape\n",
                                "\tx = vstack((s0,s1))\n",
                                "\tstat1 = 0\n",
                                "\tfor i in range(0,n):\n",
                                "\t\ty1 = vstack((ones([N0,1]),ones([N1,1])*2)) + uniform(0, 0.0000001,[N0+N1,1])\n",
                                "\t\ty0 = ones([N0+N1,1]) + uniform(0,0.0000001,[N0+N1,1])\n",
                                "\t\tstat1 = stat1 + copent(hstack((x,y1)),k,dtype) - copent(hstack((x,y0)),k,dtype)\n",
                                "\treturn stat1/n\n",
                                "\n",
                                "##### single change point detection [6]\n",
                                "def init(X,N,K,DTYPE):\n",
                                "\tglobal x,n,k,dtype\n",
                                "\tx = X\n",
                                "\tn = N\n",
                                "\tk = K\n",
                                "\tdtype = DTYPE\n",
                                "\n",
                                "def tsti(i):\n",
                                "\ts0 = x[0:(i+1),:]\n",
                                "\ts1 = x[(i+2):,:]\n",
                                "\treturn tst(s0,s1,n,k,dtype)\n",
                                "\t\n",
                                "def cpd(x, thd = 0.13, n = 30, k = 3, dtype = 'chebychev'):\n",
                                "\tx = matrix(x)\n",
                                "\tlen1 = x.shape[0]\n",
                                "\tif len1 == 1:\n",
                                "\t\tlen1 = x.shape[1]\n",
                                "\t\tx = x.T\n",
                                "\tpos = -1\n",
                                "\tmaxstat = 0\n",
                                "\tif sys.platform.startswith(\"win\"): # \"win\"\n",
                                "\t    pool = ThreadPool(initializer = init, initargs=(x,n,k,dtype))\n",
                                "\telse: # \"linux\" or \"darwin\"\n",
                                "\t    pool = Pool(initializer = init, initargs=(x,n,k,dtype))\n",
                                "\tstat1 = [0] + pool.map(tsti,range(len1-2)) + [0]\n",
                                "\tpool.close()\n",
                                "\tif(max(stat1) > thd):\n",
                                "\t\tmaxstat = max(stat1)\n",
                                "\t\tpos = where(stat1 == maxstat)[0][0]+1\n",
                                "\treturn pos, maxstat, stat1\n",
                                "\n",
                                "##### multiple change point detection [6]\n",
                                "def mcpd(x, maxp = 5, thd = 0.13, minseglen = 10, n = 30, k = 3, dtype = 'chebychev'):\n",
                                "\tx = matrix(x)\n",
                                "\tlen1 = x.shape[0]\n",
                                "\tif len1 == 1:\n",
                                "\t\tlen1 = x.shape[1]\n",
                                "\t\tx = x.T\n",
                                "\tmaxstat = []\n",
                                "\tpos = []\n",
                                "\tbisegs = matrix([0,len1-1])\n",
                                "\tfor i in range(0,maxp):\n",
                                "\t\tif i >= bisegs.shape[0]:\n",
                                "\t\t\tbreak\n",
                                "\t\trpos, rmaxstat, _ = cpd(x[bisegs[i,0]:bisegs[i,1],:],thd,n,k,dtype)\n",
                                "\t\tif rpos > -1 :\n",
                                "\t\t\trpos = rpos + bisegs[i,0]\n",
                                "\t\t\tmaxstat.append(rmaxstat)\n",
                                "\t\t\tpos.append(rpos)\n",
                                "\t\t\tif (rpos - bisegs[i,0]) > minseglen :\n",
                                "\t\t\t\tbisegs = vstack((bisegs,[bisegs[i,0],rpos-1]))\n",
                                "\t\t\tif (bisegs[i,1] - rpos +1) > minseglen :\n",
                                "\t\t\t\tbisegs = vstack((bisegs,[rpos,bisegs[i,1]]))\n",
                                "\treturn pos,maxstat"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 9,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:31:23.461861Z",
                                        "start_time": "2024-09-18T09:31:23.368017Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "def pearson_correlation(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the correlation-based features for each\n",
                                "    varibale, which are the correlation between that variable with X and Y,\n",
                                "    as well as summary statistics (max, min, mean, std) of all pairs\n",
                                "    of correlations.\n",
                                "    \"\"\"\n",
                                "\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = dataset.corr().drop([variable], axis=\"columns\").loc[variable].abs()\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"corr(v,X)\": dataset[[variable, \"X\"]].corr().loc[variable, \"X\"],\n",
                                "            \"corr(v,Y)\": dataset[[variable, \"Y\"]].corr().loc[variable, \"Y\"],\n",
                                "            \"max(corr(v, others))\": tmp.max(),\n",
                                "            \"min(corr(v, others))\": tmp.min(),\n",
                                "            \"mean(corr(v, others))\": tmp.mean(),\n",
                                "            \"std(corr(v, others))\": tmp.std(),\n",
                                "            \"25%(corr(v, others))\": tmp.quantile(0.25), \n",
                                "            \"75%(corr(v, others))\": tmp.quantile(0.75), \n",
                                "        })\n",
                                "\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr().loc[\"X\", \"Y\"]\n",
                                "\n",
                                "    # pearsonr is NaN when the variance is 0, so we fill with 0\n",
                                "    df.fillna(0, inplace=True)\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def mutual_information(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the mutual-information-based features\n",
                                "    for each variable, which are the MI between that variable\n",
                                "    and X and Y, as well as summary statistics (max, min, mean, std) of\n",
                                "    all pairs of MI.\n",
                                "    \"\"\"\n",
                                "\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = mutual_info_regression(dataset.drop(columns=[variable]), dataset[variable])\n",
                                "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"MI(v,X)\": mutual_info_regression(dataset[[variable]], dataset[\"X\"], discrete_features=False)[0],\n",
                                "            \"MI(v,Y)\": mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0],\n",
                                "            \"max(MI(v, others))\": tmp.max(),\n",
                                "            \"min(MI(v, others))\": tmp.min(),\n",
                                "            \"mean(MI(v, others))\": tmp.mean(),\n",
                                "            \"std(MI(v, others))\": tmp.std(),\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"MI(X,Y)\"] = mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def add_dimension_feature(dataset):\n",
                                "    \"\"\"\n",
                                "    Add a dimension feature to the dataset.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "    dimension = len(variables)\n",
                                "    square_dimension = dimension * dimension\n",
                                "    df = pd.DataFrame({\n",
                                "        \"variable\": variables,\n",
                                "        \"dimension\": dimension,\n",
                                "        \"square_dimension\": square_dimension\n",
                                "    })\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    return df\n",
                                "\n",
                                "def spearman_correlation(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the Spearman rank correlation-based features for each\n",
                                "    variable, which are the Spearman correlation between that variable with X and Y,\n",
                                "    as well as summary statistics (max, min, mean, std) of all pairs of Spearman correlations.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = dataset.corr(method='spearman').drop([variable], axis=\"columns\").loc[variable].abs()\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"spearman_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='spearman').loc[variable, \"X\"],\n",
                                "            \"spearman_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='spearman').loc[variable, \"Y\"],\n",
                                "            \"max(spearman_corr(v, others))\": tmp.max(),\n",
                                "            \"min(spearman_corr(v, others))\": tmp.min(),\n",
                                "            \"mean(spearman_corr(v, others))\": tmp.mean(),\n",
                                "            \"std(spearman_corr(v, others))\": tmp.std(),\n",
                                "            #TODO 分位数\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"spearman_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='spearman').loc[\"X\", \"Y\"]\n",
                                "\n",
                                "    # Spearman correlation is NaN when there are ties in rank, so we fill with 0\n",
                                "    df.fillna(0, inplace=True)\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def kendall_correlation(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the Kendall's tau correlation-based features for each\n",
                                "    variable, which are the Kendall's tau correlation between that variable with X and Y,\n",
                                "    as well as summary statistics (max, min, mean, std) of all pairs of Kendall's tau correlations.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = dataset.corr(method='kendall').drop([variable], axis=\"columns\").loc[variable].abs()\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"kendall_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='kendall').loc[variable, \"X\"],\n",
                                "            \"kendall_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='kendall').loc[variable, \"Y\"],\n",
                                "            \"max(kendall_corr(v, others))\": tmp.max(),\n",
                                "            \"min(kendall_corr(v, others))\": tmp.min(),\n",
                                "            \"mean(kendall_corr(v, others))\": tmp.mean(),\n",
                                "            \"std(kendall_corr(v, others))\": tmp.std(),\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"kendall_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='kendall').loc[\"X\", \"Y\"]\n",
                                "\n",
                                "    # Kendall's tau correlation can be NaN in some cases, so we fill with 0\n",
                                "    df.fillna(0, inplace=True)\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def distance_correlation(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the distance correlation-based features for each\n",
                                "    variable, which are the distance correlation between that variable with X and Y,\n",
                                "    as well as summary statistics (max, min, mean, std) of all pairs of distance correlations.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = []\n",
                                "        # Compute distance correlation between 'variable' and all other variables (excluding itself)\n",
                                "        other_variables = dataset.columns.drop([variable])\n",
                                "        for other_var in other_variables:\n",
                                "            corr = dcor.distance_correlation(dataset[variable], dataset[other_var])\n",
                                "            tmp.append(corr)\n",
                                "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
                                "\n",
                                "        distance_correlation_v_X = dcor.distance_correlation(dataset[variable], dataset[\"X\"])\n",
                                "        distance_correlation_v_Y = dcor.distance_correlation(dataset[variable], dataset[\"Y\"])\n",
                                "        distance_correlation_X_Y = dcor.distance_correlation(dataset[\"X\"], dataset[\"Y\"])\n",
                                "        distance_correlation_v_X_square = distance_correlation_v_X ** 2\n",
                                "        distance_correlation_v_Y_square = distance_correlation_v_Y ** 2\n",
                                "        distance_correlation_X_Y_square = distance_correlation_X_Y ** 2\n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"dcor(v,X)\": distance_correlation_v_X,\n",
                                "            \"dcor(v,Y)\": distance_correlation_v_Y,\n",
                                "            \"dcor(v,X)^2\": distance_correlation_v_X_square,\n",
                                "            \"dcor(v,Y)^2\": distance_correlation_v_Y_square,\n",
                                "            \"max(dcor(v, others))\": tmp.max(),\n",
                                "            \"min(dcor(v, others))\": tmp.min(),\n",
                                "            \"mean(dcor(v, others))\": tmp.mean(),\n",
                                "            \"std(dcor(v, others))\": tmp.std(),\n",
                                "            \"25%(dcor(v, others))\": tmp.quantile(0.25),\n",
                                "            \"75%(dcor(v, others))\": tmp.quantile(0.75),\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"dcor(X,Y)\"] = distance_correlation_X_Y\n",
                                "    df[\"dcor(X,Y)^2\"] = distance_correlation_X_Y_square\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def distance_correlation_different_exponent(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the distance correlation-based features for each\n",
                                "    variable, which are the distance correlation between that variable with X and Y,\n",
                                "    with different exponent, as well as summary statistics (max, min, mean, std) of all pairs of distance correlations.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = []\n",
                                "        # Compute distance correlation between 'variable' and all other variables (excluding itself)\n",
                                "        other_variables = dataset.columns.drop([variable])\n",
                                "        for other_var in other_variables:\n",
                                "            corr = dcor.distance_correlation(dataset[variable], dataset[other_var])\n",
                                "            tmp.append(corr)\n",
                                "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
                                "\n",
                                "        distance_correlation_v_X = dcor.distance_correlation(dataset[variable], dataset[\"X\"], exponent=0.5)\n",
                                "        distance_correlation_v_Y = dcor.distance_correlation(dataset[variable], dataset[\"Y\"], exponent=0.5)\n",
                                "        distance_correlation_X_Y = dcor.distance_correlation(dataset[\"X\"], dataset[\"Y\"], exponent=0.5)\n",
                                "        distance_correlation_v_X_square = distance_correlation_v_X ** 2\n",
                                "        distance_correlation_v_Y_square = distance_correlation_v_Y ** 2\n",
                                "        distance_correlation_X_Y_square = distance_correlation_X_Y ** 2\n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"dcor_0.5exp(v,X)\": distance_correlation_v_X,\n",
                                "            \"dcor_0.5exp(v,Y)\": distance_correlation_v_Y,\n",
                                "            \"dcor_0.5exp(v,X)^2\": distance_correlation_v_X_square,\n",
                                "            \"dcor_0.5exp(v,Y)^2\": distance_correlation_v_Y_square,\n",
                                "            \"max(dcor_0.5exp(v, others))\": tmp.max(),\n",
                                "            \"min(dcor_0.5exp(v, others))\": tmp.min(),\n",
                                "            \"mean(dcor_0.5exp(v, others))\": tmp.mean(),\n",
                                "            \"std(dcor_0.5exp(v, others))\": tmp.std(),\n",
                                "            \"25%(dcor_0.5exp(v, others))\": tmp.quantile(0.25),\n",
                                "            \"75%(dcor_0.5exp(v, others))\": tmp.quantile(0.75),\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"dcor_0.5exp(X,Y)\"] = distance_correlation_X_Y\n",
                                "    df[\"dcor_0.5exp(X,Y)^2\"] = distance_correlation_X_Y_square\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def double_centered_distance_matrix(X):\n",
                                "    \"\"\"Compute the double centered distance matrix for X.\"\"\"\n",
                                "    n = X.shape[0]\n",
                                "    dist_matrix = squareform(pdist(X[:, None]))  # Compute pairwise distances\n",
                                "    row_mean = np.mean(dist_matrix, axis=1, keepdims=True)\n",
                                "    col_mean = np.mean(dist_matrix, axis=0, keepdims=True)\n",
                                "    total_mean = np.mean(dist_matrix)\n",
                                "\n",
                                "    # Double-centering the matrix\n",
                                "    A = dist_matrix - row_mean - col_mean + total_mean\n",
                                "    return A\n",
                                "\n",
                                "def distance_covariance(X, Y):\n",
                                "    \"\"\"Compute distance covariance between X and Y.\"\"\"\n",
                                "    A = double_centered_distance_matrix(X)\n",
                                "    B = double_centered_distance_matrix(Y)\n",
                                "    dcov = np.mean(A * B)\n",
                                "    return dcov\n",
                                "\n",
                                "def partial_distance_covariance(X, Y, Z):\n",
                                "    \"\"\"Compute partial distance covariance between X and Y given Z.\"\"\"\n",
                                "    dcov_XY = distance_covariance(X, Y)\n",
                                "    dcov_XZ = distance_covariance(X, Z)\n",
                                "    dcov_YZ = distance_covariance(Y, Z)\n",
                                "    dcov_ZZ = distance_covariance(Z, Z)\n",
                                "\n",
                                "    # Partial distance covariance formula\n",
                                "    pdcov = dcov_XY - (dcov_XZ * dcov_YZ) / dcov_ZZ\n",
                                "    return pdcov\n",
                                "\n",
                                "def partial_distance_covariance_dataset(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the partial distance covariance-based features for each\n",
                                "    variable, which are the partial distance covariance between that variable with X and Y,\n",
                                "    using each as the conditioning variable.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        pdcov_v_X_given_Y = partial_distance_covariance(dataset[variable].values, dataset[\"X\"].values, dataset[\"Y\"].values)\n",
                                "        pdcov_v_Y_given_X = partial_distance_covariance(dataset[variable].values, dataset[\"Y\"].values, dataset[\"X\"].values)\n",
                                "        pdcov_X_Y_given_v = partial_distance_covariance(dataset[\"X\"].values, dataset[\"Y\"].values, dataset[variable].values)\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"pdcov(v,X|Y)\": pdcov_v_X_given_Y,\n",
                                "            \"pdcov(v,Y|X)\": pdcov_v_Y_given_X,\n",
                                "            \"pdcov(X,Y|v)\": pdcov_X_Y_given_v,\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\", \"variable\", \"pdcov(v,X|Y)\", \"pdcov(v,Y|X)\", \"pdcov(X,Y|v)\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def Squared_term(dataset, variables):\n",
                                "    for var in variables:\n",
                                "        dataset[f'{var}_squared_term'] = dataset[var] ** 2\n",
                                "    return dataset\n",
                                "    \n",
                                "def Interaction_term(dataset, variables):\n",
                                "    for i in range(len(variables)):\n",
                                "        for j in range(i + 1, len(variables)):\n",
                                "            dataset[f'{variables[i]}_{variables[j]}'] = dataset[variables[i]] * dataset[variables[j]]\n",
                                "    return dataset\n",
                                "\n",
                                "def Cos_Sin_term(dataset, variables):\n",
                                "    for var in variables:\n",
                                "        dataset[f'{var}_cos_term'] = np.cos(dataset[var])\n",
                                "        dataset[f'{var}_sin_term'] = np.sin(dataset[var])\n",
                                "    return dataset\n",
                                "\n",
                                "def linear_regression_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    # model1: Fit X, v, v^2, v_i*v_j, v_i*X, cos(v), sin(v) ~ Y\n",
                                "    model1_features = [\"X\"] + variables\n",
                                "    d1 = Squared_term(dataset[model1_features], model1_features)\n",
                                "    d1 = Interaction_term(d1, model1_features)\n",
                                "    d1 = Cos_Sin_term(d1, variables)\n",
                                "    model1_features = d1.columns.tolist()\n",
                                "    scaler = StandardScaler()\n",
                                "    d1_scaled = scaler.fit_transform(d1)\n",
                                "    model1 = LinearRegression().fit(d1_scaled, dataset[[\"Y\"]])\n",
                                "    model1_coefs = model1.coef_[0].tolist()\n",
                                "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
                                "    \n",
                                "    # model2: Fit v, v^2, v_i*v_j, cos(v), sin(v) ~ X\n",
                                "    model2_features = variables\n",
                                "    d2 = Squared_term(dataset[model2_features], model2_features)\n",
                                "    d2 = Interaction_term(d2, model2_features)\n",
                                "    d2 = Cos_Sin_term(d2, model2_features)\n",
                                "    model2_features = d2.columns.tolist()\n",
                                "    scaler = StandardScaler()\n",
                                "    d2_scaled = scaler.fit_transform(d2)\n",
                                "    model2 = LinearRegression().fit(d2_scaled, dataset[[\"X\"]])\n",
                                "    model2_coefs = model2.coef_[0].tolist()\n",
                                "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
                                "    \n",
                                "    df = []\n",
                                "    for i, variable in enumerate(variables):\n",
                                "        # model3: Fit other v, X, Y ~ v\n",
                                "        model3_features = [\"X\", \"Y\"] + dataset.columns.drop([\"X\", \"Y\", variable]).tolist()\n",
                                "        d3 = Squared_term(dataset[model3_features], model3_features)\n",
                                "        d3 = Interaction_term(d3, model3_features)\n",
                                "        d3 = Cos_Sin_term(d3, model3_features)\n",
                                "        model3_features = d3.columns.tolist()\n",
                                "        scaler = StandardScaler()\n",
                                "        d3_scaled = scaler.fit_transform(d3)\n",
                                "        model3 = LinearRegression().fit(d3_scaled, dataset[[variable]])\n",
                                "        model3_coefs = model3.coef_[0].tolist()\n",
                                "        model3_dict = {name: coef for name, coef in zip(model3_features, model3_coefs)}\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"v~Y_coefficient\": model1_dict[variable],     # <--- model1\n",
                                "            \"v_squared~Y_coefficient\": model1_dict[f\"{variable}_squared_term\"],\n",
                                "            \"v*X~Y_coefficient\": model1_dict[f\"X_{variable}\"],\n",
                                "            \"v_cos~Y_coefficient\": model1_dict[f\"{variable}_cos_term\"],\n",
                                "            \"v_sin~Y_coefficient\": model1_dict[f\"{variable}_sin_term\"],\n",
                                "            \"v~X_coefficient\": model2_dict[variable],     # <--- model2\n",
                                "            \"v_squared~X_coefficient\": model2_dict[f\"{variable}_squared_term\"],\n",
                                "            \"v_cos~X_coefficient\": model2_dict[f\"{variable}_cos_term\"], \n",
                                "            \"v_sin~X_coefficient\": model2_dict[f\"{variable}_sin_term\"],  \n",
                                "            \"X~v_coefficient\": model3_dict[\"X\"],          # <--- model3\n",
                                "            \"X_squared~v_coefficient\": model3_dict[\"X_squared_term\"],\n",
                                "            \"X_cos~v_coefficient\": model3_dict[\"X_cos_term\"],\n",
                                "            \"X_sin~v_coefficient\": model3_dict[\"X_sin_term\"],\n",
                                "            \"Y~v_coefficient\": model3_dict[\"Y\"],\n",
                                "            \"Y_squared~v_coefficient\": model3_dict[\"Y_squared_term\"],\n",
                                "            \"Y_cos~v_coefficient\": model3_dict[\"Y_cos_term\"],\n",
                                "            \"Y_sin~v_coefficient\": model3_dict[\"Y_sin_term\"],\n",
                                "            \"X*Y~v_coefficient\": model3_dict[\"X_Y\"]\n",
                                "        })\n",
                                "        \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    df[\"X~Y_coefficient\"] = model1_dict[\"X\"]\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def piecewise_linear_regression_feature(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute piecewise linear regression features for each\n",
                                "    variable with X and Y, using a single breakpoint at the median.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # For v ~ X\n",
                                "        X = dataset[variable].values.reshape(-1, 1)\n",
                                "        y = dataset[\"X\"].values\n",
                                "        \n",
                                "        breakpoint = np.median(X)\n",
                                "        \n",
                                "        X_piecewise = np.column_stack((X, np.maximum(X - breakpoint, 0)))\n",
                                "        \n",
                                "        model = LinearRegression().fit(X_piecewise, y)\n",
                                "        \n",
                                "        # For v ~ Y\n",
                                "        y_Y = dataset[\"Y\"].values\n",
                                "        model_Y = LinearRegression().fit(X_piecewise, y_Y)\n",
                                "        \n",
                                "        # For X ~ v\n",
                                "        X_X = dataset[\"X\"].values.reshape(-1, 1)\n",
                                "        y_v = dataset[variable].values\n",
                                "        breakpoint_X = np.median(X_X)\n",
                                "        X_piecewise_X = np.column_stack((X_X, np.maximum(X_X - breakpoint_X, 0)))\n",
                                "        model_X_v = LinearRegression().fit(X_piecewise_X, y_v)\n",
                                "        \n",
                                "        # For Y ~ v\n",
                                "        X_Y = dataset[\"Y\"].values.reshape(-1, 1)\n",
                                "        breakpoint_Y = np.median(X_Y)\n",
                                "        X_piecewise_Y = np.column_stack((X_Y, np.maximum(X_Y - breakpoint_Y, 0)))\n",
                                "        model_Y_v = LinearRegression().fit(X_piecewise_Y, y_v)\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"v~X_piecewise_coef1\": model.coef_[0],\n",
                                "            \"v~X_piecewise_coef2\": model.coef_[1],\n",
                                "            \"v~Y_piecewise_coef1\": model_Y.coef_[0],\n",
                                "            \"v~Y_piecewise_coef2\": model_Y.coef_[1],\n",
                                "            \"X~v_piecewise_coef1\": model_X_v.coef_[0],\n",
                                "            \"X~v_piecewise_coef2\": model_X_v.coef_[1],\n",
                                "            \"Y~v_piecewise_coef1\": model_Y_v.coef_[0],\n",
                                "            \"Y~v_piecewise_coef2\": model_Y_v.coef_[1],\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def ridge_regression_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "    # 使用GridSearchCV来选择最佳的alpha值\n",
                                "    param_grid = {'alpha': np.logspace(-6, 6, 13)}\n",
                                "    \n",
                                "    # model1: Fit X, v ~ Y\n",
                                "    model1_features = [\"X\"] + variables\n",
                                "    d1 = Squared_term(dataset[model1_features], model1_features)\n",
                                "    # d1 = Interaction_term(d1, model1_features) # 掉分\n",
                                "    model1_features = d1.columns.tolist()\n",
                                "    scaler1 = StandardScaler()\n",
                                "    d1_scaled = scaler1.fit_transform(d1)\n",
                                "    model1 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
                                "    model1.fit(d1_scaled, dataset[\"Y\"])\n",
                                "    model1_coefs = model1.best_estimator_.coef_.tolist()\n",
                                "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
                                "    \n",
                                "    # model2: Fit v ~ X\n",
                                "    model2_features = variables\n",
                                "    d2 = Squared_term(dataset[model2_features], model2_features)\n",
                                "    d2 = Interaction_term(d2, model2_features)\n",
                                "    # d2 = Cos_Sin_term(d2, model2_features)  # 掉分\n",
                                "    model2_features = d2.columns.tolist()\n",
                                "    scaler2 = StandardScaler()\n",
                                "    d2_scaled = scaler2.fit_transform(d2)\n",
                                "    model2 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
                                "    model2.fit(d2_scaled, dataset[\"X\"])\n",
                                "    model2_coefs = model2.best_estimator_.coef_.tolist()\n",
                                "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
                                "\n",
                                "    # # 获取最优的 alpha 值\n",
                                "    # best_alpha_model1 = model1.best_params_['alpha']   # 0.4730-0.4727\n",
                                "    # best_alpha_model2 = model2.best_params_['alpha']\n",
                                "    \n",
                                "    df = []\n",
                                "    for i, variable in enumerate(variables):\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"v~Y_ridge_coefficient\": model1_dict[variable],\n",
                                "            # \"v_squared~Y_ridge_coefficient\": model1_dict[f\"{variable}_squared_term\"],  # 掉分\n",
                                "            \"v~X_ridge_coefficient\": model2_dict[variable],\n",
                                "            # \"v_squared~X_ridge_coefficient\": model2_dict[f\"{variable}_squared_term\"],  # 掉分\n",
                                "            # \"v_cos~X_ridge_coefficient\": model2_dict[f\"{variable}_cos_term\"],  # 掉分\n",
                                "            # \"v_sin~X_ridge_coefficient\": model2_dict[f\"{variable}_sin_term\"],  # 掉分\n",
                                "            # \"v~Y_ridge_alpha\": best_alpha_model1,\n",
                                "            # \"v~X_ridge_alpha\": best_alpha_model2\n",
                                "        })\n",
                                "        \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    df[\"X~Y_ridge_coefficient\"] = model1_dict[\"X\"]\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def conditional_independence_tests(dataset):  # 太慢了，得三个小时\n",
                                "    \"\"\"\n",
                                "    A mixed-data residualization based conditional independence test[1].\n",
                                "    Uses XGBoost estimator to compute LS residuals[2], and then does an association test (Pillai’s Trace) on the residuals.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # v-X\n",
                                "        coef1, p_value1 = CITests.ci_pillai(X=variable, Y=\"X\", Z=dataset.columns.drop([\"X\", variable]).tolist(), data=dataset, boolean=False)\n",
                                "        # v-Y\n",
                                "        coef2, p_value2 = CITests.ci_pillai(X=variable, Y=\"Y\", Z=dataset.columns.drop([\"Y\", variable]).tolist(), data=dataset, boolean=False)\n",
                                "        # X-v\n",
                                "        coef3, p_value3 = CITests.ci_pillai(X=\"X\", Y=variable, Z=dataset.columns.drop([\"X\", variable]).tolist(), data=dataset, boolean=False)\n",
                                "        # Y-v\n",
                                "        coef4, p_value4 = CITests.ci_pillai(X=\"Y\", Y=variable, Z=dataset.columns.drop([\"Y\", variable]).tolist(), data=dataset, boolean=False)\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"v~X_ci_pillai_coef\": coef1,\n",
                                "            \"v~X_ci_pillai_p_value\": p_value1,\n",
                                "            \"v~Y_ci_pillai_coef\": coef2,\n",
                                "            \"v~Y_ci_pillai_p_value\": p_value2,\n",
                                "            \"X~v_ci_pillai_coef\": coef3,\n",
                                "            \"X~v_ci_pillai_p_value\": p_value3,\n",
                                "            \"Y~v_ci_pillai_coef\": coef4,\n",
                                "            \"Y~v_ci_pillai_p_value\": p_value4\n",
                                "        })\n",
                                "    \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def conditional_mutual_information(dataset):\n",
                                "    \"\"\"\n",
                                "    Calculate conditional mutual information for each variable with X and Y.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "    \n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # Calculate conditional MI(v, X | Y)\n",
                                "        mi_vx_given_y = mutual_info_regression(dataset[[variable, \"Y\"]], dataset[\"X\"], discrete_features=False)[0] - \\\n",
                                "                        mutual_info_regression(dataset[[\"Y\"]], dataset[\"X\"], discrete_features=False)[0]\n",
                                "        \n",
                                "        # Calculate conditional MI(v, Y | X)\n",
                                "        mi_vy_given_x = mutual_info_regression(dataset[[variable, \"X\"]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
                                "                        mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
                                "        \n",
                                "        # Calculate conditional MI(X, Y | v)\n",
                                "        mi_xy_given_v = mutual_info_regression(dataset[[\"X\", variable]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
                                "                        mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0]\n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"conditional_MI(v,X|Y)\": mi_vx_given_y,\n",
                                "            \"conditional_MI(v,Y|X)\": mi_vy_given_x,\n",
                                "            \"conditional_MI(X,Y|v)\": mi_xy_given_v,\n",
                                "        })\n",
                                "    \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "    \n",
                                "    return df\n",
                                "\n",
                                "def partial_correlation(dataset):\n",
                                "    \"\"\"\n",
                                "    Compute partial correlation coefficients for each variable with X and Y,\n",
                                "    controlling for the other variable, as well as the partial correlation\n",
                                "    between X and Y controlling for each variable.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "    \n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # Compute partial correlations\n",
                                "        pcorr_vX_Y = pg.partial_corr(data=dataset, x=variable, y='X', covar='Y')['r'].iloc[0]\n",
                                "        pcorr_vY_X = pg.partial_corr(data=dataset, x=variable, y='Y', covar='X')['r'].iloc[0]\n",
                                "        pcorr_XY_v = pg.partial_corr(data=dataset, x='X', y='Y', covar=variable)['r'].iloc[0]\n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"partial_corr(v,X|Y)\": pcorr_vX_Y,\n",
                                "            \"partial_corr(v,Y|X)\": pcorr_vY_X,\n",
                                "            \"partial_corr(X,Y|v)\": pcorr_XY_v,\n",
                                "        })\n",
                                "    \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "    \n",
                                "    return df\n",
                                "\n",
                                "\"\"\"因果发现算法的工具函数\"\"\"\n",
                                "def handle_multicollinearity(data, epsilon=1e-7, corr_threshold=0.99):\n",
                                "    \"\"\"\n",
                                "    检查数据中的多重共线性，并在需要时向存在多重共线性的列添加随机扰动。\n",
                                "    \n",
                                "    参数:\n",
                                "    - data (np.ndarray): 输入的数据矩阵，形状为 (样本数, 特征数)。\n",
                                "    - epsilon (float): 添加的随机扰动的尺度，默认值为1e-10。\n",
                                "    - corr_threshold (float): 判断高相关性的阈值，默认值为0.95。\n",
                                "    \n",
                                "    返回:\n",
                                "    - data (np.ndarray): 处理后的数据矩阵。\n",
                                "    \"\"\"\n",
                                "    # 计算相关系数矩阵\n",
                                "    corr_matrix = np.corrcoef(data, rowvar=False)\n",
                                "    n_cols = corr_matrix.shape[0]\n",
                                "\n",
                                "    # 使用集合存储所有涉及多重共线性的列索引，避免重复\n",
                                "    high_corr_indices = set()\n",
                                "    \n",
                                "    # 遍历相关系数矩阵的上三角部分，寻找高相关的列对\n",
                                "    for i in range(n_cols):\n",
                                "        for j in range(i+1, n_cols):\n",
                                "            if np.abs(corr_matrix[i, j]) > corr_threshold:\n",
                                "                high_corr_indices.add(i)\n",
                                "                high_corr_indices.add(j)\n",
                                "    \n",
                                "    if high_corr_indices:\n",
                                "        sorted_indices = sorted(high_corr_indices)\n",
                                "        # print(f\"检测到多重共线性，涉及的列索引: {sorted_indices}。正在添加随机扰动...\")\n",
                                "        \n",
                                "        # 生成与高相关性列对应的随机扰动\n",
                                "        noise = np.random.normal(0, epsilon, (data.shape[0], len(sorted_indices)))\n",
                                "        \n",
                                "        # 将扰动添加到相应的列\n",
                                "        data[:, sorted_indices] += noise\n",
                                "    \n",
                                "    return data\n",
                                "\n",
                                "def convert_bidirectional_to_dag(bidirectional_adj: pd.DataFrame) -> pd.DataFrame:\n",
                                "    \"\"\"\n",
                                "    将双向编码的邻接矩阵转换为单向编码的邻接矩阵（DAG表示）。\n",
                                "    \n",
                                "    在双向编码的邻接矩阵中：\n",
                                "    - [j, i] = 1 且 [i, j] = -1 表示 i → j\n",
                                "    - [j, i] = 1 且 [i, j] = 1 表示 i ↔ j（互为因果）\n",
                                "    - 其他情况 [i, j] = 0 表示无边\n",
                                "    \n",
                                "    转换后，单向编码的邻接矩阵将：\n",
                                "    - [i, j] = 1 表示 i → j\n",
                                "    - [i, j] = 1 且 [j, i] = 1 表示 i ↔ j\n",
                                "    - [i, j] = 0 表示无边\n",
                                "    \n",
                                "    参数:\n",
                                "    - bidirectional_adj (pd.DataFrame): 双向编码的邻接矩阵，使用 1 和 -1 表示有向边。\n",
                                "    \n",
                                "    返回:\n",
                                "    - pd.DataFrame: 单向编码的邻接矩阵（DAG表示）。\n",
                                "    \"\"\"\n",
                                "    # 确保输入是一个方阵\n",
                                "    if bidirectional_adj.shape[0] != bidirectional_adj.shape[1]:\n",
                                "        raise ValueError(\"输入的邻接矩阵必须是方阵（行数等于列数）。\")\n",
                                "    \n",
                                "    # 确保行列索引一致\n",
                                "    if not bidirectional_adj.index.equals(bidirectional_adj.columns):\n",
                                "        raise ValueError(\"邻接矩阵的行索引和列索引必须相同。\")\n",
                                "    \n",
                                "    # 初始化一个全零的邻接矩阵\n",
                                "    single_direction_adj = pd.DataFrame(0, index=bidirectional_adj.index, columns=bidirectional_adj.columns)\n",
                                "    \n",
                                "    # 遍历每一对变量，确定有向边\n",
                                "    for source in bidirectional_adj.columns:\n",
                                "        for target in bidirectional_adj.index:\n",
                                "            if source == target:\n",
                                "                # 根据DAG定义，通常不允许自环，因此设置为0\n",
                                "                single_direction_adj.at[source, target] = 0\n",
                                "                continue\n",
                                "            \n",
                                "            # 检查双向编码的邻接矩阵来确定有向边\n",
                                "            if bidirectional_adj.at[target, source] == 1:\n",
                                "                if bidirectional_adj.at[source, target] == -1:\n",
                                "                    # 表示 source → target\n",
                                "                    single_direction_adj.at[source, target] = 1\n",
                                "                elif bidirectional_adj.at[source, target] == 1:\n",
                                "                    # 表示 source ↔ target（互为因果）\n",
                                "                    single_direction_adj.at[source, target] = 1\n",
                                "                    single_direction_adj.at[target, source] = 1\n",
                                "                # 如果 [source, target] == 0 或其他情况，不设置边\n",
                                "    return single_direction_adj\n",
                                "\n",
                                "def convert_dag_with_strength_to_dag(adj: pd.DataFrame) -> pd.DataFrame:\n",
                                "    \"\"\"\n",
                                "    将表示因果影响强度的邻接矩阵数据框转换为二元邻接矩阵数据框。\n",
                                "    无论存在正向影响还是负向影响，都将其转换为1；不存在影响则为0。\n",
                                "    \n",
                                "    参数:\n",
                                "    - adj (pd.DataFrame): 原始的因果影响强度邻接矩阵，元素可以是正数、负数或0。\n",
                                "    \n",
                                "    返回:\n",
                                "    - pd.DataFrame: 二元邻接矩阵，元素为0或1。\n",
                                "    \"\"\"\n",
                                "    # 确保输入是一个方阵\n",
                                "    if adj.shape[0] != adj.shape[1]:\n",
                                "        raise ValueError(\"输入的邻接矩阵必须是方阵（行数等于列数）。\")\n",
                                "    \n",
                                "    # 确保行列索引一致\n",
                                "    if not adj.index.equals(adj.columns):\n",
                                "        raise ValueError(\"邻接矩阵的行索引和列索引必须相同。\")\n",
                                "    \n",
                                "    # 将所有非零元素转换为1，零元素保持为0\n",
                                "    return adj.ne(0).astype(int)\n",
                                "\n",
                                "\"\"\"因果发现算法\"\"\"\n",
                                "def PC_estimate(dataset, alpha=0.05, indep_test='fisherz', kernel=None, stable=True, \n",
                                "               uc_rule=0, uc_priority=2, verbose=False, show_progress=False):\n",
                                "    \"\"\"\n",
                                "    使用PC算法进行因果发现，并应用先验知识。\n",
                                "\n",
                                "    参数:\n",
                                "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
                                "    - alpha (float): 显著性水平，默认值为0.05。\n",
                                "    - indep_test (str): 独立性检验方法，默认值为'fisherz'。\n",
                                "    - kernel (str): 核函数类型，默认值为'linear'。\n",
                                "    - stable (bool): 是否运行稳定的骨架发现，默认值为True。\n",
                                "    - uc_rule (int): 未屏蔽碰撞点的定向规则，默认值为0。\n",
                                "    - uc_priority (int): 解决未屏蔽碰撞点冲突的优先规则，默认值为2。\n",
                                "    - verbose (bool): 是否打印详细输出，默认值为False。\n",
                                "    - show_progress (bool): 是否显示算法进度，默认值为False。\n",
                                "\n",
                                "    返回:\n",
                                "    - adj_df (pd.DataFrame): 因果图的邻接矩阵，格式为pivot_table。\n",
                                "    \"\"\"\n",
                                "    \n",
                                "    # 1. 将数据框转换为numpy.ndarray\n",
                                "    data = dataset.values\n",
                                "\n",
                                "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
                                "    data = handle_multicollinearity(data)\n",
                                "    \n",
                                "    # 2. 定义先验知识：'X' → 'Y'\n",
                                "    # 创建GraphNode对象\n",
                                "    try:\n",
                                "        node_X = GraphNode('X')\n",
                                "        node_Y = GraphNode('Y')\n",
                                "    except Exception as e:\n",
                                "        raise ValueError(\"确保数据框中包含名为'X'和'Y'的列。\") from e\n",
                                "    \n",
                                "    # 初始化BackgroundKnowledge对象并添加先验知识\n",
                                "    bk = BackgroundKnowledge().add_required_by_node(node_X, node_Y)\n",
                                "\n",
                                "    # 3. 配置核参数\n",
                                "    if indep_test == 'kci':\n",
                                "        if kernel is None:\n",
                                "            kernel = 'linear'\n",
                                "        if kernel == 'linear':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Linear', \n",
                                "                'kernelY': 'Linear', \n",
                                "                'kernelZ': 'Linear', \n",
                                "                'approx': True,           # 使用伽玛近似\n",
                                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        elif kernel == 'polynomial':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Polynomial', \n",
                                "                'kernelY': 'Polynomial', \n",
                                "                'kernelZ': 'Polynomial', \n",
                                "                'polyd': 3,               # 多项式次数设置为3\n",
                                "                'approx': True,           # 使用伽玛近似\n",
                                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        elif kernel == 'gaussian':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Gaussian', \n",
                                "                'kernelY': 'Gaussian', \n",
                                "                'kernelZ': 'Gaussian', \n",
                                "                'est_width': 'empirical', # 使用经验宽度\n",
                                "                'approx': True,           # 使用伽玛近似\n",
                                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        elif kernel == 'mix':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Polynomial', \n",
                                "                'kernelY': 'Polynomial', \n",
                                "                'kernelZ': 'Gaussian',     # Z使用高斯核\n",
                                "                'polyd': 3,                # 多项式次数设置为3\n",
                                "                'est_width': 'median',     # Z的高斯核带宽使用中位数技巧\n",
                                "                'approx': True,            # 使用伽玛近似\n",
                                "                'nullss': 1000,           # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        else:\n",
                                "            raise ValueError(f'Unknown kernel: {kernel}')\n",
                                "    else:\n",
                                "        kernel_kwargs = {}\n",
                                "\n",
                                "    # 4. 运行PC算法，传入先验知识\n",
                                "    cg = pc(data, alpha=alpha, indep_test=indep_test, stable=stable, uc_rule=uc_rule, uc_priority=uc_priority, \n",
                                "            background_knowledge=bk, verbose=verbose, show_progress=show_progress, **kernel_kwargs)\n",
                                "    \n",
                                "    # 5. 提取邻接矩阵\n",
                                "    adj_matrix = cg.G.graph\n",
                                "    # 6. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
                                "    adj_df = pd.DataFrame(adj_matrix, index=dataset.columns, columns=dataset.columns)\n",
                                "    \n",
                                "    return adj_df\n",
                                "\n",
                                "def PC_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    estimate_adj_df_bidirectional = PC_estimate(dataset)  # 双向的估计因果图\n",
                                "    estimate_adj_df_dag = convert_bidirectional_to_dag(estimate_adj_df_bidirectional)  # 将双向图转换为有向图\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # 检查变量与'X'和'Y'之间的边\n",
                                "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
                                "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
                                "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
                                "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
                                "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"PC(v,X)\": v_to_X,\n",
                                "            \"PC(X,v)\": X_to_v,\n",
                                "            \"PC(v,Y)\": v_to_Y,\n",
                                "            \"PC(Y,v)\": Y_to_v,\n",
                                "            \"PC(X,Y)\": X_to_Y\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def ExactSearch_estimate(dataset, search_method='astar', use_path_extension=True, \n",
                                "                        use_k_cycle_heuristic=False, k=3, max_parents=None):\n",
                                "    \"\"\"\n",
                                "    使用Exact Search算法进行因果发现。\n",
                                "    \n",
                                "    参数:\n",
                                "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
                                "    - search_method (str): Exact Search方法，'astar'或'dp'。默认值为'astar'。\n",
                                "    - use_path_extension (bool): 是否使用路径扩展。默认值为True。\n",
                                "    - use_k_cycle_heuristic (bool): 是否使用k-cycle冲突启发式。仅适用于'astar'方法。默认值为False。\n",
                                "    - k (int): k-cycle启发式的参数。默认值为3。\n",
                                "    - max_parents (int or None): 节点的最大父节点数量。默认值为None。\n",
                                "    \n",
                                "    返回:\n",
                                "    - adj_df (pd.DataFrame): 因果图的邻接矩阵，格式为DataFrame。\n",
                                "    \"\"\"\n",
                                "    \n",
                                "    # 1. 将数据框转换为numpy.ndarray\n",
                                "    data = dataset.values\n",
                                "\n",
                                "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
                                "    data = handle_multicollinearity(data)\n",
                                "\n",
                                "    # 2. 定义X→Y的超级图\n",
                                "    d = dataset.shape[1]   \n",
                                "    super_graph = np.ones((d, d), dtype=int)  # 初始化为全1，表示所有边默认允许\n",
                                "    columns = dataset.columns.tolist()\n",
                                "    x_idx = columns.index('X')\n",
                                "    y_idx = columns.index('Y')\n",
                                "    super_graph[y_idx][x_idx] = 0     # 禁止'Y'→'X'\n",
                                "    np.fill_diagonal(super_graph, 0)  # 禁止自环：确保对角线为0\n",
                                "    \n",
                                "    # 3. 运行Exact Search算法，使用指定的参数\n",
                                "    dag_est, search_stats = bic_exact_search(X=data, super_graph=super_graph, search_method=search_method,\n",
                                "        use_path_extension=use_path_extension, use_k_cycle_heuristic=use_k_cycle_heuristic,\n",
                                "        k=k, verbose=False, max_parents=max_parents)\n",
                                "    \n",
                                "    # 4. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
                                "    adj_df = pd.DataFrame(dag_est, index=dataset.columns, columns=dataset.columns)\n",
                                "    \n",
                                "    return adj_df\n",
                                "\n",
                                "def ExactSearch_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    estimate_adj_df = ExactSearch_estimate(dataset)\n",
                                "    estimate_adj_df_dag = estimate_adj_df.astype(int)  # 将双向图转换为有向图\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # 检查变量与'X'和'Y'之间的边\n",
                                "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
                                "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
                                "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
                                "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
                                "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
                                "\n",
                                "        # v_to_others = sum(estimate_adj_df_dag.loc[variable, variables])\n",
                                "        # others_to_v = sum(estimate_adj_df_dag.loc[variables, variable])\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"ExactSearch(v,X)\": v_to_X,\n",
                                "            \"ExactSearch(X,v)\": X_to_v,\n",
                                "            \"ExactSearch(v,Y)\": v_to_Y,\n",
                                "            \"ExactSearch(Y,v)\": Y_to_v,\n",
                                "            \"ExactSearch(X,Y)\": X_to_Y\n",
                                "            # \"ExactSearch(v,others)\": v_to_others,\n",
                                "            # \"ExactSearch(others,v)\": others_to_v\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def FCI_estimate(dataset, alpha=0.05, indep_test='fisherz', kernel=None, \n",
                                "               depth=-1, max_path_length=-1, verbose=False, show_progress=False):\n",
                                "    \"\"\"\n",
                                "    使用FCI算法进行因果发现，并应用先验知识。\n",
                                "\n",
                                "    参数:\n",
                                "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
                                "    - alpha (float): 显著性水平，默认值为0.05。\n",
                                "    - indep_test (str): 独立性检验方法，默认值为'fisherz'。\n",
                                "    - kernel (str): 核函数类型，默认值为'linear'。\n",
                                "    - verbose (bool): 是否打印详细输出，默认值为False。\n",
                                "    - show_progress (bool): 是否显示算法进度，默认值为False。\n",
                                "\n",
                                "    返回:\n",
                                "    - adj_df (pd.DataFrame): 因果图的邻接矩阵，格式为pivot_table。\n",
                                "    \"\"\"\n",
                                "    \n",
                                "    # 1. 将数据框转换为numpy.ndarray\n",
                                "    data = dataset.values\n",
                                "\n",
                                "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
                                "    data = handle_multicollinearity(data)\n",
                                "    \n",
                                "    # 2. 定义先验知识：'X' → 'Y'\n",
                                "    # 创建GraphNode对象\n",
                                "    try:\n",
                                "        node_X = GraphNode('X')\n",
                                "        node_Y = GraphNode('Y')\n",
                                "    except Exception as e:\n",
                                "        raise ValueError(\"确保数据框中包含名为'X'和'Y'的列。\") from e\n",
                                "    \n",
                                "    # 初始化BackgroundKnowledge对象并添加先验知识\n",
                                "    bk = BackgroundKnowledge().add_required_by_node(node_X, node_Y)\n",
                                "\n",
                                "    # 3. 配置核参数\n",
                                "    if indep_test == 'kci':\n",
                                "        if kernel is None:\n",
                                "            kernel = 'linear'\n",
                                "        if kernel == 'linear':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Linear', \n",
                                "                'kernelY': 'Linear', \n",
                                "                'kernelZ': 'Linear', \n",
                                "                'approx': True,           # 使用伽玛近似\n",
                                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        elif kernel == 'polynomial':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Polynomial', \n",
                                "                'kernelY': 'Polynomial', \n",
                                "                'kernelZ': 'Polynomial', \n",
                                "                'polyd': 3,               # 多项式次数设置为3\n",
                                "                'approx': True,           # 使用伽玛近似\n",
                                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        elif kernel == 'gaussian':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Gaussian', \n",
                                "                'kernelY': 'Gaussian', \n",
                                "                'kernelZ': 'Gaussian', \n",
                                "                'est_width': 'empirical', # 使用经验宽度\n",
                                "                'approx': True,           # 使用伽玛近似\n",
                                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        elif kernel == 'mix':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Polynomial', \n",
                                "                'kernelY': 'Polynomial', \n",
                                "                'kernelZ': 'Gaussian',     # Z使用高斯核\n",
                                "                'polyd': 3,                # 多项式次数设置为3\n",
                                "                'est_width': 'median',     # Z的高斯核带宽使用中位数技巧\n",
                                "                'approx': True,            # 使用伽玛近似\n",
                                "                'nullss': 1000,           # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        else:\n",
                                "            raise ValueError(f'Unknown kernel: {kernel}')\n",
                                "    else:\n",
                                "        kernel_kwargs = {}\n",
                                "\n",
                                "    # 4. 运行FCI算法，传入先验知识\n",
                                "    try:\n",
                                "        g, edges = fci(data, \n",
                                "                alpha=alpha, \n",
                                "                independence_test_method=indep_test, \n",
                                "                depth=depth,\n",
                                "                max_path_length=max_path_length,\n",
                                "                background_knowledge=bk, \n",
                                "                verbose=verbose, \n",
                                "                show_progress=show_progress,\n",
                                "                **kernel_kwargs\n",
                                "        )\n",
                                "\n",
                                "        # 5. 提取邻接矩阵\n",
                                "        adj_matrix = g.graph\n",
                                "    except Exception as e:\n",
                                "        adj_matrix = np.zeros((data.shape[1], data.shape[1]))\n",
                                "    \n",
                                "    # 6. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
                                "    adj_df = pd.DataFrame(adj_matrix, index=dataset.columns, columns=dataset.columns)\n",
                                "    \n",
                                "    return adj_df\n",
                                "\n",
                                "def FCI_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    estimate_adj_df_bidirectional = FCI_estimate(dataset)  # PAG\n",
                                "    estimate_adj_df_dag = estimate_adj_df_bidirectional.astype('int')\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # 检查变量与'X'和'Y'之间的边\n",
                                "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
                                "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
                                "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
                                "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
                                "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"FCI(v,X)\": v_to_X,\n",
                                "            \"FCI(X,v)\": X_to_v,\n",
                                "            \"FCI(v,Y)\": v_to_Y,\n",
                                "            \"FCI(Y,v)\": Y_to_v,\n",
                                "            \"FCI(X,Y)\": X_to_Y\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # # One-hot encode the edge types (-1, 0, 1, 2)\n",
                                "    # one_hot_columns = [\"FCI(v,X)\", \"FCI(X,v)\", \"FCI(v,Y)\", \"FCI(Y,v)\"]\n",
                                "    \n",
                                "    # for col in one_hot_columns:\n",
                                "    #     one_hot = pd.get_dummies(df[col], prefix=col)\n",
                                "    #     df = pd.concat([df, one_hot], axis=1)\n",
                                "\n",
                                "    # # Remove original edge type columns after one-hot encoding\n",
                                "    # df = df.drop(columns=one_hot_columns)\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\", \"variable\"] + [col for col in df.columns if col not in [\"dataset\", \"variable\"]]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"DML\"\"\"\n",
                                "def DML_estimate(T_, Y_, X_, data):\n",
                                "    # 设置处理变量、结果变量和控制变量\n",
                                "    T = data[T_].values\n",
                                "    Y = data[Y_].values\n",
                                "    X = data[X_].values\n",
                                "\n",
                                "    # 定义 LassoCV 作为第一阶段模型\n",
                                "    model_t = LassoCV(random_state=42, n_jobs=None)\n",
                                "    model_y = LassoCV(random_state=42, n_jobs=None)\n",
                                "\n",
                                "    # # 定义 \"forest\" 作为第二阶段模型\n",
                                "    # model_t = \"forest\"\n",
                                "    # model_y = \"forest\"\n",
                                "\n",
                                "    # 初始化 CausalForestDML 使用自定义的估计器\n",
                                "    model = CausalForestDML(model_t=model_t, model_y=model_y,\n",
                                "        cv=4, n_estimators=36, n_jobs=None, random_state=42, inference=True)\n",
                                "    \n",
                                "    # # 初始化 LinearDML 使用自定义的估计器\n",
                                "    # model = LinearDML(model_t=model_t, model_y=model_y, \n",
                                "    #     cv=4, n_jobs=None, random_state=42, inference=True)\n",
                                "    \n",
                                "    model.fit(Y, T, X=X)\n",
                                "\n",
                                "    # 平均边际处理效应的推断结果\n",
                                "    amte_inference = model.ate_inference(X=X)\n",
                                "\n",
                                "    return amte_inference\n",
                                "\n",
                                "def DML_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # 判断v-X的因果效应，设置variables中的其他v和Y为控制变量\n",
                                "        amte_inference1 = DML_estimate(variable, \"X\", [\"Y\"] + list(variables.drop(variable)), dataset)\n",
                                "        # 判断v-Y的因果效应，设置variables中的其他v和X为控制变量\n",
                                "        amte_inference2 = DML_estimate(variable, \"Y\", [\"X\"] + list(variables.drop(variable)), dataset)\n",
                                "        # 判断X-v的因果效应，设置variables中的其他v和Y为控制变量\n",
                                "        amte_inference3 = DML_estimate(\"X\", variable, [\"Y\"] + list(variables.drop(variable)), dataset)\n",
                                "        # 判断Y-v的因果效应，设置variables中的其他v和X为控制变量\n",
                                "        amte_inference4 = DML_estimate(\"Y\", variable, [\"X\"] + list(variables.drop(variable)), dataset)\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"v~X_DML_AMTE\": amte_inference1.mean_point,  \n",
                                "            \"v~X_DML_AMTE_zstat\": amte_inference1.zstat(),\n",
                                "            \"v~X_DML_AMTE_pvalue\": amte_inference1.pvalue(),\n",
                                "            \"v~X_DML_std_point\": amte_inference1.std_point,\n",
                                "            \"v~X_DML_stderr_point\": amte_inference1.stderr_point,\n",
                                "\n",
                                "            \"v~Y_DML_AMTE\": amte_inference2.mean_point,\n",
                                "            \"v~Y_DML_AMTE_zstat\": amte_inference2.zstat(),\n",
                                "            \"v~Y_DML_AMTE_pvalue\": amte_inference2.pvalue(),\n",
                                "            \"v~Y_DML_std_point\": amte_inference2.std_point,\n",
                                "            \"v~Y_DML_stderr_point\": amte_inference2.stderr_point,\n",
                                "\n",
                                "            \"X~v_DML_AMTE\": amte_inference3.mean_point,\n",
                                "            \"X~v_DML_AMTE_zstat\": amte_inference3.zstat(),\n",
                                "            \"X~v_DML_AMTE_pvalue\": amte_inference3.pvalue(),\n",
                                "            \"X~v_DML_std_point\": amte_inference3.std_point,\n",
                                "            \"X~v_DML_stderr_point\": amte_inference3.stderr_point,\n",
                                "\n",
                                "            \"Y~v_DML_AMTE\": amte_inference4.mean_point,\n",
                                "            \"Y~v_DML_AMTE_zstat\": amte_inference4.zstat(),\n",
                                "            \"Y~v_DML_AMTE_pvalue\": amte_inference4.pvalue(),\n",
                                "            \"Y~v_DML_std_point\": amte_inference4.std_point,\n",
                                "            \"Y~v_DML_stderr_point\": amte_inference4.stderr_point,\n",
                                "        })\n",
                                "    \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def copula_entropy(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the Copula entropy-based features for each\n",
                                "    variable, which are the Copula entropy between that variable with X and Y,\n",
                                "    as well as summary statistics of all pairs of Copula entropies.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # Compute copula entropy between v and X\n",
                                "        ce_v_X = copent(dataset[[variable, \"X\"]].values)\n",
                                "        \n",
                                "        # Compute copula entropy between v and Y\n",
                                "        ce_v_Y = copent(dataset[[variable, \"Y\"]].values)\n",
                                "        \n",
                                "        # # Compute transfer entropy from v to X and X to v\n",
                                "        # te_v_X = transent(dataset[variable].values, dataset[\"X\"].values)\n",
                                "        # te_X_v = transent(dataset[\"X\"].values, dataset[variable].values)\n",
                                "        \n",
                                "        # # Compute transfer entropy from v to Y and Y to v\n",
                                "        # te_v_Y = transent(dataset[variable].values, dataset[\"Y\"].values)\n",
                                "        # te_Y_v = transent(dataset[\"Y\"].values, dataset[variable].values)\n",
                                "        \n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"copula_entropy(v,X)\": ce_v_X,\n",
                                "            \"copula_entropy(v,Y)\": ce_v_Y,\n",
                                "            # \"transfer_entropy(v->X)\": te_v_X,\n",
                                "            # \"transfer_entropy(X->v)\": te_X_v,\n",
                                "            # \"transfer_entropy(v->Y)\": te_v_Y,\n",
                                "            # \"transfer_entropy(Y->v)\": te_Y_v,\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Compute copula entropy between X and Y\n",
                                "    df[\"copula_entropy(X,Y)\"] = copent(dataset[[\"X\", \"Y\"]].values)\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def ANM_feature(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the ANM-based features for each\n",
                                "    variable, which are the p-values of ANM test between that variable with X and Y.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "    anm = ANM()\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # Compute ANM p-values between v and X\n",
                                "        p_value_v_X, p_value_X_v = anm.cause_or_effect(dataset[variable].values.reshape(-1, 1), \n",
                                "                                                       dataset[\"X\"].values.reshape(-1, 1))\n",
                                "        \n",
                                "        # Compute ANM p-values between v and Y\n",
                                "        p_value_v_Y, p_value_Y_v = anm.cause_or_effect(dataset[variable].values.reshape(-1, 1), \n",
                                "                                                       dataset[\"Y\"].values.reshape(-1, 1))\n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"ANM_p_value(v->X)\": p_value_v_X,\n",
                                "            \"ANM_p_value(X->v)\": p_value_X_v,\n",
                                "            \"ANM_p_value(v->Y)\": p_value_v_Y,\n",
                                "            \"ANM_p_value(Y->v)\": p_value_Y_v,\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Compute ANM p-values between X and Y\n",
                                "    p_value_X_Y, p_value_Y_X = anm.cause_or_effect(dataset[\"X\"].values.reshape(-1, 1), \n",
                                "                                                   dataset[\"Y\"].values.reshape(-1, 1))\n",
                                "    df[\"ANM_p_value(X->Y)\"] = p_value_X_Y\n",
                                "    df[\"ANM_p_value(Y->X)\"] = p_value_Y_X\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "    \n",
                                "    return df\n"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 10,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:31:25.402707Z",
                                        "start_time": "2024-09-18T09:31:25.388081Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "def label(adjacency_matrix):\n",
                                "    \"\"\"\n",
                                "    Given a graph as adjacency_matrix, create the class labels of each variable.\n",
                                "    \"\"\"\n",
                                "\n",
                                "    adjacency_graph, adjacency_label = create_graph_label()\n",
                                "    labels = get_labels(adjacency_matrix, adjacency_label)\n",
                                "    variables = adjacency_matrix.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = pd.DataFrame({\n",
                                "        \"variable\": variables,\n",
                                "        \"label\": [labels[variable] for variable in variables],\n",
                                "    })\n",
                                "    df[\"dataset\"] = adjacency_matrix.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 11,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:31:26.636102Z",
                                        "start_time": "2024-09-18T09:31:26.629666Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "def create_some_columns(names_datasets, function):\n",
                                "    \"\"\"\n",
                                "    Apply an embedding function to a list of datasets.\n",
                                "    \"\"\"\n",
                                "\n",
                                "    df = []\n",
                                "    for name, dataset in tqdm(names_datasets.items()):\n",
                                "        dataset = names_datasets[name]\n",
                                "        dataset.name = name\n",
                                "    \n",
                                "        try:\n",
                                "            df_dataset = function(dataset)\n",
                                "        except ValueError as e:\n",
                                "            print(name, e)\n",
                                "            raise NotImplementedError\n",
                                "\n",
                                "        df_dataset[\"dataset\"] = name\n",
                                "        df.append(df_dataset)\n",
                                "\n",
                                "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
                                "    return df"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 12,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:31:27.391364Z",
                                        "start_time": "2024-09-18T09:31:27.380030Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "def create_some_columns_parallel(names_datasets, function, n_jobs=-1):\n",
                                "    \"\"\"\n",
                                "    Apply an embedding function to a list of datasets.\n",
                                "\n",
                                "    Parallel version.\n",
                                "    \"\"\"\n",
                                "\n",
                                "    def f(name, dataset, function):\n",
                                "        dataset.name = name\n",
                                "        df_dataset = function(dataset)\n",
                                "        df_dataset[\"dataset\"] = name\n",
                                "        return df_dataset\n",
                                "\n",
                                "    df = joblib.Parallel(n_jobs=n_jobs)(\n",
                                "        joblib.delayed(f)(name, dataset, function)\n",
                                "        for name, dataset in tqdm(names_datasets.items())\n",
                                "    )\n",
                                "\n",
                                "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
                                "    return df"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 13,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:31:28.411964Z",
                                        "start_time": "2024-09-18T09:31:28.402880Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "def create_all_columns(functions_names_datasets, n_jobs=-1, create_dimension_feature = False):\n",
                                "    \"\"\"\n",
                                "    given a dictionary of {function1:names, function2:names,...} apply\n",
                                "    the desired functions to the list of datasets and merge all of them\n",
                                "    in a single X_y_group dataframe.\n",
                                "    \"\"\"\n",
                                "\n",
                                "    columns = []\n",
                                "    if create_dimension_feature:\n",
                                "        dimension_feature = create_some_columns(functions_names_datasets[list(functions_names_datasets.keys())[0]], add_dimension_feature)\n",
                                "        columns.append(dimension_feature)\n",
                                "    \n",
                                "    for function, names_datasets in functions_names_datasets.items():\n",
                                "        print(f\"set: {function.__name__}\")\n",
                                "\n",
                                "        if n_jobs != 1:\n",
                                "            feature_set = create_some_columns_parallel(names_datasets, function, n_jobs=n_jobs)\n",
                                "        else:\n",
                                "            feature_set = create_some_columns(names_datasets, function)\n",
                                "\n",
                                "        columns.append(feature_set)\n",
                                "\n",
                                "    # Merge all feature sets into a single dataframe:\n",
                                "    columns = functools.reduce(\n",
                                "        lambda left, right: pd.merge(left, right, on=[\"dataset\", \"variable\"]),\n",
                                "        columns,\n",
                                "    )\n",
                                "\n",
                                "    return columns"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 15,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:44:41.738202Z",
                                        "start_time": "2024-09-18T09:44:40.143933Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [
                                {
                                        "name": "stdout",
                                        "output_type": "stream",
                                        "text": [
                                                "(142910, 121)\n",
                                                "Index(['dataset', 'variable', 'dimension', 'corr(v,X)', 'corr(v,Y)',\n",
                                                "       'max(corr(v, others))', 'min(corr(v, others))', 'mean(corr(v, others))',\n",
                                                "       'std(corr(v, others))', 'corr(X,Y)',\n",
                                                "       ...\n",
                                                "       'X~v_DML_AMTE', 'X~v_DML_AMTE_zstat', 'X~v_DML_AMTE_pvalue',\n",
                                                "       'X~v_DML_std_point', 'X~v_DML_stderr_point', 'Y~v_DML_AMTE',\n",
                                                "       'Y~v_DML_AMTE_zstat', 'Y~v_DML_AMTE_pvalue', 'Y~v_DML_std_point',\n",
                                                "       'Y~v_DML_stderr_point'],\n",
                                                "      dtype='object', length=121)\n"
                                        ]
                                }
                        ],
                        "source": [
                                "X_y_group_train = pd.read_csv('mid_data/X_y_group_train_updated_v12.1_DML.csv')\n",
                                "print(X_y_group_train.shape)\n",
                                "print(X_y_group_train.columns)"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 16,
                        "metadata": {},
                        "outputs": [
                                {
                                        "name": "stdout",
                                        "output_type": "stream",
                                        "text": [
                                                "set: piecewise_linear_regression_feature\n"
                                        ]
                                },
                                {
                                        "name": "stderr",
                                        "output_type": "stream",
                                        "text": [
                                                "100%|██████████| 23500/23500 [01:27<00:00, 267.26it/s]\n"
                                        ]
                                },
                                {
                                        "name": "stdout",
                                        "output_type": "stream",
                                        "text": [
                                                "X_y_group_train.shape (142910, 131)\n",
                                                "去重后X_y_group_train.shape (142910, 129)\n"
                                        ]
                                }
                        ],
                        "source": [
                                "names_datasets_train = X_train\n",
                                "names_graphs_train = y_train\n",
                                "X_y_group_train_additional = create_all_columns(\n",
                                "    {\n",
                                "        # conditional_independence_tests: names_datasets_train,\n",
                                "        # partial_distance_covariance_dataset: names_datasets_train,\n",
                                "        # PC_feature: names_datasets_train,\n",
                                "        # FCI_feature: names_datasets_train,\n",
                                "        # copula_entropy: names_datasets_train,\n",
                                "        # linear_regression_feature: names_datasets_train,\n",
                                "        # DML_feature: names_datasets_train,\n",
                                "        piecewise_linear_regression_feature: names_datasets_train,\n",
                                "    },\n",
                                "    n_jobs=-1,\n",
                                ")\n",
                                "X_y_group_train = pd.concat([X_y_group_train, X_y_group_train_additional], axis=1)\n",
                                "print('X_y_group_train.shape', X_y_group_train.shape)\n",
                                "# 去掉重复的列\n",
                                "X_y_group_train = X_y_group_train.loc[:,~X_y_group_train.columns.duplicated()]\n",
                                "print('去重后X_y_group_train.shape', X_y_group_train.shape)"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 17,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "X_y_group_train.to_csv('./mid_data/X_y_group_train_updated_v12.2_piecewise.csv', index=False)"
                        ]
                },
                {
                        "cell_type": "markdown",
                        "metadata": {},
                        "source": [
                                "set: double_machine_learning\n",
                                "  1%|          | 288/23500 [02:17<2:48:55,  2.29it/s]"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": null,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:34:49.212104Z",
                                        "start_time": "2024-09-18T09:34:48.478623Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "print(\"Adding numeric labels y\")\n",
                                "le = LabelEncoder()\n",
                                "X_y_group_train[\"y\"] = le.fit_transform(X_y_group_train[\"label\"])\n",
                                "# reordering columns:\n",
                                "X_y_group_train = X_y_group_train[[\"dataset\", \"variable\"] + X_y_group_train.columns.drop([\"dataset\", \"variable\", \"label\", \"y\"]).tolist() + [\"label\", \"y\"]]\n",
                                "\n",
                                "numeric_columns = X_y_group_train.select_dtypes(include=[np.number]).columns\n",
                                "X_y_group_train[numeric_columns] = X_y_group_train[numeric_columns].fillna(X_y_group_train[numeric_columns].mean())\n",
                                "\n",
                                "blacklist = [\"ttest(v,X)\", \"pvalue(ttest(v,X))<=0.05\", \"ttest(v,Y)\", \"pvalue(ttest(v,Y))<=0.05\", \"ttest(X,Y)\", \"pvalue(ttest(X,Y))<=0.05\", \"max(energy_dist(v, others))\", \"min(energy_dist(v, others))\",\"mean(energy_dist(v, others))\", \"std(energy_dist(v, others))\", \"25%(energy_dist(v, others))\", \"75%(energy_dist(v, others))\"]\n",
                                "columns_to_drop = [col for col in blacklist if col in X_y_group_train.columns]\n",
                                "X_y_group_train = X_y_group_train.drop(columns=columns_to_drop)\n",
                                "\n",
                                "display(X_y_group_train)\n",
                                "\n",
                                "print(\"Extracting X_train, y_train, and group\")\n",
                                "X_train = X_y_group_train.drop([\"variable\", \"dataset\", \"label\", \"y\"], axis=\"columns\")\n",
                                "y_train = X_y_group_train[\"y\"]\n",
                                "group_train = X_y_group_train[\"dataset\"]"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": null,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:40:09.797137Z",
                                        "start_time": "2024-09-18T09:39:33.165697Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "model = RandomForestClassifier(\n",
                                "    n_estimators=100,\n",
                                "    max_depth=13,\n",
                                "    n_jobs=-1,\n",
                                "    class_weight=\"balanced\",\n",
                                "    random_state=42\n",
                                ")\n",
                                "display(model)\n",
                                "\n",
                                "cv = GroupKFold(n_splits=4)\n",
                                "results = cross_val_score(\n",
                                "    model,\n",
                                "    X_train,\n",
                                "    y_train,\n",
                                "    groups=group_train,\n",
                                "    cv=cv,\n",
                                "    verbose=True,\n",
                                "    scoring=\"balanced_accuracy\"\n",
                                ")\n",
                                "\n",
                                "print(f\"multiclass balanced accuracy: mean={results.mean()}\")"
                        ]
                },
                {
                        "cell_type": "markdown",
                        "metadata": {},
                        "source": [
                                "before:0.489\n",
                                "after: 0.488, 差别不大...\n",
                                "\n",
                                "0.488120992009518"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": null,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "from sklearn.model_selection import train_test_split\n",
                                "from sklearn.metrics import accuracy_score\n",
                                "import matplotlib.pyplot as plt\n",
                                "\n",
                                "\n",
                                "X = X_train\n",
                                "y = y_train\n",
                                "# Assuming X and y are your features and target variables\n",
                                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                "\n",
                                "model = RandomForestClassifier(\n",
                                "    n_estimators=100,\n",
                                "    max_depth=13,\n",
                                "    n_jobs=-1,\n",
                                "    class_weight=\"balanced\",\n",
                                "    random_state=42\n",
                                ")\n",
                                "# Fit the model\n",
                                "model.fit(X_train, y_train)\n",
                                "\n",
                                "# Get predictions\n",
                                "y_pred = model.predict(X_test)\n",
                                "\n",
                                "\n",
                                "# Calculate accuracy for each dimension\n",
                                "dimensions = sorted(X_test['dimension'].unique())\n",
                                "accuracies = []\n",
                                "\n",
                                "for dim in dimensions:\n",
                                "    mask = X_test['dimension'] == dim\n",
                                "    y_true_dim = y_test[mask]\n",
                                "    y_pred_dim = y_pred[mask]\n",
                                "    acc = accuracy_score(y_true_dim, y_pred_dim)\n",
                                "    accuracies.append(acc)\n",
                                "\n",
                                "# Visualize the results\n",
                                "plt.figure(figsize=(12, 6))\n",
                                "plt.plot(dimensions, accuracies, marker='o')\n",
                                "plt.xlabel('Dimension')\n",
                                "plt.ylabel('Accuracy')\n",
                                "plt.title('Model Accuracy vs Dimension')\n",
                                "plt.grid(True)\n",
                                "plt.xticks(dimensions)  # Ensure all dimensions are shown on x-axis\n",
                                "plt.tight_layout()\n",
                                "plt.show()\n",
                                "\n",
                                "# Print accuracies\n",
                                "print(\"Accuracies for each dimension:\")\n",
                                "for dim, acc in zip(dimensions, accuracies):\n",
                                "    print(f\"Dimension {dim}: Accuracy = {acc:.4f}\")\n",
                                "\n",
                                "# Calculate and print overall accuracy\n",
                                "overall_accuracy = accuracy_score(y_test, y_pred)\n",
                                "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")\n",
                                "\n",
                                "# Get feature importances\n",
                                "importances = model.feature_importances_\n",
                                "feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n",
                                "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
                                "\n",
                                "# Display top 20 most important features\n",
                                "print(\"\\nTop 20 most important features:\")\n",
                                "print(feature_importances)\n",
                                "\n",
                                "\n",
                                "print(\"\\nModel and feature importances have been saved.\")"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": null,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "feature_importances"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": null,
                        "metadata": {},
                        "outputs": [],
                        "source": []
                }
        ],
        "metadata": {
                "kernelspec": {
                        "display_name": "casual",
                        "language": "python",
                        "name": "python3"
                },
                "language_info": {
                        "codemirror_mode": {
                                "name": "ipython",
                                "version": 3
                        },
                        "file_extension": ".py",
                        "mimetype": "text/x-python",
                        "name": "python",
                        "nbconvert_exporter": "python",
                        "pygments_lexer": "ipython3",
                        "version": "3.10.14"
                }
        },
        "nbformat": 4,
        "nbformat_minor": 2
}
