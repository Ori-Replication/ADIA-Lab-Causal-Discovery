{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crunch-cli in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (3.19.1)\n",
      "Requirement already satisfied: astor in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (0.8.1)\n",
      "Requirement already satisfied: click in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (8.1.7)\n",
      "Requirement already satisfied: coloredlogs in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (15.0.1)\n",
      "Requirement already satisfied: dataclasses-json in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (0.6.7)\n",
      "Requirement already satisfied: gitignorefile in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (1.1.2)\n",
      "Requirement already satisfied: humanfriendly in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (10.0)\n",
      "Requirement already satisfied: inflection in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (0.5.1)\n",
      "Requirement already satisfied: inquirer in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (3.4.0)\n",
      "Requirement already satisfied: joblib in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (1.4.2)\n",
      "Requirement already satisfied: networkx in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (3.3)\n",
      "Requirement already satisfied: packaging in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (24.1)\n",
      "Requirement already satisfied: pandas in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (2.2.2)\n",
      "Requirement already satisfied: psutil in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (6.0.0)\n",
      "Requirement already satisfied: pyarrow in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (17.0.0)\n",
      "Requirement already satisfied: pytest in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (8.3.3)\n",
      "Requirement already satisfied: python-dotenv in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (1.0.1)\n",
      "Requirement already satisfied: requests in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (1.0.0)\n",
      "Requirement already satisfied: requirements-parser in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (0.11.0)\n",
      "Requirement already satisfied: scikit-learn in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (1.5.2)\n",
      "Requirement already satisfied: scipy in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (1.14.1)\n",
      "Requirement already satisfied: sseclient-py in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (1.8.0)\n",
      "Requirement already satisfied: tqdm in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from crunch-cli) (4.66.5)\n",
      "Requirement already satisfied: colorama in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from click->crunch-cli) (0.4.6)\n",
      "Requirement already satisfied: pyreadline3 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from humanfriendly->crunch-cli) (3.5.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from dataclasses-json->crunch-cli) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from dataclasses-json->crunch-cli) (0.9.0)\n",
      "Requirement already satisfied: blessed>=1.19.0 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from inquirer->crunch-cli) (1.20.0)\n",
      "Requirement already satisfied: editor>=1.6.0 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from inquirer->crunch-cli) (1.6.6)\n",
      "Requirement already satisfied: readchar>=4.2.0 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from inquirer->crunch-cli) (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from pandas->crunch-cli) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from pandas->crunch-cli) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from pandas->crunch-cli) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from pandas->crunch-cli) (2024.1)\n",
      "Requirement already satisfied: iniconfig in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from pytest->crunch-cli) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from pytest->crunch-cli) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from pytest->crunch-cli) (1.2.2)\n",
      "Requirement already satisfied: tomli>=1 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from pytest->crunch-cli) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from requests->crunch-cli) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from requests->crunch-cli) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from requests->crunch-cli) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from requests->crunch-cli) (2024.8.30)\n",
      "Requirement already satisfied: types-setuptools>=69.1.0 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from requirements-parser->crunch-cli) (74.1.0.20240907)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from scikit-learn->crunch-cli) (3.5.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from blessed>=1.19.0->inquirer->crunch-cli) (0.2.13)\n",
      "Requirement already satisfied: six>=1.9.0 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from blessed>=1.19.0->inquirer->crunch-cli) (1.16.0)\n",
      "Requirement already satisfied: jinxed>=1.1.0 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from blessed>=1.19.0->inquirer->crunch-cli) (1.3.0)\n",
      "Requirement already satisfied: runs in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from editor>=1.6.0->inquirer->crunch-cli) (1.2.2)\n",
      "Requirement already satisfied: xmod in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from editor>=1.6.0->inquirer->crunch-cli) (1.8.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->crunch-cli) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->crunch-cli) (4.12.2)\n",
      "Requirement already satisfied: ansicon in e:\\softwares\\anaconda\\condaenvs\\casual\\lib\\site-packages (from jinxed>=1.1.0->blessed>=1.19.0->inquirer->crunch-cli) (1.89.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "---\n",
      "Your token seems to have expired or is invalid.\n",
      "\n",
      "Please follow this link to copy and paste your new setup command:\n",
      "https://hub.crunchdao.com/competitions/causality-discovery/submit\n",
      "\n",
      "If you think that is an error, please contact an administrator.\n"
     ]
    }
   ],
   "source": [
    "%pip install crunch-cli --upgrade\n",
    "!crunch setup --notebook causality-discovery default --token vITxSN15eES05JqzBvEIAZfMzm081Wg3qIPit7wX7MmuQt23S9Yr80C2f5qbHmNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import typing\n",
    "\n",
    "import joblib\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dcor\n",
    "from scipy.stats import pearsonr, ttest_rel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n"
     ]
    }
   ],
   "source": [
    "import crunch\n",
    "crunch = crunch.load_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_nodes_representation(graph, nodelist):\n",
    "    \"\"\"\n",
    "    Create an alternative representation of a graph which is hashable\n",
    "    and equivalent graphs have the same hash.\n",
    "\n",
    "    Python cannot PROPERLY use nx.Graph/DiGraph as key for\n",
    "    dictionaries, because two equivalent graphs with just different\n",
    "    order of the nodes would result in different keys. This is\n",
    "    undesirable here.\n",
    "\n",
    "    So here we transform the graph into an equivalent form that is\n",
    "    based on a specific nodelist and that is hashable. In this way,\n",
    "    two equivalent graphs, once transformed, will result in identical\n",
    "    keys.\n",
    "\n",
    "    So we use the following trick: extract the adjacency matrix\n",
    "    (with nodes in a fixed order) and then make a hashable thing out\n",
    "    of it, through tuple(array.flatten()):\n",
    "    \"\"\"\n",
    "\n",
    "    # This get the adjacency matrix with nodes in a given order, as\n",
    "    # numpy array (which is not hashable):\n",
    "    adjacency_matrix = nx.adjacency_matrix(graph, nodelist=nodelist).todense()\n",
    "\n",
    "    # This transforms the numpy array into a hashable object:\n",
    "    hashable = tuple(adjacency_matrix.flatten())\n",
    "\n",
    "    return hashable\n",
    "\n",
    "def create_graph_label():\n",
    "    \"\"\"\n",
    "    Create a dictionary from graphs to labels, in two formats.\n",
    "    \"\"\"\n",
    "    graph_label = {\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]):             \"Cause of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]):             \"Cause of Y\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]):             \"Consequence of X\",\n",
    "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]):             \"Consequence of Y\",\n",
    "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}):                \"Independent\",\n",
    "    }\n",
    "\n",
    "    nodelist = [\"v\", \"X\", \"Y\"]\n",
    "\n",
    "    # This is an equivalent alternative to graph_label but in a form\n",
    "    # for which two equivalent graphs have the same key:\n",
    "    adjacency_label = {\n",
    "        graph_nodes_representation(graph, nodelist): label\n",
    "        for graph, label in graph_label.items()\n",
    "    }\n",
    "\n",
    "    return graph_label, adjacency_label\n",
    "\n",
    "def get_labels(adjacency_matrix, adjacency_label):\n",
    "    \"\"\"\n",
    "    Transform an adjacency_matrix (as pd.DataFrame) into a dictionary of variable:label\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
    "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]  # this is not hashable\n",
    "        key = tuple(submatrix.values.flatten())  # this is hashable and a compatible with adjacency_label\n",
    "    \n",
    "        result[variable] = adjacency_label[key]\n",
    "\n",
    "    return result\n",
    "\n",
    "graph_label, adjacency_label = create_graph_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download data\\X_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_train.pickle (1523944532 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_train.pickle (7017735 bytes)\n",
      "already exists: file length match\n",
      "download data\\X_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_test_reduced.pickle (122341879 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_test_reduced.pickle (562930 bytes)\n",
      "already exists: file length match\n",
      "download data\\example_prediction.parquet from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/example_prediction_reduced.parquet (668981 bytes)\n",
      "already exists: file length match\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test = crunch.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the correlation-based features for each\n",
    "    varibale, which are the correlation between that variable with X and Y,\n",
    "    as well as summary statistics (max, min, mean, std) of all pairs\n",
    "    of correlations.\n",
    "    \"\"\"\n",
    "\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = dataset.corr().drop([variable], axis=\"columns\").loc[variable].abs()\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"corr(v,X)\": dataset[[variable, \"X\"]].corr().loc[variable, \"X\"],\n",
    "            \"corr(v,Y)\": dataset[[variable, \"Y\"]].corr().loc[variable, \"Y\"],\n",
    "            \"max(corr(v, others))\": tmp.max(),\n",
    "            \"min(corr(v, others))\": tmp.min(),\n",
    "            \"mean(corr(v, others))\": tmp.mean(),\n",
    "            \"std(corr(v, others))\": tmp.std(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr().loc[\"X\", \"Y\"]\n",
    "\n",
    "    # pearsonr is NaN when the variance is 0, so we fill with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def ttest(dataset, pvalue_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Given a dataset, this function computes the t-test between the\n",
    "    values each variable v and X, Y. The t value and the result of the\n",
    "    t-test with a given pvalue_threshold, are used to create features\n",
    "    to describe/embed v, as well as the t-test result between the\n",
    "    values of X and Y.\n",
    "    \"\"\"\n",
    "\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        ttest_vX = ttest_rel(dataset[variable], dataset[\"X\"])\n",
    "        ttest_vY = ttest_rel(dataset[variable], dataset[\"Y\"])\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"ttest(v,X)\": ttest_vX.statistic,\n",
    "            f\"pvalue(ttest(v,X))<={pvalue_threshold}\": (ttest_vX.pvalue <= pvalue_threshold).astype(float),\n",
    "            \"ttest(v,Y)\": ttest_vY.statistic,\n",
    "            f\"pvalue(ttest(v,Y))<={pvalue_threshold}\": (ttest_vY.pvalue <= pvalue_threshold).astype(float),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    ttest_XY = ttest_rel(dataset[\"X\"], dataset[\"Y\"])\n",
    "    df[\"ttest(X,Y)\"] = ttest_XY.statistic\n",
    "    df[f\"pvalue(ttest(X,Y))<={pvalue_threshold}\"] = (ttest_XY.pvalue <= pvalue_threshold).astype(float)\n",
    "\n",
    "    # some the ttest returns NaN when the variance is 0, so we fill with 0:\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def mutual_information(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the mutual-information-based features\n",
    "    for each varibale, which are the correlation between that variable\n",
    "    X and Y, as well as summary statistics (max, min, mean, std) of\n",
    "    all pairs of correlations.\n",
    "    \"\"\"\n",
    "\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        tmp = mutual_info_regression(dataset.drop(columns=[variable]), dataset[variable])\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"MI(v,X)\": mutual_info_regression(dataset[[variable]], dataset[\"X\"], discrete_features=False)[0],\n",
    "            \"MI(v,Y)\": mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0],\n",
    "            \"max(MI(v, others))\": tmp.max(),\n",
    "            \"min(MI(v, others))\": tmp.min(),\n",
    "            \"mean(MI(v, others))\": tmp.mean(),\n",
    "            \"std(MI(v, others))\": tmp.std(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"MI(X,Y)\"] = mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_dimension_feature(dataset):\n",
    "    \"\"\"\n",
    "    Add a dimension feature to the dataset.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "    dimension = len(variables)\n",
    "    square_dimension = dimension * dimension\n",
    "    df = pd.DataFrame({\n",
    "        \"variable\": variables,\n",
    "        \"dimension\": dimension,\n",
    "        \"square_dimension\": square_dimension\n",
    "    })\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    return df\n",
    "\n",
    "def spearman_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the Spearman rank correlation-based features for each\n",
    "    variable, which are the Spearman correlation between that variable with X and Y,\n",
    "    as well as the Spearman correlation between X and Y.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"spearman_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='spearman').loc[variable, \"X\"],\n",
    "            \"spearman_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='spearman').loc[variable, \"Y\"],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"spearman_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='spearman').loc[\"X\", \"Y\"]\n",
    "\n",
    "    # Spearman correlation is NaN when there are ties in rank, so we fill with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def kendall_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the Kendall rank correlation-based features for each\n",
    "    variable, which are the Kendall correlation between that variable with X and Y,\n",
    "    as well as the Kendall correlation between X and Y.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"kendall_corr(v,X)\": stats.kendalltau(dataset[variable], dataset[\"X\"])[0],\n",
    "            \"kendall_corr(v,Y)\": stats.kendalltau(dataset[variable], dataset[\"Y\"])[0],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"kendall_corr(X,Y)\"] = stats.kendalltau(dataset[\"X\"], dataset[\"Y\"])[0]\n",
    "\n",
    "    # Kendall correlation is NaN when there are ties in rank, so we fill with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def distance_correlation(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, we compute the distance correlation-based features for each\n",
    "    variable, which are the distance correlation between that variable with X and Y,\n",
    "    as well as the distance correlation between X and Y.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"dcor(v,X)\": dcor.distance_correlation(dataset[variable], dataset[\"X\"]),\n",
    "            \"dcor(v,Y)\": dcor.distance_correlation(dataset[variable], dataset[\"Y\"]),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    df[\"dcor(X,Y)\"] = dcor.distance_correlation(dataset[\"X\"], dataset[\"Y\"])\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def linear_regression_features(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, fit linear regression models for v~X and v~Y,\n",
    "    and extract the model parameters as features.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # Fit v~X\n",
    "        model_X = LinearRegression().fit(dataset[[\"X\"]], dataset[variable])\n",
    "        # Fit v~Y\n",
    "        model_Y = LinearRegression().fit(dataset[[\"Y\"]], dataset[variable])\n",
    "\n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"v~X_intercept\": model_X.intercept_,\n",
    "            \"v~X_coefficient\": model_X.coef_[0],\n",
    "            \"v~Y_intercept\": model_Y.intercept_,\n",
    "            \"v~Y_coefficient\": model_Y.coef_[0],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "def conditional_mutual_information(dataset):\n",
    "    \"\"\"\n",
    "    Calculate conditional mutual information for each variable with X and Y.\n",
    "    \"\"\"\n",
    "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
    "    \n",
    "    df = []\n",
    "    for variable in variables:\n",
    "        # Calculate conditional MI(v, X | Y)\n",
    "        mi_vx_given_y = mutual_info_regression(dataset[[variable, \"Y\"]], dataset[\"X\"], discrete_features=False)[0] - \\\n",
    "                        mutual_info_regression(dataset[[\"Y\"]], dataset[\"X\"], discrete_features=False)[0]\n",
    "        \n",
    "        # Calculate conditional MI(v, Y | X)\n",
    "        mi_vy_given_x = mutual_info_regression(dataset[[variable, \"X\"]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
    "                        mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
    "        \n",
    "        # Calculate conditional MI(X, Y | v)\n",
    "        mi_xy_given_v = mutual_info_regression(dataset[[\"X\", variable]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
    "                        mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0]\n",
    "        \n",
    "        df.append({\n",
    "            \"variable\": variable,\n",
    "            \"conditional_MI(v,X|Y)\": mi_vx_given_y,\n",
    "            \"conditional_MI(v,Y|X)\": mi_vy_given_x,\n",
    "            \"conditional_MI(X,Y|v)\": mi_xy_given_v,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"dataset\"] = dataset.name\n",
    "    \n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Given a graph as adjacency_matrix, create the class labels of each variable.\n",
    "    \"\"\"\n",
    "\n",
    "    adjacency_graph, adjacency_label = create_graph_label()\n",
    "    labels = get_labels(adjacency_matrix, adjacency_label)\n",
    "    variables = adjacency_matrix.columns.drop([\"X\", \"Y\"])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"variable\": variables,\n",
    "        \"label\": [labels[variable] for variable in variables],\n",
    "    })\n",
    "    df[\"dataset\"] = adjacency_matrix.name\n",
    "\n",
    "    # Reorder columns:\n",
    "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_some_columns(names_datasets, function):\n",
    "    \"\"\"\n",
    "    Apply an embedding function to a list of datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    df = []\n",
    "    for name, dataset in tqdm(names_datasets.items()):\n",
    "        dataset = names_datasets[name]\n",
    "        dataset.name = name\n",
    "    \n",
    "        try:\n",
    "            df_dataset = function(dataset)\n",
    "        except ValueError as e:\n",
    "            print(name, e)\n",
    "            raise NotImplementedError\n",
    "\n",
    "        df_dataset[\"dataset\"] = name\n",
    "        df.append(df_dataset)\n",
    "\n",
    "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_some_columns_parallel(names_datasets, function, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Apply an embedding function to a list of datasets.\n",
    "\n",
    "    Parallel version.\n",
    "    \"\"\"\n",
    "\n",
    "    def f(name, dataset, function):\n",
    "        dataset.name = name\n",
    "        df_dataset = function(dataset)\n",
    "        df_dataset[\"dataset\"] = name\n",
    "        return df_dataset\n",
    "\n",
    "    df = joblib.Parallel(n_jobs=n_jobs)(\n",
    "        joblib.delayed(f)(name, dataset, function)\n",
    "        for name, dataset in tqdm(names_datasets.items())\n",
    "    )\n",
    "\n",
    "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_columns(functions_names_datasets, n_jobs=-1, create_dimension_feature = False):\n",
    "    \"\"\"\n",
    "    given a dictionary of {function1:names, function2:names,...} apply\n",
    "    the desired functions to the list of datasets and merge all of them\n",
    "    in a single X_y_group dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    columns = []\n",
    "    if create_dimension_feature:\n",
    "        dimension_feature = create_some_columns(functions_names_datasets[list(functions_names_datasets.keys())[0]], add_dimension_feature)\n",
    "        columns.append(dimension_feature)\n",
    "    \n",
    "    for function, names_datasets in functions_names_datasets.items():\n",
    "        print(f\"set: {function.__name__}\")\n",
    "\n",
    "        if n_jobs != 1:\n",
    "            feature_set = create_some_columns_parallel(names_datasets, function, n_jobs=n_jobs)\n",
    "        else:\n",
    "            feature_set = create_some_columns(names_datasets, function)\n",
    "\n",
    "        columns.append(feature_set)\n",
    "\n",
    "    # Merge all feature sets into a single dataframe:\n",
    "    columns = functools.reduce(\n",
    "        lambda left, right: pd.merge(left, right, on=[\"dataset\", \"variable\"]),\n",
    "        columns,\n",
    "    )\n",
    "\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_datasets_train = X_train\n",
    "names_graphs_train = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, columns, threshold=3):\n",
    "    z_scores = np.abs(stats.zscore(df[columns]))\n",
    "    df['z_score_max'] = np.max(z_scores, axis=1)\n",
    "    df['is_outlier'] = df['z_score_max'] > threshold\n",
    "    df_cleaned = df[df['is_outlier'] == False].copy()\n",
    "    df_cleaned.drop(columns=['z_score_max', 'is_outlier'], inplace=True)\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment what you need!\n",
    "def train(\n",
    "    X_train: typing.Dict[str, pd.DataFrame],\n",
    "    y_train: typing.Dict[str, pd.DataFrame],\n",
    "    # number_of_features: int,\n",
    "    model_directory_path: str,\n",
    "    # id_column_name: str,\n",
    "    # prediction_column_name: str,\n",
    "    # has_gpu: bool,\n",
    ") -> None:\n",
    "    # 1) Create the variable embedding:\n",
    "    return\n",
    "    X_y_group_train_pathname = \"/tmp/X_y_group_train.parquet\"\n",
    "    try:\n",
    "        print(f\"Loading {X_y_group_train_pathname}\")\n",
    "        X_y_group_train = pd.read_parquet(X_y_group_train_pathname)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Creating X_y_group_train\")\n",
    "        names_datasets_train = X_train\n",
    "        names_graphs_train = y_train\n",
    "        X_y_group_train = create_all_columns(\n",
    "            {\n",
    "                pearson_correlation: names_datasets_train,\n",
    "                ttest: names_datasets_train,\n",
    "                mutual_information: names_datasets_train,  # comment this line to greatly reduce computation\n",
    "                label: names_graphs_train,\n",
    "                spearman_correlation: names_datasets_train,\n",
    "                kendall_correlation: names_datasets_train,\n",
    "                distance_correlation: names_datasets_train,\n",
    "                conditional_mutual_information: names_datasets_train,\n",
    "            },\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        X_y_group_train = remove_outliers(X_y_group_train, ['corr(v,X)', 'corr(v,Y)', 'ttest(v,X)', 'ttest(v,Y)'])\n",
    "        # 2) Massage X_y_group_train to prepare what is needed by the model:\n",
    "        print(\"Adding numeric labels y\") # sklearn wants numeric labels\n",
    "        le = LabelEncoder()\n",
    "        le.classes_ = np.array([\n",
    "            'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
    "            'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
    "        ])\n",
    "        X_y_group_train[\"y\"] = le.transform(X_y_group_train[\"label\"])\n",
    "\n",
    "        # reordering columns:\n",
    "        X_y_group_train = X_y_group_train[[\"dataset\", \"variable\"] + X_y_group_train.columns.drop([\"dataset\", \"variable\", \"label\", \"y\"]).tolist() + [\"label\", \"y\"]]\n",
    "        display(X_y_group_train)\n",
    "\n",
    "        print(\"Saving X_y_group_train\")\n",
    "        os.makedirs(os.path.dirname(X_y_group_train_pathname), exist_ok=True)\n",
    "        X_y_group_train.to_parquet(X_y_group_train_pathname)\n",
    "\n",
    "    print(\"Extracting X_train, y_train, and group\")\n",
    "    X_train = X_y_group_train.drop([\"variable\", \"dataset\", \"label\", \"y\"], axis=\"columns\")\n",
    "    y_train = X_y_group_train[\"y\"]\n",
    "    group_train = X_y_group_train[\"dataset\"]\n",
    "\n",
    "    # Model:\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=11, n_jobs=-1, class_weight=\"balanced\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    joblib.dump(\n",
    "        model,\n",
    "        os.path.join(model_directory_path, \"model.joblib\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(X_y_pred_test):\n",
    "    \"\"\"\n",
    "    From the predicted test set, for each dataset, take predicted\n",
    "    classes of all variables, create the adjacency matrix, then create\n",
    "    the submission in the requested format.\n",
    "    \"\"\"\n",
    "\n",
    "    submission = {}\n",
    "    for name, prediction in tqdm(X_y_pred_test.groupby(\"dataset\"), delay=10):\n",
    "        variables_labels = prediction[[\"variable\", \"label_predicted\"]].set_index(\"variable\")\n",
    "        variables = variables_labels.index.tolist()\n",
    "        variables_all = [\"X\", \"Y\"] + variables\n",
    "\n",
    "        adjacency_matrix = pd.DataFrame(index=variables_all, columns=variables_all)\n",
    "        adjacency_matrix.index.name = \"parent\"\n",
    "        adjacency_matrix[:] = 0\n",
    "        adjacency_matrix.loc[\"X\", \"Y\"] = 1\n",
    "\n",
    "        for v in variables:\n",
    "            l = variables_labels.loc[v].item()\n",
    "            if l == \"Cause of X\":\n",
    "                adjacency_matrix.loc[v, \"X\"] = 1\n",
    "            elif l == \"Cause of Y\":\n",
    "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
    "            elif l == \"Consequence of X\":\n",
    "                adjacency_matrix.loc[\"X\", v] = 1\n",
    "            elif l == \"Consequence of Y\":\n",
    "                adjacency_matrix.loc[\"Y\", v] = 1\n",
    "            elif l == \"Confounder\":\n",
    "                adjacency_matrix.loc[v, \"X\"] = 1\n",
    "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
    "            elif l == \"Collider\":\n",
    "                adjacency_matrix.loc[\"X\", v] = 1\n",
    "                adjacency_matrix.loc[\"Y\", v] = 1\n",
    "            elif l == \"Mediator\":\n",
    "                adjacency_matrix.loc[\"X\", v] = 1\n",
    "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
    "            elif l == \"Confounder\":\n",
    "                pass\n",
    "\n",
    "        for i in variables_all:\n",
    "            for j in variables_all:\n",
    "                submission[f'{name}_{i}_{j}'] = int(adjacency_matrix.loc[i, j])\n",
    "\n",
    "    return submission\n",
    "\n",
    "\n",
    "# Uncomment what you need!\n",
    "def infer(\n",
    "    X_test: typing.Dict[str, pd.DataFrame],\n",
    "    # number_of_features: int,\n",
    "    model_directory_path: str,\n",
    "    id_column_name: str,\n",
    "    prediction_column_name: str,\n",
    "    # has_gpu: bool,\n",
    "    # has_trained: bool,\n",
    ") -> pd.DataFrame:\n",
    "    model = joblib.load(os.path.join(model_directory_path, \"random_forest_model_v0.joblib\"))\n",
    "\n",
    "    names_datasets_test = X_test\n",
    "    X_group_test = create_all_columns(\n",
    "        {\n",
    "                pearson_correlation: names_datasets_test,\n",
    "                ttest: names_datasets_test,\n",
    "                mutual_information: names_datasets_test,  # comment this line to greatly reduce computation\n",
    "                spearman_correlation: names_datasets_test,\n",
    "                kendall_correlation: names_datasets_test,\n",
    "                distance_correlation: names_datasets_test,\n",
    "                conditional_mutual_information: names_datasets_test,\n",
    "        },\n",
    "        n_jobs=-1,\n",
    "        create_dimension_feature=True\n",
    "    )\n",
    "    blacklist = [\"ttest(v,X)\", \"pvalue(ttest(v,X))<=0.05\", \"ttest(v,Y)\", \"pvalue(ttest(v,Y))<=0.05\", \"ttest(X,Y)\", \"pvalue(ttest(X,Y))<=0.05\"]\n",
    "    X_group_test = X_group_test.drop(columns=blacklist)\n",
    "    X_group_test['MI(v,X)^2'] = X_group_test['MI(v,X)'] ** 2\n",
    "    X_group_test['MI(v,Y)^2'] = X_group_test['MI(v,Y)'] ** 2\n",
    "    X_group_test['MI(X,Y)^2'] = X_group_test['MI(X,Y)'] ** 2\n",
    "    X_group_test['max(MI(v, others))^2'] = X_group_test['max(MI(v, others))'] ** 2\n",
    "    X_group_test['min(MI(v, others))^2'] = X_group_test['min(MI(v, others))'] ** 2\n",
    "    # square_dimension\n",
    "    X_test = X_group_test.drop(columns=[\"dataset\", \"variable\"])\n",
    "    y_predicted = model.predict(X_test)\n",
    "    X_y_pred_test = X_group_test\n",
    "    X_y_pred_test[\"y_predicted\"] = y_predicted\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.classes_ = np.array([\n",
    "        'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
    "        'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
    "    ])\n",
    "\n",
    "    X_y_pred_test[\"label_predicted\"] = le.inverse_transform(y_predicted)\n",
    "\n",
    "    submission = create_submission(X_y_pred_test)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        submission.items(),\n",
    "        columns=[\n",
    "            id_column_name,\n",
    "            prediction_column_name\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignoring cell #3: invalid syntax (<unknown>, line 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m13:33:07\u001b[0m \u001b[33mno forbidden library found\u001b[0m\n",
      "\u001b[32m13:33:07\u001b[0m \u001b[33m\u001b[0m\n",
      "\u001b[32m13:33:07\u001b[0m started\n",
      "\u001b[32m13:33:07\u001b[0m running local test\n",
      "\u001b[32m13:33:07\u001b[0m \u001b[33minternet access isn't restricted, no check will be done\u001b[0m\n",
      "\u001b[32m13:33:07\u001b[0m \n",
      "\u001b[32m13:33:15\u001b[0m starting dag process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download data\\X_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_train.pickle (1523944532 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_train.pickle (7017735 bytes)\n",
      "already exists: file length match\n",
      "download data\\X_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_test_reduced.pickle (122341879 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_test_reduced.pickle (562930 bytes)\n",
      "already exists: file length match\n",
      "download data\\example_prediction.parquet from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/example_prediction_reduced.parquet (668981 bytes)\n",
      "already exists: file length match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m13:33:44\u001b[0m \u001b[33mcall: train\u001b[0m\n",
      "\u001b[32m13:33:44\u001b[0m \u001b[33mcall: infer\u001b[0m\n",
      "100%|██████████| 1880/1880 [00:01<00:00, 1674.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: pearson_correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [00:10<00:00, 187.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: ttest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [00:08<00:00, 219.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: mutual_information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "crunch.test(\n",
    "    no_determinism_check=True\n",
    ")\n",
    "\n",
    "print(\"Download this notebook and submit it to the platform: https://hub.crunchdao.com/competitions/causality-discovery/submit/via/notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
