{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3f6e3dd2-888c-4541-b02f-d3caaae44ede",
      "metadata": {
        "id": "3f6e3dd2-888c-4541-b02f-d3caaae44ede"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import os\n",
        "import typing\n",
        "\n",
        "import joblib\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dcor\n",
        "from scipy.stats import pearsonr, ttest_rel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, GroupKFold, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.utils import check_random_state\n",
        "from tqdm.auto import tqdm\n",
        "from scipy import stats\n",
        "from scipy.stats import entropy\n",
        "import statsmodels.api as sm\n",
        "import pingouin as pg\n",
        "from scipy.spatial.distance import pdist, squareform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "98414dd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98414dd1",
        "outputId": "d805a278-6422-4544-cce9-5060ad26d85a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded inline runner with module: <module '__main__'>\n"
          ]
        }
      ],
      "source": [
        "import crunch\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CSSzb8ssGhGQ",
      "metadata": {
        "id": "CSSzb8ssGhGQ"
      },
      "source": [
        "## Useful Functions\n",
        "\n",
        "The following functions are provided to help you get the class labels of the variables in a graph according to their relationship to `X` and `Y`. The variables can be colliders, confounders, mediators, etc.\n",
        "\n",
        "This is the core of the solution code, which reads one record at a time, applies the PC algorithm, ensures that the result is a DAG, and then puts the result into a single data frame in the required format, ready for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1cd05a2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def graph_nodes_representation(graph, nodelist):\n",
        "    \"\"\"\n",
        "    Create an alternative representation of a graph which is hashable\n",
        "    and equivalent graphs have the same hash.\n",
        "\n",
        "    Python cannot PROPERLY use nx.Graph/DiGraph as key for\n",
        "    dictionaries, because two equivalent graphs with just different\n",
        "    order of the nodes would result in different keys. This is\n",
        "    undesirable here.\n",
        "\n",
        "    So here we transform the graph into an equivalent form that is\n",
        "    based on a specific nodelist and that is hashable. In this way,\n",
        "    two equivalent graphs, once transformed, will result in identical\n",
        "    keys.\n",
        "\n",
        "    So we use the following trick: extract the adjacency matrix\n",
        "    (with nodes in a fixed order) and then make a hashable thing out\n",
        "    of it, through tuple(array.flatten()):\n",
        "    \"\"\"\n",
        "\n",
        "    # This get the adjacency matrix with nodes in a given order, as\n",
        "    # numpy array (which is not hashable):\n",
        "    adjacency_matrix = nx.adjacency_matrix(graph, nodelist=nodelist).todense()\n",
        "\n",
        "    # This transforms the numpy array into a hashable object:\n",
        "    hashable = tuple(adjacency_matrix.flatten())\n",
        "\n",
        "    return hashable\n",
        "\n",
        "def create_graph_label():\n",
        "    \"\"\"\n",
        "    Create a dictionary from graphs to labels, in two formats.\n",
        "    \"\"\"\n",
        "    graph_label = {\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]):             \"Cause of X\",\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]):             \"Cause of Y\",\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]):             \"Consequence of X\",\n",
        "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]):             \"Consequence of Y\",\n",
        "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}):                \"Independent\",\n",
        "    }\n",
        "\n",
        "    nodelist = [\"v\", \"X\", \"Y\"]\n",
        "\n",
        "    # This is an equivalent alternative to graph_label but in a form\n",
        "    # for which two equivalent graphs have the same key:\n",
        "    adjacency_label = {\n",
        "        graph_nodes_representation(graph, nodelist): label\n",
        "        for graph, label in graph_label.items()\n",
        "    }\n",
        "\n",
        "    return graph_label, adjacency_label\n",
        "\n",
        "def get_labels(adjacency_matrix, adjacency_label):\n",
        "    \"\"\"\n",
        "    Transform an adjacency_matrix (as pd.DataFrame) into a dictionary of variable:label\n",
        "    \"\"\"\n",
        "\n",
        "    result = {}\n",
        "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
        "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]  # this is not hashable\n",
        "        key = tuple(submatrix.values.flatten())  # this is hashable and a compatible with adjacency_label\n",
        "    \n",
        "        result[variable] = adjacency_label[key]\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "08d7d3e2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "download data\\X_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_train.pickle (1523944532 bytes)\n",
            "already exists: file length match\n",
            "download data\\y_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_train.pickle (7017735 bytes)\n",
            "already exists: file length match\n",
            "download data\\X_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_test_reduced.pickle (122341879 bytes)\n",
            "already exists: file length match\n",
            "download data\\y_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_test_reduced.pickle (562930 bytes)\n",
            "already exists: file length match\n",
            "download data\\example_prediction.parquet from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/example_prediction_reduced.parquet (668981 bytes)\n",
            "already exists: file length match\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train, X_test = crunch.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d193daf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pearson_correlation(dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, we compute the correlation-based features for each\n",
        "    varibale, which are the correlation between that variable with X and Y,\n",
        "    as well as summary statistics (max, min, mean, std) of all pairs\n",
        "    of correlations.\n",
        "    \"\"\"\n",
        "\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        tmp = dataset.corr().drop([variable], axis=\"columns\").loc[variable].abs()\n",
        "\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"corr(v,X)\": dataset[[variable, \"X\"]].corr().loc[variable, \"X\"],\n",
        "            \"corr(v,Y)\": dataset[[variable, \"Y\"]].corr().loc[variable, \"Y\"],\n",
        "            \"max(corr(v, others))\": tmp.max(),\n",
        "            \"min(corr(v, others))\": tmp.min(),\n",
        "            \"mean(corr(v, others))\": tmp.mean(),\n",
        "            \"std(corr(v, others))\": tmp.std(),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    df[\"corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr().loc[\"X\", \"Y\"]\n",
        "\n",
        "    # pearsonr is NaN when the variance is 0, so we fill with 0\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def ttest(dataset, pvalue_threshold=0.05):\n",
        "    \"\"\"\n",
        "    Given a dataset, this function computes the t-test between the\n",
        "    values each variable v and X, Y. The t value and the result of the\n",
        "    t-test with a given pvalue_threshold, are used to create features\n",
        "    to describe/embed v, as well as the t-test result between the\n",
        "    values of X and Y.\n",
        "    \"\"\"\n",
        "\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        ttest_vX = ttest_rel(dataset[variable], dataset[\"X\"])\n",
        "        ttest_vY = ttest_rel(dataset[variable], dataset[\"Y\"])\n",
        "\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"ttest(v,X)\": ttest_vX.statistic,\n",
        "            f\"pvalue(ttest(v,X))<={pvalue_threshold}\": (ttest_vX.pvalue <= pvalue_threshold).astype(float),\n",
        "            \"ttest(v,Y)\": ttest_vY.statistic,\n",
        "            f\"pvalue(ttest(v,Y))<={pvalue_threshold}\": (ttest_vY.pvalue <= pvalue_threshold).astype(float),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    ttest_XY = ttest_rel(dataset[\"X\"], dataset[\"Y\"])\n",
        "    df[\"ttest(X,Y)\"] = ttest_XY.statistic\n",
        "    df[f\"pvalue(ttest(X,Y))<={pvalue_threshold}\"] = (ttest_XY.pvalue <= pvalue_threshold).astype(float)\n",
        "\n",
        "    # some the ttest returns NaN when the variance is 0, so we fill with 0:\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def mutual_information(dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, we compute the mutual-information-based features\n",
        "    for each varibale, which are the correlation between that variable\n",
        "    X and Y, as well as summary statistics (max, min, mean, std) of\n",
        "    all pairs of correlations.\n",
        "    \"\"\"\n",
        "\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        tmp = mutual_info_regression(dataset.drop(columns=[variable]), dataset[variable])\n",
        "\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"MI(v,X)\": mutual_info_regression(dataset[[variable]], dataset[\"X\"], discrete_features=False)[0],\n",
        "            \"MI(v,Y)\": mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0],\n",
        "            \"max(MI(v, others))\": tmp.max(),\n",
        "            \"min(MI(v, others))\": tmp.min(),\n",
        "            \"mean(MI(v, others))\": tmp.mean(),\n",
        "            \"std(MI(v, others))\": tmp.std(),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    df[\"MI(X,Y)\"] = mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_dimension_feature(dataset):\n",
        "    \"\"\"\n",
        "    Add a dimension feature to the dataset.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "    dimension = len(variables)\n",
        "    square_dimension = dimension * dimension\n",
        "    df = pd.DataFrame({\n",
        "        \"variable\": variables,\n",
        "        \"dimension\": dimension,\n",
        "        \"square_dimension\": square_dimension\n",
        "    })\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    return df\n",
        "\n",
        "def spearman_correlation(dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, we compute the Spearman rank correlation-based features for each\n",
        "    variable, which are the Spearman correlation between that variable with X and Y,\n",
        "    as well as summary statistics (max, min, mean, std) of all pairs of Spearman correlations.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        tmp = dataset.corr(method='spearman').drop([variable], axis=\"columns\").loc[variable].abs()\n",
        "\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"spearman_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='spearman').loc[variable, \"X\"],\n",
        "            \"spearman_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='spearman').loc[variable, \"Y\"],\n",
        "            \"max(spearman_corr(v, others))\": tmp.max(),\n",
        "            \"min(spearman_corr(v, others))\": tmp.min(),\n",
        "            \"mean(spearman_corr(v, others))\": tmp.mean(),\n",
        "            \"std(spearman_corr(v, others))\": tmp.std(),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    df[\"spearman_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='spearman').loc[\"X\", \"Y\"]\n",
        "\n",
        "    # Spearman correlation is NaN when there are ties in rank, so we fill with 0\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def kendall_correlation(dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, we compute the Kendall's tau correlation-based features for each\n",
        "    variable, which are the Kendall's tau correlation between that variable with X and Y,\n",
        "    as well as summary statistics (max, min, mean, std) of all pairs of Kendall's tau correlations.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        tmp = dataset.corr(method='kendall').drop([variable], axis=\"columns\").loc[variable].abs()\n",
        "\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"kendall_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='kendall').loc[variable, \"X\"],\n",
        "            \"kendall_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='kendall').loc[variable, \"Y\"],\n",
        "            \"max(kendall_corr(v, others))\": tmp.max(),\n",
        "            \"min(kendall_corr(v, others))\": tmp.min(),\n",
        "            \"mean(kendall_corr(v, others))\": tmp.mean(),\n",
        "            \"std(kendall_corr(v, others))\": tmp.std(),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    df[\"kendall_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='kendall').loc[\"X\", \"Y\"]\n",
        "\n",
        "    # Kendall's tau correlation can be NaN in some cases, so we fill with 0\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def distance_correlation(dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, we compute the distance correlation-based features for each\n",
        "    variable, which are the distance correlation between that variable with X and Y,\n",
        "    as well as the distance correlation between X and Y.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        # todo:尝试不同的exponent（欧氏距离的系数）\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"dcor(v,X)\": dcor.distance_correlation(dataset[variable], dataset[\"X\"]),\n",
        "            \"dcor(v,Y)\": dcor.distance_correlation(dataset[variable], dataset[\"Y\"]),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    df[\"dcor(X,Y)\"] = dcor.distance_correlation(dataset[\"X\"], dataset[\"Y\"])\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def distance_correlation_v2(dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, we compute the distance correlation-based features for each\n",
        "    variable, which are the distance correlation between that variable with X and Y,\n",
        "    as well as the distance correlation between X and Y.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    def distcorr(X, Y):\n",
        "        X = np.atleast_1d(X)\n",
        "        Y = np.atleast_1d(Y)\n",
        "        if np.prod(X.shape) == len(X):\n",
        "            X = X[:, None]\n",
        "        if np.prod(Y.shape) == len(Y):\n",
        "            Y = Y[:, None]\n",
        "        X = np.atleast_2d(X)\n",
        "        Y = np.atleast_2d(Y)\n",
        "        n = X.shape[0]\n",
        "        if Y.shape[0] != X.shape[0]:\n",
        "            raise ValueError('Number of samples must match')\n",
        "        a = squareform(pdist(X))\n",
        "        b = squareform(pdist(Y))\n",
        "        A = a - a.mean(axis=0)[None, :] - a.mean(axis=1)[:, None] + a.mean()\n",
        "        B = b - b.mean(axis=0)[None, :] - b.mean(axis=1)[:, None] + b.mean()\n",
        "        \n",
        "        dcov2_xy = (A * B).sum()/float(n * n)\n",
        "        dcov2_xx = (A * A).sum()/float(n * n)\n",
        "        dcov2_yy = (B * B).sum()/float(n * n)\n",
        "        dcor = np.sqrt(dcov2_xy)/np.sqrt(np.sqrt(dcov2_xx) * np.sqrt(dcov2_yy))\n",
        "        return dcor\n",
        "\n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"dcor(v,X)_v2\": distcorr(dataset[variable], dataset[\"X\"]),\n",
        "            \"dcor(v,Y)_v2\": distcorr(dataset[variable], dataset[\"Y\"]),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "\n",
        "    df[\"dcor(X,Y)_v2\"] = distcorr(dataset[\"X\"], dataset[\"Y\"])\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def linear_regression_feature(dataset):\n",
        "    def Squared_term(dataset, variables):\n",
        "        for var in variables:\n",
        "            dataset[f'{var}_squared_term'] = dataset[var] ** 2\n",
        "        return dataset\n",
        "\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
        "\n",
        "    # model1: Fit X, v, v^2 ~ Y\n",
        "    model1_features = [\"X\"] + variables\n",
        "    d1 = Squared_term(dataset[model1_features], variables)\n",
        "    model1_features = d1.columns.tolist()\n",
        "    scaler = StandardScaler()\n",
        "    d1_scaled = scaler.fit_transform(d1)\n",
        "    model1 = LinearRegression().fit(d1_scaled, dataset[[\"Y\"]])\n",
        "    model1_coefs = model1.coef_[0].tolist()\n",
        "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
        "    \n",
        "    # model2: Fit v, v^2 ~ X\n",
        "    model2_features = variables\n",
        "    d2 = Squared_term(dataset[model2_features], variables)\n",
        "    model2_features = d2.columns.tolist()\n",
        "    scaler = StandardScaler()\n",
        "    d2_scaled = scaler.fit_transform(d2)\n",
        "    model2 = LinearRegression().fit(d2_scaled, dataset[[\"X\"]])\n",
        "    model2_coefs = model2.coef_[0].tolist()\n",
        "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
        "    \n",
        "    df = []\n",
        "    for i, variable in enumerate(variables):\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"v~Y_coefficient\": model1_dict[variable],\n",
        "            \"v_squared~Y_coefficient\": model1_dict[f\"{variable}_squared_term\"],\n",
        "            \"v~X_coefficient\": model2_dict[variable],\n",
        "            \"v_squared~X_coefficient\": model2_dict[f\"{variable}_squared_term\"],\n",
        "        })\n",
        "        \n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    df[\"X~Y_coefficient\"] = model1_dict[\"X\"]\n",
        "    \n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def ridge_regression_feature(dataset):\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
        "    # 使用GridSearchCV来选择最佳的alpha值\n",
        "    param_grid = {'alpha': np.logspace(-6, 6, 13)}\n",
        "    \n",
        "    # model1: Fit X, v ~ Y\n",
        "    model1_features = [\"X\"] + variables\n",
        "    scaler1 = StandardScaler()\n",
        "    d1_scaled = scaler1.fit_transform(dataset[model1_features])\n",
        "    model1 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
        "    model1.fit(d1_scaled, dataset[\"Y\"])\n",
        "    model1_coefs = model1.best_estimator_.coef_.tolist()\n",
        "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
        "    \n",
        "    # model2: Fit v ~ X\n",
        "    model2_features = variables\n",
        "    scaler2 = StandardScaler()\n",
        "    d2_scaled = scaler2.fit_transform(dataset[model2_features])\n",
        "    model2 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
        "    model2.fit(d2_scaled, dataset[\"X\"])\n",
        "    model2_coefs = model2.best_estimator_.coef_.tolist()\n",
        "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
        "    \n",
        "    df = []\n",
        "    for i, variable in enumerate(variables):\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"v~Y_ridge_coefficient\": model1_dict[variable],\n",
        "            \"v~X_ridge_coefficient\": model2_dict[variable]\n",
        "        })\n",
        "        \n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    df[\"X~Y_ridge_coefficient\"] = model1_dict[\"X\"]\n",
        "    \n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def conditional_mutual_information(dataset):\n",
        "    \"\"\"\n",
        "    Calculate conditional mutual information for each variable with X and Y.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "    \n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        # Calculate conditional MI(v, X | Y)\n",
        "        mi_vx_given_y = mutual_info_regression(dataset[[variable, \"Y\"]], dataset[\"X\"], discrete_features=False)[0] - \\\n",
        "                        mutual_info_regression(dataset[[\"Y\"]], dataset[\"X\"], discrete_features=False)[0]\n",
        "        \n",
        "        # Calculate conditional MI(v, Y | X)\n",
        "        mi_vy_given_x = mutual_info_regression(dataset[[variable, \"X\"]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
        "                        mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
        "        \n",
        "        # Calculate conditional MI(X, Y | v)\n",
        "        mi_xy_given_v = mutual_info_regression(dataset[[\"X\", variable]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
        "                        mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0]\n",
        "        \n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"conditional_MI(v,X|Y)\": mi_vx_given_y,\n",
        "            \"conditional_MI(v,Y|X)\": mi_vy_given_x,\n",
        "            \"conditional_MI(X,Y|v)\": mi_xy_given_v,\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "    \n",
        "    return df\n",
        "\n",
        "def partial_correlation(dataset):\n",
        "    \"\"\"\n",
        "    Compute partial correlation coefficients for each variable with X and Y,\n",
        "    controlling for the other variable, as well as the partial correlation\n",
        "    between X and Y controlling for each variable.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "    \n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        # Compute partial correlations\n",
        "        pcorr_vX_Y = pg.partial_corr(data=dataset, x=variable, y='X', covar='Y')['r'].iloc[0]\n",
        "        pcorr_vY_X = pg.partial_corr(data=dataset, x=variable, y='Y', covar='X')['r'].iloc[0]\n",
        "        pcorr_XY_v = pg.partial_corr(data=dataset, x='X', y='Y', covar=variable)['r'].iloc[0]\n",
        "        \n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"partial_corr(v,X|Y)\": pcorr_vX_Y,\n",
        "            \"partial_corr(v,Y|X)\": pcorr_vY_X,\n",
        "            \"partial_corr(X,Y|v)\": pcorr_XY_v,\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "    \n",
        "    return df\n",
        "\n",
        "def entropy_features(dataset):\n",
        "    \"\"\"\n",
        "    Calculate entropy and conditional entropy features for each variable.\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "    \n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        \n",
        "        # Calculate conditional entropies\n",
        "        joint_vX, _, _ = np.histogram2d(dataset[variable], dataset[\"X\"], bins=20)\n",
        "        joint_vY, _, _ = np.histogram2d(dataset[variable], dataset[\"Y\"], bins=20)\n",
        "        \n",
        "        # Normalize the joint distributions\n",
        "        joint_vX = joint_vX / joint_vX.sum()\n",
        "        joint_vY = joint_vY / joint_vY.sum()\n",
        "        \n",
        "        # Calculate marginal distributions\n",
        "        p_v_X = joint_vX.sum(axis=1)\n",
        "        p_v_Y = joint_vY.sum(axis=1)\n",
        "        \n",
        "        # Calculate conditional entropies\n",
        "        cond_entropy_X_v = np.sum(p_v_X * entropy(joint_vX.T, axis=0))\n",
        "        cond_entropy_Y_v = np.sum(p_v_Y * entropy(joint_vY.T, axis=0))\n",
        "        \n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"conditional_entropy(X|v)\": cond_entropy_X_v,\n",
        "            \"conditional_entropy(Y|v)\": cond_entropy_Y_v,\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "    \n",
        "    return df\n",
        "\n",
        "def hilbert_schmidt_independence(dataset):\n",
        "    \"\"\"\n",
        "    Compute the Hilbert-Schmidt Independence Criterion (HSIC) between variables.\n",
        "    Features:\n",
        "        - hsic(v, X)\n",
        "        - hsic(v, Y)\n",
        "        - hsic(X, Y)\n",
        "    \"\"\"\n",
        "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
        "    \n",
        "    df = []\n",
        "    for variable in variables:\n",
        "        hsic_vX = hsic(dataset[variable].values, dataset[\"X\"].values)\n",
        "        hsic_vY = hsic(dataset[variable].values, dataset[\"Y\"].values)\n",
        "        df.append({\n",
        "            \"variable\": variable,\n",
        "            \"hsic(v,X)\": hsic_vX,\n",
        "            \"hsic(v,Y)\": hsic_vY,\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(df)\n",
        "    df[\"dataset\"] = dataset.name\n",
        "    \n",
        "    hsic_XY = hsic(dataset[\"X\"].values, dataset[\"Y\"].values)\n",
        "    df[\"hsic(X,Y)\"] = hsic_XY\n",
        "    \n",
        "    # Reorder columns\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "    \n",
        "    return df\n",
        "\n",
        "def hsic(x, y, sigma=None):\n",
        "    \"\"\"\n",
        "    Compute the HSIC between two variables x and y.\n",
        "    x and y are numpy arrays of shape (n_samples,)\n",
        "    \"\"\"\n",
        "    n = x.shape[0]\n",
        "    x = x.reshape((n, -1))\n",
        "    y = y.reshape((n, -1))\n",
        "    \n",
        "    if sigma is None:\n",
        "        sigma = np.std(np.concatenate((x, y), axis=0))\n",
        "        if sigma == 0:\n",
        "            sigma = 1.0\n",
        "\n",
        "    # Compute the Gram matrices using RBF kernel\n",
        "    K = rbf_kernel(x, gamma=1.0/(2*sigma**2))\n",
        "    L = rbf_kernel(y, gamma=1.0/(2*sigma**2))\n",
        "    \n",
        "    # Center the Gram matrices\n",
        "    H = np.eye(n) - np.ones((n, n))/n\n",
        "    Kc = H @ K @ H\n",
        "    Lc = H @ L @ H\n",
        "    \n",
        "    # Compute HSIC\n",
        "    hsic_value = (1/(n-1)**2) * np.trace(Kc @ Lc)\n",
        "    return hsic_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "086680b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def label(adjacency_matrix):\n",
        "    \"\"\"\n",
        "    Given a graph as adjacency_matrix, create the class labels of each variable.\n",
        "    \"\"\"\n",
        "\n",
        "    adjacency_graph, adjacency_label = create_graph_label()\n",
        "    labels = get_labels(adjacency_matrix, adjacency_label)\n",
        "    variables = adjacency_matrix.columns.drop([\"X\", \"Y\"])\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"variable\": variables,\n",
        "        \"label\": [labels[variable] for variable in variables],\n",
        "    })\n",
        "    df[\"dataset\"] = adjacency_matrix.name\n",
        "\n",
        "    # Reorder columns:\n",
        "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "174d66b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_some_columns(names_datasets, function):\n",
        "    \"\"\"\n",
        "    Apply an embedding function to a list of datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    df = []\n",
        "    for name, dataset in tqdm(names_datasets.items()):\n",
        "        dataset = names_datasets[name]\n",
        "        dataset.name = name\n",
        "    \n",
        "        try:\n",
        "            df_dataset = function(dataset)\n",
        "        except ValueError as e:\n",
        "            print(name, e)\n",
        "            raise NotImplementedError\n",
        "\n",
        "        df_dataset[\"dataset\"] = name\n",
        "        df.append(df_dataset)\n",
        "\n",
        "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d5d95f91",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_some_columns_parallel(names_datasets, function, n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Apply an embedding function to a list of datasets.\n",
        "\n",
        "    Parallel version.\n",
        "    \"\"\"\n",
        "\n",
        "    def f(name, dataset, function):\n",
        "        dataset.name = name\n",
        "        df_dataset = function(dataset)\n",
        "        df_dataset[\"dataset\"] = name\n",
        "        return df_dataset\n",
        "\n",
        "    df = joblib.Parallel(n_jobs=n_jobs)(\n",
        "        joblib.delayed(f)(name, dataset, function)\n",
        "        for name, dataset in tqdm(names_datasets.items())\n",
        "    )\n",
        "\n",
        "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f344d206",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_all_columns(functions_names_datasets, n_jobs=-1, create_dimension_feature = False):\n",
        "    \"\"\"\n",
        "    given a dictionary of {function1:names, function2:names,...} apply\n",
        "    the desired functions to the list of datasets and merge all of them\n",
        "    in a single X_y_group dataframe.\n",
        "    \"\"\"\n",
        "\n",
        "    columns = []\n",
        "    if create_dimension_feature:\n",
        "        dimension_feature = create_some_columns(functions_names_datasets[list(functions_names_datasets.keys())[0]], add_dimension_feature)\n",
        "        columns.append(dimension_feature)\n",
        "    \n",
        "    for function, names_datasets in functions_names_datasets.items():\n",
        "        print(f\"set: {function.__name__}\")\n",
        "\n",
        "        if n_jobs != 1:\n",
        "            feature_set = create_some_columns_parallel(names_datasets, function, n_jobs=n_jobs)\n",
        "        else:\n",
        "            feature_set = create_some_columns(names_datasets, function)\n",
        "\n",
        "        columns.append(feature_set)\n",
        "\n",
        "    # Merge all feature sets into a single dataframe:\n",
        "    columns = functools.reduce(\n",
        "        lambda left, right: pd.merge(left, right, on=[\"dataset\", \"variable\"]),\n",
        "        columns,\n",
        "    )\n",
        "\n",
        "    return columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d2a15476",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment what you need!\n",
        "def train(\n",
        "    X_train: typing.Dict[str, pd.DataFrame],\n",
        "    y_train: typing.Dict[str, pd.DataFrame],\n",
        "    # number_of_features: int,\n",
        "    model_directory_path: str,\n",
        "    # id_column_name: str,\n",
        "    # prediction_column_name: str,\n",
        "    # has_gpu: bool,\n",
        ") -> None:\n",
        "    # 1) Create the variable embedding:\n",
        "    X_y_group_train_pathname = \"/tmp/X_y_group_train.parquet\"\n",
        "    try:\n",
        "        print(f\"Loading {X_y_group_train_pathname}\")\n",
        "        X_y_group_train = pd.read_parquet(X_y_group_train_pathname)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Creating X_y_group_train\")\n",
        "        names_datasets_train = X_train\n",
        "        names_graphs_train = y_train\n",
        "        X_y_group_train = create_all_columns(\n",
        "            {\n",
        "                pearson_correlation: names_datasets_train,\n",
        "                # ttest: names_datasets_train,\n",
        "                mutual_information: names_datasets_train, \n",
        "                label: names_graphs_train,\n",
        "                spearman_correlation: names_datasets_train,\n",
        "                kendall_correlation: names_datasets_train,\n",
        "                distance_correlation: names_datasets_train,\n",
        "                conditional_mutual_information: names_datasets_train,\n",
        "                partial_correlation: names_datasets_train,\n",
        "                linear_regression_feature: names_datasets_train,\n",
        "                ridge_regression_feature: names_datasets_train,\n",
        "            },\n",
        "            n_jobs=-1,\n",
        "            create_dimension_feature=True,\n",
        "        )\n",
        "        X_y_group_train['MI(v,X)^2'] = X_y_group_train['MI(v,X)'] ** 2\n",
        "        X_y_group_train['MI(v,Y)^2'] = X_y_group_train['MI(v,Y)'] ** 2\n",
        "        X_y_group_train['MI(X,Y)^2'] = X_y_group_train['MI(X,Y)'] ** 2\n",
        "        X_y_group_train['max(MI(v, others))^2'] = X_y_group_train['max(MI(v, others))'] ** 2\n",
        "        X_y_group_train['min(MI(v, others))^2'] = X_y_group_train['min(MI(v, others))'] ** 2\n",
        "        \n",
        "        blacklist = [\"ttest(v,X)\", \"pvalue(ttest(v,X))<=0.05\", \"ttest(v,Y)\", \"pvalue(ttest(v,Y))<=0.05\", \"ttest(X,Y)\", \"pvalue(ttest(X,Y))<=0.05\"]\n",
        "        columns_to_drop = [col for col in blacklist if col in X_y_group_train.columns]\n",
        "        X_y_group_train = X_y_group_train.drop(columns=columns_to_drop)\n",
        "\n",
        "        # 2) Massage X_y_group_train to prepare what is needed by the model:\n",
        "        print(\"Adding numeric labels y\") # sklearn wants numeric labels\n",
        "        le = LabelEncoder()\n",
        "        le.classes_ = np.array([\n",
        "            'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
        "            'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
        "        ])\n",
        "        X_y_group_train[\"y\"] = le.transform(X_y_group_train[\"label\"])\n",
        "\n",
        "        # reordering columns:\n",
        "        X_y_group_train = X_y_group_train[[\"dataset\", \"variable\"] + X_y_group_train.columns.drop([\"dataset\", \"variable\", \"label\", \"y\"]).tolist() + [\"label\", \"y\"]]\n",
        "        display(X_y_group_train)\n",
        "\n",
        "        print(\"Saving X_y_group_train\")\n",
        "        os.makedirs(os.path.dirname(X_y_group_train_pathname), exist_ok=True)\n",
        "        X_y_group_train.to_parquet(X_y_group_train_pathname)\n",
        "\n",
        "    print(\"Extracting X_train, y_train, and group\")\n",
        "    X_train = X_y_group_train.drop([\"variable\", \"dataset\", \"label\", \"y\"], axis=\"columns\")\n",
        "    y_train = X_y_group_train[\"y\"]\n",
        "    group_train = X_y_group_train[\"dataset\"]\n",
        "\n",
        "    # Model:\n",
        "    model = RandomForestClassifier(n_estimators=100, max_depth=13, n_jobs=-1, class_weight=\"balanced\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    joblib.dump(\n",
        "        model,\n",
        "        os.path.join(model_directory_path, \"model.joblib\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "221c521a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_submission(X_y_pred_test):\n",
        "    \"\"\"\n",
        "    From the predicted test set, for each dataset, take predicted\n",
        "    classes of all variables, create the adjacency matrix, then create\n",
        "    the submission in the requested format.\n",
        "    \"\"\"\n",
        "\n",
        "    submission = {}\n",
        "    for name, prediction in tqdm(X_y_pred_test.groupby(\"dataset\"), delay=10):\n",
        "        variables_labels = prediction[[\"variable\", \"label_predicted\"]].set_index(\"variable\")\n",
        "        variables = variables_labels.index.tolist()\n",
        "        variables_all = [\"X\", \"Y\"] + variables\n",
        "\n",
        "        adjacency_matrix = pd.DataFrame(index=variables_all, columns=variables_all)\n",
        "        adjacency_matrix.index.name = \"parent\"\n",
        "        adjacency_matrix[:] = 0\n",
        "        adjacency_matrix.loc[\"X\", \"Y\"] = 1\n",
        "\n",
        "        for v in variables:\n",
        "            l = variables_labels.loc[v].item()\n",
        "            if l == \"Cause of X\":\n",
        "                adjacency_matrix.loc[v, \"X\"] = 1\n",
        "            elif l == \"Cause of Y\":\n",
        "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
        "            elif l == \"Consequence of X\":\n",
        "                adjacency_matrix.loc[\"X\", v] = 1\n",
        "            elif l == \"Consequence of Y\":\n",
        "                adjacency_matrix.loc[\"Y\", v] = 1\n",
        "            elif l == \"Confounder\":\n",
        "                adjacency_matrix.loc[v, \"X\"] = 1\n",
        "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
        "            elif l == \"Collider\":\n",
        "                adjacency_matrix.loc[\"X\", v] = 1\n",
        "                adjacency_matrix.loc[\"Y\", v] = 1\n",
        "            elif l == \"Mediator\":\n",
        "                adjacency_matrix.loc[\"X\", v] = 1\n",
        "                adjacency_matrix.loc[v, \"Y\"] = 1\n",
        "            elif l == \"Confounder\":\n",
        "                pass\n",
        "\n",
        "        for i in variables_all:\n",
        "            for j in variables_all:\n",
        "                submission[f'{name}_{i}_{j}'] = int(adjacency_matrix.loc[i, j])\n",
        "\n",
        "    return submission\n",
        "\n",
        "\n",
        "# Uncomment what you need!\n",
        "def infer(\n",
        "    X_test: typing.Dict[str, pd.DataFrame],\n",
        "    # number_of_features: int,\n",
        "    model_directory_path: str,\n",
        "    id_column_name: str,\n",
        "    prediction_column_name: str,\n",
        "    # has_gpu: bool,\n",
        "    # has_trained: bool,\n",
        ") -> pd.DataFrame:\n",
        "    model = joblib.load(os.path.join(model_directory_path, \"model.joblib\"))\n",
        "\n",
        "    names_datasets_test = X_test\n",
        "    X_group_test = create_all_columns(\n",
        "        {\n",
        "            pearson_correlation: names_datasets_test,\n",
        "            # ttest: names_datasets_test,\n",
        "            mutual_information: names_datasets_test,  \n",
        "            spearman_correlation: names_datasets_test,\n",
        "            kendall_correlation: names_datasets_test,\n",
        "            distance_correlation: names_datasets_test,\n",
        "            conditional_mutual_information: names_datasets_test,\n",
        "            partial_correlation: names_datasets_test,\n",
        "            linear_regression_feature: names_datasets_test,\n",
        "            ridge_regression_feature: names_datasets_test,\n",
        "            },\n",
        "            n_jobs=-1,\n",
        "            create_dimension_feature=True,\n",
        "        )\n",
        "    X_group_test['MI(v,X)^2'] = X_group_test['MI(v,X)'] ** 2\n",
        "    X_group_test['MI(v,Y)^2'] = X_group_test['MI(v,Y)'] ** 2\n",
        "    X_group_test['MI(X,Y)^2'] = X_group_test['MI(X,Y)'] ** 2\n",
        "    X_group_test['max(MI(v, others))^2'] = X_group_test['max(MI(v, others))'] ** 2\n",
        "    X_group_test['min(MI(v, others))^2'] = X_group_test['min(MI(v, others))'] ** 2\n",
        "    \n",
        "    blacklist = [\"ttest(v,X)\", \"pvalue(ttest(v,X))<=0.05\", \"ttest(v,Y)\", \"pvalue(ttest(v,Y))<=0.05\", \"ttest(X,Y)\", \"pvalue(ttest(X,Y))<=0.05\"]\n",
        "    columns_to_drop = [col for col in blacklist if col in X_group_test.columns]\n",
        "    X_group_test = X_group_test.drop(columns=columns_to_drop)\n",
        "    \n",
        "    \n",
        "    X_test = X_group_test.drop(columns=[\"dataset\", \"variable\"])\n",
        "    y_predicted = model.predict(X_test)\n",
        "    X_y_pred_test = X_group_test\n",
        "    X_y_pred_test[\"y_predicted\"] = y_predicted\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    le.classes_ = np.array([\n",
        "        'Cause of X', 'Consequence of X', 'Confounder', 'Collider',\n",
        "        'Mediator', 'Independent', 'Cause of Y', 'Consequence of Y',\n",
        "    ])\n",
        "\n",
        "    X_y_pred_test[\"label_predicted\"] = le.inverse_transform(y_predicted)\n",
        "\n",
        "    submission = create_submission(X_y_pred_test)\n",
        "\n",
        "    return pd.DataFrame(\n",
        "        submission.items(),\n",
        "        columns=[\n",
        "            id_column_name,\n",
        "            prediction_column_name\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5fceff69-b855-4146-8c57-b32d0ecb666e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672,
          "referenced_widgets": [
            "5d4bf0130ac44e16a1d4370808370166",
            "da29edc9799d4e75aa295b2bd55a1b51",
            "a415eaf9f34a44668631676733ac54e0",
            "d78da2e4bc954c22bea79589fdf9dd48",
            "2d48ee12333d41dfa160bcc8e76d9f60",
            "3199aedf32034fe8b1af3314619b6adc",
            "3b80fc748a7a4e5ab2540c4aee162fbe",
            "f9007189f8be400883429ec748b879e7",
            "d9f86e16a9db4c78b7fa3d5bca8e033f",
            "ce75f0f9d92c4480b9a584319a8d76d8",
            "1b12c1b71ab64191a1ee6508179012e3",
            "477a34c495084301b7a1954ce704aebc",
            "d77bc032f7fb458b8acc60d26b8875b6",
            "26a1e0060aee4841921b8358153e6795",
            "cb0a36f6e0c048759f6f77a6db163e27",
            "08c97be01f87441580c9d6a97a199bc2",
            "af557519c67b413aa9f16a5665054cde",
            "248587de4d264e258321fcca56216cfb",
            "03270cd004de4c50bde33929d94babc4",
            "59200470439b44b1a55723c1786517fe",
            "cf4e1aa461404d2c8f63b25a50ae2513",
            "cfb1d1f914b9415284b39aeacbe632e3"
          ]
        },
        "id": "5fceff69-b855-4146-8c57-b32d0ecb666e",
        "outputId": "78d88eeb-80a6-4872-d674-6e63981f31e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m18:42:17\u001b[0m \u001b[33mno forbidden library found\u001b[0m\n",
            "\u001b[32m18:42:17\u001b[0m \u001b[33m\u001b[0m\n",
            "\u001b[32m18:42:17\u001b[0m started\n",
            "\u001b[32m18:42:17\u001b[0m running local test\n",
            "\u001b[32m18:42:17\u001b[0m \u001b[33minternet access isn't restricted, no check will be done\u001b[0m\n",
            "\u001b[32m18:42:17\u001b[0m \n",
            "\u001b[32m18:42:24\u001b[0m starting dag process...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "download data\\X_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_train.pickle (1523944532 bytes)\n",
            "already exists: file length match\n",
            "download data\\y_train.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_train.pickle (7017735 bytes)\n",
            "already exists: file length match\n",
            "download data\\X_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/X_test_reduced.pickle (122341879 bytes)\n",
            "already exists: file length match\n",
            "download data\\y_test.pickle from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/y_test_reduced.pickle (562930 bytes)\n",
            "already exists: file length match\n",
            "download data\\example_prediction.parquet from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/48/example_prediction_reduced.parquet (668981 bytes)\n",
            "already exists: file length match\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m18:42:34\u001b[0m \u001b[33mcall: train\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading /tmp/X_y_group_train.parquet\n",
            "Creating X_y_group_train\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 23500/23500 [00:12<00:00, 1815.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set: pearson_correlation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 23500/23500 [00:47<00:00, 499.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set: mutual_information\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 23500/23500 [20:01<00:00, 19.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set: label\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 23500/23500 [00:41<00:00, 570.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding numeric labels y\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>variable</th>\n",
              "      <th>dimension</th>\n",
              "      <th>square_dimension</th>\n",
              "      <th>corr(v,X)</th>\n",
              "      <th>corr(v,Y)</th>\n",
              "      <th>max(corr(v, others))</th>\n",
              "      <th>min(corr(v, others))</th>\n",
              "      <th>mean(corr(v, others))</th>\n",
              "      <th>std(corr(v, others))</th>\n",
              "      <th>corr(X,Y)</th>\n",
              "      <th>MI(v,X)</th>\n",
              "      <th>MI(v,Y)</th>\n",
              "      <th>max(MI(v, others))</th>\n",
              "      <th>min(MI(v, others))</th>\n",
              "      <th>mean(MI(v, others))</th>\n",
              "      <th>std(MI(v, others))</th>\n",
              "      <th>MI(X,Y)</th>\n",
              "      <th>label</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00000</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>64</td>\n",
              "      <td>0.169735</td>\n",
              "      <td>-0.113595</td>\n",
              "      <td>0.791467</td>\n",
              "      <td>0.027355</td>\n",
              "      <td>0.282376</td>\n",
              "      <td>0.298969</td>\n",
              "      <td>-0.771058</td>\n",
              "      <td>0.051660</td>\n",
              "      <td>0.060295</td>\n",
              "      <td>0.527389</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.138799</td>\n",
              "      <td>0.175843</td>\n",
              "      <td>0.442316</td>\n",
              "      <td>Consequence of Y</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00000</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>64</td>\n",
              "      <td>-0.072334</td>\n",
              "      <td>0.139419</td>\n",
              "      <td>0.230139</td>\n",
              "      <td>0.003233</td>\n",
              "      <td>0.090511</td>\n",
              "      <td>0.082201</td>\n",
              "      <td>-0.771058</td>\n",
              "      <td>0.015770</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062381</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015224</td>\n",
              "      <td>0.020654</td>\n",
              "      <td>0.442316</td>\n",
              "      <td>Independent</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00000</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>64</td>\n",
              "      <td>0.123115</td>\n",
              "      <td>-0.238769</td>\n",
              "      <td>0.781051</td>\n",
              "      <td>0.012225</td>\n",
              "      <td>0.257037</td>\n",
              "      <td>0.288165</td>\n",
              "      <td>-0.771058</td>\n",
              "      <td>0.031141</td>\n",
              "      <td>0.058973</td>\n",
              "      <td>0.506139</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.115655</td>\n",
              "      <td>0.176491</td>\n",
              "      <td>0.442316</td>\n",
              "      <td>Cause of Y</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00000</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>64</td>\n",
              "      <td>-0.001935</td>\n",
              "      <td>0.013921</td>\n",
              "      <td>0.147408</td>\n",
              "      <td>0.001935</td>\n",
              "      <td>0.052451</td>\n",
              "      <td>0.053457</td>\n",
              "      <td>-0.771058</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028844</td>\n",
              "      <td>0.056019</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012733</td>\n",
              "      <td>0.018254</td>\n",
              "      <td>0.442316</td>\n",
              "      <td>Cause of Y</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00000</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>64</td>\n",
              "      <td>0.284323</td>\n",
              "      <td>-0.466570</td>\n",
              "      <td>0.791467</td>\n",
              "      <td>0.064815</td>\n",
              "      <td>0.426306</td>\n",
              "      <td>0.250048</td>\n",
              "      <td>-0.771058</td>\n",
              "      <td>0.063658</td>\n",
              "      <td>0.160992</td>\n",
              "      <td>0.528119</td>\n",
              "      <td>0.027708</td>\n",
              "      <td>0.188844</td>\n",
              "      <td>0.180402</td>\n",
              "      <td>0.442316</td>\n",
              "      <td>Mediator</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142905</th>\n",
              "      <td>46997</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>49</td>\n",
              "      <td>0.014740</td>\n",
              "      <td>0.697089</td>\n",
              "      <td>0.748635</td>\n",
              "      <td>0.013858</td>\n",
              "      <td>0.376630</td>\n",
              "      <td>0.352655</td>\n",
              "      <td>-0.089641</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.412607</td>\n",
              "      <td>0.622771</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.397121</td>\n",
              "      <td>0.243529</td>\n",
              "      <td>0.057772</td>\n",
              "      <td>Cause of Y</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142906</th>\n",
              "      <td>46997</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>49</td>\n",
              "      <td>-0.007397</td>\n",
              "      <td>-0.116481</td>\n",
              "      <td>0.998843</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.180741</td>\n",
              "      <td>0.333308</td>\n",
              "      <td>-0.089641</td>\n",
              "      <td>0.041594</td>\n",
              "      <td>0.056474</td>\n",
              "      <td>2.958672</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.464312</td>\n",
              "      <td>0.962749</td>\n",
              "      <td>0.057772</td>\n",
              "      <td>Cause of Y</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142907</th>\n",
              "      <td>46997</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>49</td>\n",
              "      <td>-0.011658</td>\n",
              "      <td>0.908029</td>\n",
              "      <td>0.970364</td>\n",
              "      <td>0.011658</td>\n",
              "      <td>0.469777</td>\n",
              "      <td>0.441105</td>\n",
              "      <td>-0.089641</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.864494</td>\n",
              "      <td>1.352074</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.459673</td>\n",
              "      <td>0.486585</td>\n",
              "      <td>0.057772</td>\n",
              "      <td>Cause of Y</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142908</th>\n",
              "      <td>46998</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0.083546</td>\n",
              "      <td>-0.019665</td>\n",
              "      <td>0.083546</td>\n",
              "      <td>0.016856</td>\n",
              "      <td>0.040022</td>\n",
              "      <td>0.037719</td>\n",
              "      <td>0.036862</td>\n",
              "      <td>0.064410</td>\n",
              "      <td>0.003847</td>\n",
              "      <td>0.064658</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022676</td>\n",
              "      <td>0.029718</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Consequence of X</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142909</th>\n",
              "      <td>46998</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.201922</td>\n",
              "      <td>0.666742</td>\n",
              "      <td>0.666742</td>\n",
              "      <td>0.016856</td>\n",
              "      <td>0.295173</td>\n",
              "      <td>0.334828</td>\n",
              "      <td>0.036862</td>\n",
              "      <td>0.081206</td>\n",
              "      <td>0.316117</td>\n",
              "      <td>0.315320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.132756</td>\n",
              "      <td>0.133460</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Collider</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>142910 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       dataset variable  dimension  square_dimension  corr(v,X)  corr(v,Y)  \\\n",
              "0        00000        0          8                64   0.169735  -0.113595   \n",
              "1        00000        2          8                64  -0.072334   0.139419   \n",
              "2        00000        3          8                64   0.123115  -0.238769   \n",
              "3        00000        4          8                64  -0.001935   0.013921   \n",
              "4        00000        5          8                64   0.284323  -0.466570   \n",
              "...        ...      ...        ...               ...        ...        ...   \n",
              "142905   46997        6          7                49   0.014740   0.697089   \n",
              "142906   46997        7          7                49  -0.007397  -0.116481   \n",
              "142907   46997        8          7                49  -0.011658   0.908029   \n",
              "142908   46998        0          2                 4   0.083546  -0.019665   \n",
              "142909   46998        3          2                 4  -0.201922   0.666742   \n",
              "\n",
              "        max(corr(v, others))  min(corr(v, others))  mean(corr(v, others))  \\\n",
              "0                   0.791467              0.027355               0.282376   \n",
              "1                   0.230139              0.003233               0.090511   \n",
              "2                   0.781051              0.012225               0.257037   \n",
              "3                   0.147408              0.001935               0.052451   \n",
              "4                   0.791467              0.064815               0.426306   \n",
              "...                      ...                   ...                    ...   \n",
              "142905              0.748635              0.013858               0.376630   \n",
              "142906              0.998843              0.007397               0.180741   \n",
              "142907              0.970364              0.011658               0.469777   \n",
              "142908              0.083546              0.016856               0.040022   \n",
              "142909              0.666742              0.016856               0.295173   \n",
              "\n",
              "        std(corr(v, others))  corr(X,Y)   MI(v,X)   MI(v,Y)  \\\n",
              "0                   0.298969  -0.771058  0.051660  0.060295   \n",
              "1                   0.082201  -0.771058  0.015770  0.000000   \n",
              "2                   0.288165  -0.771058  0.031141  0.058973   \n",
              "3                   0.053457  -0.771058  0.000000  0.028844   \n",
              "4                   0.250048  -0.771058  0.063658  0.160992   \n",
              "...                      ...        ...       ...       ...   \n",
              "142905              0.352655  -0.089641  0.000000  0.412607   \n",
              "142906              0.333308  -0.089641  0.041594  0.056474   \n",
              "142907              0.441105  -0.089641  0.000000  0.864494   \n",
              "142908              0.037719   0.036862  0.064410  0.003847   \n",
              "142909              0.334828   0.036862  0.081206  0.316117   \n",
              "\n",
              "        max(MI(v, others))  min(MI(v, others))  mean(MI(v, others))  \\\n",
              "0                 0.527389            0.000000             0.138799   \n",
              "1                 0.062381            0.000000             0.015224   \n",
              "2                 0.506139            0.000000             0.115655   \n",
              "3                 0.056019            0.000000             0.012733   \n",
              "4                 0.528119            0.027708             0.188844   \n",
              "...                    ...                 ...                  ...   \n",
              "142905            0.622771            0.000000             0.397121   \n",
              "142906            2.958672            0.000000             0.464312   \n",
              "142907            1.352074            0.000000             0.459673   \n",
              "142908            0.064658            0.000000             0.022676   \n",
              "142909            0.315320            0.000000             0.132756   \n",
              "\n",
              "        std(MI(v, others))   MI(X,Y)             label  y  \n",
              "0                 0.175843  0.442316  Consequence of Y  7  \n",
              "1                 0.020654  0.442316       Independent  5  \n",
              "2                 0.176491  0.442316        Cause of Y  6  \n",
              "3                 0.018254  0.442316        Cause of Y  6  \n",
              "4                 0.180402  0.442316          Mediator  4  \n",
              "...                    ...       ...               ... ..  \n",
              "142905            0.243529  0.057772        Cause of Y  6  \n",
              "142906            0.962749  0.057772        Cause of Y  6  \n",
              "142907            0.486585  0.057772        Cause of Y  6  \n",
              "142908            0.029718  0.000000  Consequence of X  1  \n",
              "142909            0.133460  0.000000          Collider  3  \n",
              "\n",
              "[142910 rows x 20 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving X_y_group_train\n",
            "Extracting X_train, y_train, and group\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m19:04:40\u001b[0m \u001b[33mcall: infer\u001b[0m\n",
            "100%|██████████| 1880/1880 [00:00<00:00, 1909.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set: pearson_correlation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1880/1880 [00:04<00:00, 466.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set: mutual_information\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1880/1880 [01:36<00:00, 19.54it/s]\n",
            "\u001b[32m19:06:25\u001b[0m \u001b[33mduration - time=00:24:08\u001b[0m\n",
            "\u001b[32m19:06:25\u001b[0m \u001b[33mmemory - before=\"1.99 GB\" after=\"2.44 GB\" consumed=\"458.40 MB\"\u001b[0m\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- MI(X,Y)^2\n- MI(v,X)^2\n- MI(v,Y)^2\n- conditional_MI(X,Y|v)\n- conditional_MI(v,X|Y)\n- ...\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcrunch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_determinism_check\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload this notebook and submit it to the platform: https://hub.crunchdao.com/competitions/causality-discovery/submit/via/notebook\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\inline.py:69\u001b[0m, in \u001b[0;36m_Inline.test\u001b[1;34m(self, force_first_train, train_frequency, raise_abort, round_number, no_checks, no_determinism_check, read_kwargs, write_kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     library\u001b[38;5;241m.\u001b[39mscan(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule)\n\u001b[0;32m     67\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_first_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mround_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompetition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mno_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mno_determinism_check\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m click\u001b[38;5;241m.\u001b[39mAbort \u001b[38;5;28;01mas\u001b[39;00m abort:\n\u001b[0;32m     83\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAborted!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\tester.py:52\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(module, model_directory_path, force_first_train, train_frequency, round_number, competition_format, has_gpu, checks, determinism_check_enabled, read_kwargs, write_kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocalRunner\n\u001b[0;32m     38\u001b[0m runner \u001b[38;5;241m=\u001b[39m LocalRunner(\n\u001b[0;32m     39\u001b[0m     module,\n\u001b[0;32m     40\u001b[0m     model_directory_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     write_kwargs,\n\u001b[0;32m     50\u001b[0m )\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\runner\\local.py:47\u001b[0m, in \u001b[0;36mLocalRunner.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration - time=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     51\u001b[0m         time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mgmtime(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start))\n\u001b[0;32m     52\u001b[0m     )\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\runner\\runner.py:37\u001b[0m, in \u001b[0;36mRunner.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompetition_format \u001b[38;5;241m==\u001b[39m api\u001b[38;5;241m.\u001b[39mCompetitionFormat\u001b[38;5;241m.\u001b[39mDAG:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting dag process...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_dag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompetition_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\runner\\runner.py:90\u001b[0m, in \u001b[0;36mRunner.start_dag\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_dag\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 90\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdag_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic:\n\u001b[0;32m     93\u001b[0m         prediction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdag_loop(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\runner\\local.py:206\u001b[0m, in \u001b[0;36mLocalRunner.dag_loop\u001b[1;34m(self, train)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall: infer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 206\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m     ensure\u001b[38;5;241m.\u001b[39mreturn_infer(\n\u001b[0;32m    212\u001b[0m         prediction,\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;241m.\u001b[39moutputs,\n\u001b[0;32m    216\u001b[0m     )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prediction\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\crunch\\utils.py:231\u001b[0m, in \u001b[0;36msmart_call\u001b[1;34m(function, default_values, specific_values, log)\u001b[0m\n\u001b[0;32m    228\u001b[0m     debug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    229\u001b[0m     arguments[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marguments)\n",
            "Cell \u001b[1;32mIn[11], line 72\u001b[0m, in \u001b[0;36minfer\u001b[1;34m(X_test, model_directory_path, id_column_name, prediction_column_name)\u001b[0m\n\u001b[0;32m     61\u001b[0m X_group_test \u001b[38;5;241m=\u001b[39m create_all_columns(\n\u001b[0;32m     62\u001b[0m     {\n\u001b[0;32m     63\u001b[0m         pearson_correlation: names_datasets_test,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     create_dimension_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     69\u001b[0m )\n\u001b[0;32m     71\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_group_test\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 72\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m X_y_pred_test \u001b[38;5;241m=\u001b[39m X_group_test\n\u001b[0;32m     74\u001b[0m X_y_pred_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_predicted\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m y_predicted\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:904\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    884\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 904\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:946\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    944\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    945\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 946\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    949\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:641\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    639\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 641\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
            "File \u001b[1;32me:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
            "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- MI(X,Y)^2\n- MI(v,X)^2\n- MI(v,Y)^2\n- conditional_MI(X,Y|v)\n- conditional_MI(v,X|Y)\n- ...\n"
          ]
        }
      ],
      "source": [
        "crunch.test(\n",
        "    no_determinism_check=True\n",
        ")\n",
        "\n",
        "print(\"Download this notebook and submit it to the platform: https://hub.crunchdao.com/competitions/causality-discovery/submit/via/notebook\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03270cd004de4c50bde33929d94babc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08c97be01f87441580c9d6a97a199bc2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b12c1b71ab64191a1ee6508179012e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "248587de4d264e258321fcca56216cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26a1e0060aee4841921b8358153e6795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03270cd004de4c50bde33929d94babc4",
            "max": 1880,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59200470439b44b1a55723c1786517fe",
            "value": 1880
          }
        },
        "2d48ee12333d41dfa160bcc8e76d9f60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3199aedf32034fe8b1af3314619b6adc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b80fc748a7a4e5ab2540c4aee162fbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "477a34c495084301b7a1954ce704aebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d77bc032f7fb458b8acc60d26b8875b6",
              "IPY_MODEL_26a1e0060aee4841921b8358153e6795",
              "IPY_MODEL_cb0a36f6e0c048759f6f77a6db163e27"
            ],
            "layout": "IPY_MODEL_08c97be01f87441580c9d6a97a199bc2"
          }
        },
        "59200470439b44b1a55723c1786517fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d4bf0130ac44e16a1d4370808370166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da29edc9799d4e75aa295b2bd55a1b51",
              "IPY_MODEL_a415eaf9f34a44668631676733ac54e0",
              "IPY_MODEL_d78da2e4bc954c22bea79589fdf9dd48"
            ],
            "layout": "IPY_MODEL_2d48ee12333d41dfa160bcc8e76d9f60"
          }
        },
        "a415eaf9f34a44668631676733ac54e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9007189f8be400883429ec748b879e7",
            "max": 1880,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9f86e16a9db4c78b7fa3d5bca8e033f",
            "value": 1880
          }
        },
        "af557519c67b413aa9f16a5665054cde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0a36f6e0c048759f6f77a6db163e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf4e1aa461404d2c8f63b25a50ae2513",
            "placeholder": "​",
            "style": "IPY_MODEL_cfb1d1f914b9415284b39aeacbe632e3",
            "value": " 1880/1880 [00:10&lt;00:00, 206.39it/s]"
          }
        },
        "ce75f0f9d92c4480b9a584319a8d76d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf4e1aa461404d2c8f63b25a50ae2513": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb1d1f914b9415284b39aeacbe632e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d77bc032f7fb458b8acc60d26b8875b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af557519c67b413aa9f16a5665054cde",
            "placeholder": "​",
            "style": "IPY_MODEL_248587de4d264e258321fcca56216cfb",
            "value": "100%"
          }
        },
        "d78da2e4bc954c22bea79589fdf9dd48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce75f0f9d92c4480b9a584319a8d76d8",
            "placeholder": "​",
            "style": "IPY_MODEL_1b12c1b71ab64191a1ee6508179012e3",
            "value": " 1880/1880 [00:46&lt;00:00, 47.91it/s]"
          }
        },
        "d9f86e16a9db4c78b7fa3d5bca8e033f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da29edc9799d4e75aa295b2bd55a1b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3199aedf32034fe8b1af3314619b6adc",
            "placeholder": "​",
            "style": "IPY_MODEL_3b80fc748a7a4e5ab2540c4aee162fbe",
            "value": "100%"
          }
        },
        "f9007189f8be400883429ec748b879e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
