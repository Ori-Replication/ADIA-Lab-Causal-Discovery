{
        "cells": [
                {
                        "cell_type": "code",
                        "execution_count": 1,
                        "metadata": {},
                        "outputs": [
                                {
                                        "data": {
                                                "text/plain": [
                                                        "'\\n                       _oo0oo_\\n                      o8888888o\\n                      88\" . \"88\\n                      (| -_- |)\\n                      0\\\\  =  /0\\n                    ___/`---\\'\\\\___\\n                  .\\' \\\\|     |// \\'.\\n                 / \\\\|||  :  |||//                 / _||||| -:- |||||-                |   | \\\\\\\\  - /// |   |\\n               | \\\\_|  \\'\\'\\\\---/\\'\\'  |_/ |\\n               \\\\  .-\\\\__  \\'-\\'  ___/-. /\\n             ___\\'. .\\'  /--.--\\\\  `. .\\'___\\n          .\"\" \\'<  `.___\\\\_<|>_/___.\\' >\\' \"\".\\n         | | :  `- \\\\`.;`\\\\ _ /`;.`/ - ` : | |\\n         \\\\  \\\\ `_.   \\\\_ __\\\\ /__ _/   .-` /  /\\n     =====`-.____`.___ \\\\_____/___.-`___.-\\'=====\\n                       `=---=\\'\\n\\n\\n     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\n           佛祖保佑     永不宕机     永无BUG\\n'"
                                                ]
                                        },
                                        "execution_count": 1,
                                        "metadata": {},
                                        "output_type": "execute_result"
                                }
                        ],
                        "source": [
                                "'''\n",
                                "                       _oo0oo_\n",
                                "                      o8888888o\n",
                                "                      88\" . \"88\n",
                                "                      (| -_- |)\n",
                                "                      0\\  =  /0\n",
                                "                    ___/`---'\\___\n",
                                "                  .' \\\\|     |// '.\n",
                                "                 / \\\\|||  :  |||// \\\n",
                                "                / _||||| -:- |||||- \\\n",
                                "               |   | \\\\\\  - /// |   |\n",
                                "               | \\_|  ''\\---/''  |_/ |\n",
                                "               \\  .-\\__  '-'  ___/-. /\n",
                                "             ___'. .'  /--.--\\  `. .'___\n",
                                "          .\"\" '<  `.___\\_<|>_/___.' >' \"\".\n",
                                "         | | :  `- \\`.;`\\ _ /`;.`/ - ` : | |\n",
                                "         \\  \\ `_.   \\_ __\\ /__ _/   .-` /  /\n",
                                "     =====`-.____`.___ \\_____/___.-`___.-'=====\n",
                                "                       `=---='\n",
                                "\n",
                                "\n",
                                "     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
                                "\n",
                                "           佛祖保佑     永不宕机     永无BUG\n",
                                "'''"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 2,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-21T11:20:39.508775Z",
                                        "start_time": "2024-09-21T11:20:32.677062Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [
                                {
                                        "name": "stderr",
                                        "output_type": "stream",
                                        "text": [
                                                "e:\\Softwares\\Anaconda\\CondaEnvs\\casual\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                                                "  from .autonotebook import tqdm as notebook_tqdm\n",
                                                "2024-10-21 16:18:15,694\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
                                        ]
                                }
                        ],
                        "source": [
                                "import functools\n",
                                "import os\n",
                                "import typing\n",
                                "import random\n",
                                "import joblib\n",
                                "import dcor\n",
                                "import math\n",
                                "import networkx as nx\n",
                                "import numpy as np\n",
                                "import pandas as pd\n",
                                "import pingouin as pg\n",
                                "from tqdm.auto import tqdm\n",
                                "import scipy.stats as stats\n",
                                "import scipy.special as special\n",
                                "import statsmodels.api as sm\n",
                                "import statsmodels.formula.api as smf\n",
                                "from sklearn.ensemble import RandomForestClassifier\n",
                                "from sklearn.feature_selection import mutual_info_regression\n",
                                "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
                                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                                "from sklearn.linear_model import Ridge, LinearRegression, LassoCV\n",
                                "from sklearn.metrics import adjusted_mutual_info_score\n",
                                "from collections import Counter, defaultdict\n",
                                "from econml.dml import CausalForestDML, LinearDML\n",
                                "from causallearn.graph.GraphNode import GraphNode\n",
                                "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge\n",
                                "from causallearn.search.ConstraintBased.PC import pc\n",
                                "from causallearn.search.ScoreBased.ExactSearch import bic_exact_search\n",
                                "from causallearn.search.ConstraintBased.FCI import fci\n",
                                "from causallearn.search.PermutationBased.GRaSP import grasp\n",
                                "import semopy\n",
                                "from semopy import Model\n",
                                "from semopy.inspector import inspect"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 3,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "import warnings\n",
                                "warnings.filterwarnings('ignore', category=DeprecationWarning)   # 忽略PPS的警告"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 4,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-20T02:42:13.047359Z",
                                        "start_time": "2024-09-20T02:42:12.325055Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [
                                {
                                        "name": "stdout",
                                        "output_type": "stream",
                                        "text": [
                                                "loaded inline runner with module: <module '__main__'>\n"
                                        ]
                                }
                        ],
                        "source": [
                                "import crunch\n",
                                "crunch = crunch.load_notebook()"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 5,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:30:54.182971Z",
                                        "start_time": "2024-09-18T09:30:54.145678Z"
                                }
                        },
                        "outputs": [],
                        "source": [
                                "def graph_nodes_representation(graph, nodelist):\n",
                                "    \"\"\"\n",
                                "    Create an alternative representation of a graph which is hashable\n",
                                "    and equivalent graphs have the same hash.\n",
                                "\n",
                                "    Python cannot PROPERLY use nx.Graph/DiGraph as key for\n",
                                "    dictionaries, because two equivalent graphs with just different\n",
                                "    order of the nodes would result in different keys. This is\n",
                                "    undesirable here.\n",
                                "\n",
                                "    So here we transform the graph into an equivalent form that is\n",
                                "    based on a specific nodelist and that is hashable. In this way,\n",
                                "    two equivalent graphs, once transformed, will result in identical\n",
                                "    keys.\n",
                                "\n",
                                "    So we use the following trick: extract the adjacency matrix\n",
                                "    (with nodes in a fixed order) and then make a hashable thing out\n",
                                "    of it, through tuple(array.flatten()):\n",
                                "    \"\"\"\n",
                                "\n",
                                "    # This get the adjacency matrix with nodes in a given order, as\n",
                                "    # numpy array (which is not hashable):\n",
                                "    adjacency_matrix = nx.adjacency_matrix(graph, nodelist=nodelist).todense()\n",
                                "\n",
                                "    # This transforms the numpy array into a hashable object:\n",
                                "    hashable = tuple(adjacency_matrix.flatten())\n",
                                "\n",
                                "    return hashable\n",
                                "\n",
                                "def create_graph_label():\n",
                                "    \"\"\"\n",
                                "    Create a dictionary from graphs to labels, in two formats.\n",
                                "    \"\"\"\n",
                                "    graph_label = {\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\"), (\"v\", \"Y\")]): \"Confounder\",\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"Y\", \"v\")]): \"Collider\",\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\"), (\"v\", \"Y\")]): \"Mediator\",\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"X\")]):             \"Cause of X\",\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"v\", \"Y\")]):             \"Cause of Y\",\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"X\", \"v\")]):             \"Consequence of X\",\n",
                                "        nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"v\")]):             \"Consequence of Y\",\n",
                                "        nx.DiGraph({\"X\": [\"Y\"], \"v\": []}):                \"Independent\",\n",
                                "    }\n",
                                "\n",
                                "    nodelist = [\"v\", \"X\", \"Y\"]\n",
                                "\n",
                                "    # This is an equivalent alternative to graph_label but in a form\n",
                                "    # for which two equivalent graphs have the same key:\n",
                                "    adjacency_label = {\n",
                                "        graph_nodes_representation(graph, nodelist): label\n",
                                "        for graph, label in graph_label.items()\n",
                                "    }\n",
                                "\n",
                                "    return graph_label, adjacency_label\n",
                                "\n",
                                "def get_labels(adjacency_matrix, adjacency_label):\n",
                                "    \"\"\"\n",
                                "    Transform an adjacency_matrix (as pd.DataFrame) into a dictionary of variable:label\n",
                                "    \"\"\"\n",
                                "\n",
                                "    result = {}\n",
                                "    for variable in adjacency_matrix.columns.drop([\"X\", \"Y\"]):\n",
                                "        submatrix = adjacency_matrix.loc[[variable, \"X\", \"Y\"], [variable, \"X\", \"Y\"]]  # this is not hashable\n",
                                "        key = tuple(submatrix.values.flatten())  # this is hashable and a compatible with adjacency_label\n",
                                "    \n",
                                "        result[variable] = adjacency_label[key]\n",
                                "\n",
                                "    return result\n",
                                "\n",
                                "graph_label, adjacency_label = create_graph_label()"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 6,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "def apply_mapping(df, mapping):\n",
                                "    df_new = df.copy()\n",
                                "    # 创建临时映射以避免冲突\n",
                                "    temp_mapping = {k: f'_temp_{k}' for k in mapping.keys()}\n",
                                "    df_new.rename(columns=temp_mapping, inplace=True)\n",
                                "    if df_new.shape[0] == df_new.shape[1]:  # 如果是方阵，如标签矩阵\n",
                                "        df_new.rename(index=temp_mapping, inplace=True)\n",
                                "    # 应用最终映射\n",
                                "    final_mapping = {f'_temp_{k}': v for k, v in mapping.items()}\n",
                                "    df_new.rename(columns=final_mapping, inplace=True)\n",
                                "    if df_new.shape[0] == df_new.shape[1]:\n",
                                "        df_new.rename(index=final_mapping, inplace=True)\n",
                                "    return df_new\n",
                                "\n",
                                "def check_duplicate_columns(df):\n",
                                "    \"\"\"检查是否存在重复的列名\"\"\"\n",
                                "    return df.columns.duplicated().any()\n",
                                "\n",
                                "def augment_data(X_train, y_train):\n",
                                "    new_X_train = X_train.copy()\n",
                                "    new_y_train = y_train.copy()\n",
                                "    for sample_id in X_train.keys():\n",
                                "        X = X_train[sample_id]\n",
                                "        y = y_train[sample_id]\n",
                                "        variables = list(X.columns)\n",
                                "        dim = len(variables)\n",
                                "        # 提取因果关系对\n",
                                "        edges = []\n",
                                "        for u in y.index:\n",
                                "            for v in y.columns:\n",
                                "                if y.loc[u, v] == 1:\n",
                                "                    edges.append((u, v))\n",
                                "        # 排除涉及 X 和 Y 的边\n",
                                "        edges_no_XY = [(u, v) for (u, v) in edges if u not in ['X', 'Y'] and v not in ['X', 'Y']]\n",
                                "        if dim >= 4:\n",
                                "            edges_to_use = edges_no_XY\n",
                                "            attempts = 0\n",
                                "            success = False\n",
                                "            while attempts < 3 and not success:\n",
                                "                if not edges_to_use:\n",
                                "                    break  # 没有合适的边，跳出循环\n",
                                "                u, v = random.choice(edges_to_use)\n",
                                "                mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
                                "                # 应用映射到特征矩阵和标签矩阵\n",
                                "                X_new = apply_mapping(X, mapping)\n",
                                "                y_new = apply_mapping(y, mapping)\n",
                                "                # 检查特征矩阵是否有重复列\n",
                                "                if check_duplicate_columns(X_new):\n",
                                "                    attempts += 1\n",
                                "                    continue  # 重试\n",
                                "                else:\n",
                                "                    # 没有重复列，存储新的数据\n",
                                "                    new_sample_id = '0' + sample_id\n",
                                "                    new_X_train[new_sample_id] = X_new\n",
                                "                    new_y_train[new_sample_id] = y_new\n",
                                "                    success = True\n",
                                "            if not success:\n",
                                "                # 没有找到合适的映射，复制原始数据\n",
                                "                new_sample_id = '0' + sample_id\n",
                                "                new_X_train[new_sample_id] = X.copy()\n",
                                "                new_y_train[new_sample_id] = y.copy()\n",
                                "        else:\n",
                                "            # 对于维度较低的数据，允许涉及 X 和 Y 的边\n",
                                "            edges_to_use = edges\n",
                                "            if not edges_to_use:\n",
                                "                # 没有边，复制原始数据\n",
                                "                new_sample_id = '0' + sample_id\n",
                                "                new_X_train[new_sample_id] = X.copy()\n",
                                "                new_y_train[new_sample_id] = y.copy()\n",
                                "                continue\n",
                                "            u, v = random.choice(edges_to_use)\n",
                                "            mapping = {'X': u, 'Y': v, u: 'X', v: 'Y'}\n",
                                "            # 应用映射到特征矩阵和标签矩阵\n",
                                "            X_new = apply_mapping(X, mapping)\n",
                                "            y_new = apply_mapping(y, mapping)\n",
                                "            # 检查特征矩阵是否有重复列\n",
                                "            if check_duplicate_columns(X_new):\n",
                                "                # 如果有重复列，复制原始数据\n",
                                "                new_sample_id = '0' + sample_id\n",
                                "                new_X_train[new_sample_id] = X.copy()\n",
                                "                new_y_train[new_sample_id] = y.copy()\n",
                                "            else:\n",
                                "                # 没有重复列，存储新的数据\n",
                                "                new_sample_id = '0' + sample_id\n",
                                "                new_X_train[new_sample_id] = X_new\n",
                                "                new_y_train[new_sample_id] = y_new\n",
                                "    return new_X_train, new_y_train"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 7,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-20T02:43:04.532840Z",
                                        "start_time": "2024-09-20T02:42:51.402011Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [
                                {
                                        "name": "stdout",
                                        "output_type": "stream",
                                        "text": [
                                                "23500 23500\n"
                                        ]
                                }
                        ],
                        "source": [
                                "# X_train, y_train, X_test = crunch.load_data()\n",
                                "X_train = pd.read_pickle('./data/X_train.pickle')\n",
                                "y_train = pd.read_pickle('./data/y_train.pickle')\n",
                                "print(len(X_train), len(y_train))\n",
                                "# X_train, y_train = augment_data(X_train, y_train)"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 8,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "\"\"\"Copula Entropy的工具函数\"\"\"\n",
                                "###  Cite: https://github.com/majianthu/pycopent\n",
                                "from scipy.special import digamma\n",
                                "from scipy.stats import rankdata as rank \n",
                                "from scipy.spatial.distance import cdist\n",
                                "from math import gamma, log, pi\n",
                                "from numpy import array, abs, max, hstack, vstack, ones, zeros, cov, matrix, where\n",
                                "from numpy.random import uniform, normal as rnorm\n",
                                "from numpy.linalg import det\n",
                                "from multiprocessing.pool import Pool,ThreadPool\n",
                                "import sys\n",
                                "\n",
                                "##### constructing empirical copula density [1]\n",
                                "def construct_empirical_copula(x):\n",
                                "\t(N,d) = x.shape\t\n",
                                "\txc = zeros([N,d]) \n",
                                "\tfor i in range(0,d):\n",
                                "\t\txc[:,i] = rank(x[:,i]) / N\n",
                                "\t\n",
                                "\treturn xc\n",
                                "##### Estimating entropy with kNN method [2]\n",
                                "def entknn(x, k = 3, dtype = 'chebychev'):\n",
                                "\t(N,d) = x.shape\n",
                                "\t\n",
                                "\tg1 = digamma(N) - digamma(k)\n",
                                "\t\n",
                                "\tif dtype == 'euclidean':\n",
                                "\t\tcd = pi**(d/2) / 2**d / gamma(1+d/2)\n",
                                "\telse:\t# (chebychev) maximum distance\n",
                                "\t\tcd = 1;\n",
                                "\n",
                                "\tlogd = 0\n",
                                "\tdists = cdist(x, x, dtype)\n",
                                "\tdists.sort()\n",
                                "\tfor i in range(0,N):\n",
                                "\t\tlogd = logd + log( 2 * dists[i,k] ) * d / N\n",
                                "\n",
                                "\treturn (g1 + log(cd) + logd)\n",
                                "##### 2-step Nonparametric estimation of copula entropy [1]\n",
                                "def copent(x, k = 3, dtype = 'chebychev', log0 = False):\n",
                                "\txarray = array(x)\n",
                                "\n",
                                "\tif log0:\n",
                                "\t\t(N,d) = xarray.shape\n",
                                "\t\tmax1 = max(abs(xarray), axis = 0)\n",
                                "\t\tfor i in range(0,d):\n",
                                "\t\t\tif max1[i] == 0:\n",
                                "\t\t\t\txarray[:,i] = rnorm(0,1,N)\n",
                                "\t\t\telse:\n",
                                "\t\t\t\txarray[:,i] = xarray[:,i] + rnorm(0,1,N) * max1[i] * 0.000005\n",
                                "\n",
                                "\txc = construct_empirical_copula(xarray)\n",
                                "\n",
                                "\ttry:\n",
                                "\t\treturn -entknn(xc, k, dtype)\n",
                                "\texcept ValueError: # log0 error\n",
                                "\t\treturn copent(x, k, dtype, log0 = True)\n",
                                "\n",
                                "##### conditional independence test [3]\n",
                                "##### to test independence of (x,y) conditioned on z\n",
                                "def ci(x, y, z, k = 3, dtype = 'chebychev'):\n",
                                "\txyz = vstack((x,y,z)).T\n",
                                "\tyz = vstack((y,z)).T\n",
                                "\txz = vstack((x,z)).T\n",
                                "\treturn copent(xyz,k,dtype) - copent(yz,k,dtype) - copent(xz,k,dtype)\n",
                                "##### estimating transfer entropy from y to x with lag [3]\n",
                                "def transent(x, y, lag = 1, k = 3, dtype = 'chebychev'):\n",
                                "\txlen = len(x)\n",
                                "\tylen = len(y)\n",
                                "\tif (xlen > ylen):\n",
                                "\t\tl = ylen\n",
                                "\telse:\n",
                                "\t\tl = xlen\n",
                                "\tif (l < (lag + k + 1)):\n",
                                "\t\treturn 0\n",
                                "\tx1 = x[0:(l-lag)]\n",
                                "\tx2 = x[lag:l]\n",
                                "\ty = y[0:(l-lag)]\n",
                                "\treturn ci(x2,y,x1,k,dtype)\n",
                                "##### multivariate normality test [4]\n",
                                "def mvnt(x, k = 3, dtype = 'chebychev'):\n",
                                "\treturn -0.5 * log(det(cov(x.T))) - copent(x,k,dtype)\n",
                                "##### two-sample test [5]\n",
                                "def tst(s0,s1,n=12, k = 3, dtype = 'chebychev'):\n",
                                "\t(N0,d0) = s0.shape\n",
                                "\t(N1,d1) = s1.shape\n",
                                "\tx = vstack((s0,s1))\n",
                                "\tstat1 = 0\n",
                                "\tfor i in range(0,n):\n",
                                "\t\ty1 = vstack((ones([N0,1]),ones([N1,1])*2)) + uniform(0, 0.0000001,[N0+N1,1])\n",
                                "\t\ty0 = ones([N0+N1,1]) + uniform(0,0.0000001,[N0+N1,1])\n",
                                "\t\tstat1 = stat1 + copent(hstack((x,y1)),k,dtype) - copent(hstack((x,y0)),k,dtype)\n",
                                "\treturn stat1/n\n",
                                "##### single change point detection [6]\n",
                                "def init(X,N,K,DTYPE):\n",
                                "\tglobal x,n,k,dtype\n",
                                "\tx = X\n",
                                "\tn = N\n",
                                "\tk = K\n",
                                "\tdtype = DTYPE\n",
                                "\n",
                                "def tsti(i):\n",
                                "\ts0 = x[0:(i+1),:]\n",
                                "\ts1 = x[(i+2):,:]\n",
                                "\treturn tst(s0,s1,n,k,dtype)\n",
                                "\t\n",
                                "def cpd(x, thd = 0.13, n = 30, k = 3, dtype = 'chebychev'):\n",
                                "\tx = matrix(x)\n",
                                "\tlen1 = x.shape[0]\n",
                                "\tif len1 == 1:\n",
                                "\t\tlen1 = x.shape[1]\n",
                                "\t\tx = x.T\n",
                                "\tpos = -1\n",
                                "\tmaxstat = 0\n",
                                "\tif sys.platform.startswith(\"win\"): # \"win\"\n",
                                "\t\tpool = ThreadPool(initializer = init, initargs=(x,n,k,dtype))\n",
                                "\telse: # \"linux\" or \"darwin\"\n",
                                "\t\tpool = Pool(initializer = init, initargs=(x,n,k,dtype))\n",
                                "\tstat1 = [0] + pool.map(tsti,range(len1-2)) + [0]\n",
                                "\tpool.close()\n",
                                "\tif(max(stat1) > thd):\n",
                                "\t\tmaxstat = max(stat1)\n",
                                "\t\tpos = where(stat1 == maxstat)[0][0]+1\n",
                                "\treturn pos, maxstat, stat1\n",
                                "##### multiple change point detection [6]\n",
                                "def mcpd(x, maxp = 5, thd = 0.13, minseglen = 10, n = 30, k = 3, dtype = 'chebychev'):\n",
                                "\tx = matrix(x)\n",
                                "\tlen1 = x.shape[0]\n",
                                "\tif len1 == 1:\n",
                                "\t\tlen1 = x.shape[1]\n",
                                "\t\tx = x.T\n",
                                "\tmaxstat = []\n",
                                "\tpos = []\n",
                                "\tbisegs = matrix([0,len1-1])\n",
                                "\tfor i in range(0,maxp):\n",
                                "\t\tif i >= bisegs.shape[0]:\n",
                                "\t\t\tbreak\n",
                                "\t\trpos, rmaxstat, _ = cpd(x[bisegs[i,0]:bisegs[i,1],:],thd,n,k,dtype)\n",
                                "\t\tif rpos > -1 :\n",
                                "\t\t\trpos = rpos + bisegs[i,0]\n",
                                "\t\t\tmaxstat.append(rmaxstat)\n",
                                "\t\t\tpos.append(rpos)\n",
                                "\t\t\tif (rpos - bisegs[i,0]) > minseglen :\n",
                                "\t\t\t\tbisegs = vstack((bisegs,[bisegs[i,0],rpos-1]))\n",
                                "\t\t\tif (bisegs[i,1] - rpos +1) > minseglen :\n",
                                "\t\t\t\tbisegs = vstack((bisegs,[rpos,bisegs[i,1]]))\n",
                                "\treturn pos,maxstat"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 9,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "\"\"\"PPS的工具函数\"\"\"\n",
                                "### Cite: https://github.com/8080labs/ppscore/\n",
                                "\n",
                                "from sklearn import tree\n",
                                "from sklearn import preprocessing\n",
                                "from sklearn.model_selection import cross_val_score\n",
                                "from sklearn.metrics import mean_absolute_error, f1_score\n",
                                "\n",
                                "from pandas.api.types import (\n",
                                "    is_numeric_dtype,\n",
                                "    is_bool_dtype,\n",
                                "    is_object_dtype,\n",
                                "    is_categorical_dtype,\n",
                                "    is_string_dtype,\n",
                                "    is_datetime64_any_dtype,\n",
                                "    is_timedelta64_dtype,\n",
                                ")\n",
                                "\n",
                                "def _calculate_model_cv_score_(\n",
                                "    df, target, feature, task, cross_validation, random_seed, **kwargs\n",
                                "):\n",
                                "    \"Calculates the mean model score based on cross-validation\"\n",
                                "    # Sources about the used methods:\n",
                                "    # https://scikit-learn.org/stable/modules/tree.html\n",
                                "    # https://scikit-learn.org/stable/modules/cross_validation.html\n",
                                "    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
                                "    metric = task[\"metric_key\"]\n",
                                "    model = task[\"model\"]\n",
                                "    # shuffle the rows - this is important for cross-validation\n",
                                "    # because the cross-validation just takes the first n lines\n",
                                "    # if there is a strong pattern in the rows eg 0,0,0,0,1,1,1,1\n",
                                "    # then this will lead to problems because the first cv sees mostly 0 and the later 1\n",
                                "    # this approach might be wrong for timeseries because it might leak information\n",
                                "    df = df.sample(frac=1, random_state=random_seed, replace=False)\n",
                                "\n",
                                "    # preprocess target\n",
                                "    if task[\"type\"] == \"classification\":\n",
                                "        label_encoder = preprocessing.LabelEncoder()\n",
                                "        df[target] = label_encoder.fit_transform(df[target])\n",
                                "        target_series = df[target]\n",
                                "    else:\n",
                                "        target_series = df[target]\n",
                                "\n",
                                "    # preprocess feature\n",
                                "    if _dtype_represents_categories(df[feature]):\n",
                                "        one_hot_encoder = preprocessing.OneHotEncoder()\n",
                                "        array = df[feature].__array__()\n",
                                "        sparse_matrix = one_hot_encoder.fit_transform(array.reshape(-1, 1))\n",
                                "        feature_input = sparse_matrix\n",
                                "    else:\n",
                                "        # reshaping needed because there is only 1 feature\n",
                                "        array = df[feature].values\n",
                                "        if not isinstance(array, np.ndarray):  # e.g Int64 IntegerArray\n",
                                "            array = array.to_numpy()\n",
                                "        feature_input = array.reshape(-1, 1)\n",
                                "\n",
                                "    # Cross-validation is stratifiedKFold for classification, KFold for regression\n",
                                "    # CV on one core (n_job=1; default) has shown to be fastest\n",
                                "    scores = cross_val_score(\n",
                                "        model, feature_input, target_series.to_numpy(), cv=cross_validation, scoring=metric\n",
                                "    )\n",
                                "\n",
                                "    return scores.mean()\n",
                                "\n",
                                "def _normalized_mae_score(model_mae, naive_mae):\n",
                                "    \"Normalizes the model MAE score, given the baseline score\"\n",
                                "    # # Value range of MAE is [0, infinity), 0 is best\n",
                                "    # 10, 5 ==> 0 because worse than naive\n",
                                "    # 10, 20 ==> 0.5\n",
                                "    # 5, 20 ==> 0.75 = 1 - (mae/base_mae)\n",
                                "    if model_mae > naive_mae:\n",
                                "        return 0\n",
                                "    else:\n",
                                "        return 1 - (model_mae / naive_mae)\n",
                                "\n",
                                "def _mae_normalizer(df, y, model_score, **kwargs):\n",
                                "    \"In case of MAE, calculates the baseline score for y and derives the PPS.\"\n",
                                "    df[\"naive\"] = df[y].median()\n",
                                "    baseline_score = mean_absolute_error(df[y].to_numpy(), df[\"naive\"].to_numpy())  # true, pred\n",
                                "\n",
                                "    ppscore = _normalized_mae_score(abs(model_score), baseline_score)\n",
                                "    return ppscore, baseline_score\n",
                                "\n",
                                "def _normalized_f1_score(model_f1, baseline_f1):\n",
                                "    \"Normalizes the model F1 score, given the baseline score\"\n",
                                "    # # F1 ranges from 0 to 1\n",
                                "    # # 1 is best\n",
                                "    # 0.5, 0.7 ==> 0 because model is worse than naive baseline\n",
                                "    # 0.75, 0.5 ==> 0.5\n",
                                "    #\n",
                                "    if model_f1 < baseline_f1:\n",
                                "        return 0\n",
                                "    else:\n",
                                "        scale_range = 1.0 - baseline_f1  # eg 0.3\n",
                                "        f1_diff = model_f1 - baseline_f1  # eg 0.1\n",
                                "        return f1_diff / scale_range  # 0.1/0.3 = 0.33\n",
                                "\n",
                                "def _f1_normalizer(df, y, model_score, random_seed):\n",
                                "    \"In case of F1, calculates the baseline score for y and derives the PPS.\"\n",
                                "    label_encoder = preprocessing.LabelEncoder()\n",
                                "    df[\"truth\"] = label_encoder.fit_transform(df[y])\n",
                                "    df[\"most_common_value\"] = df[\"truth\"].value_counts().index[0]\n",
                                "    random = df[\"truth\"].sample(frac=1, random_state=random_seed)\n",
                                "\n",
                                "    baseline_score = max(\n",
                                "        f1_score(df[\"truth\"], df[\"most_common_value\"], average=\"weighted\"),\n",
                                "        f1_score(df[\"truth\"], random, average=\"weighted\"),\n",
                                "    )\n",
                                "\n",
                                "    ppscore = _normalized_f1_score(model_score, baseline_score)\n",
                                "    return ppscore, baseline_score\n",
                                "\n",
                                "def _dtype_represents_categories(series) -> bool:\n",
                                "    \"Determines if the dtype of the series represents categorical values\"\n",
                                "    return (\n",
                                "        is_bool_dtype(series)\n",
                                "        or is_object_dtype(series)\n",
                                "        or is_string_dtype(series)\n",
                                "        or is_categorical_dtype(series)\n",
                                "    )\n",
                                "\n",
                                "def _determine_case_and_prepare_df(df, x, y, sample=5_000, random_seed=123):\n",
                                "    \"Returns str with the name of the determined case based on the columns x and y\"\n",
                                "    if x == y:\n",
                                "        return df, \"predict_itself\"\n",
                                "\n",
                                "    df = df[[x, y]]\n",
                                "    # IDEA: log.warning when values have been dropped\n",
                                "    df = df.dropna()\n",
                                "\n",
                                "    if len(df) == 0:\n",
                                "        return df, \"empty_dataframe_after_dropping_na\"\n",
                                "        # IDEA: show warning\n",
                                "        # raise Exception(\n",
                                "        #     \"After dropping missing values, there are no valid rows left\"\n",
                                "        # )\n",
                                "\n",
                                "    df = _maybe_sample(df, sample, random_seed=random_seed)\n",
                                "\n",
                                "    if _feature_is_id(df, x):\n",
                                "        return df, \"feature_is_id\"\n",
                                "\n",
                                "    category_count = df[y].value_counts().count()\n",
                                "    if category_count == 1:\n",
                                "        # it is helpful to separate this case in order to save unnecessary calculation time\n",
                                "        return df, \"target_is_constant\"\n",
                                "    if _dtype_represents_categories(df[y]) and (category_count == len(df[y])):\n",
                                "        # it is important to separate this case in order to save unnecessary calculation time\n",
                                "        return df, \"target_is_id\"\n",
                                "\n",
                                "    if _dtype_represents_categories(df[y]):\n",
                                "        return df, \"classification\"\n",
                                "    if is_numeric_dtype(df[y]):\n",
                                "        # this check needs to be after is_bool_dtype (which is part of _dtype_represents_categories) because bool is considered numeric by pandas\n",
                                "        return df, \"regression\"\n",
                                "\n",
                                "    if is_datetime64_any_dtype(df[y]) or is_timedelta64_dtype(df[y]):\n",
                                "        # IDEA: show warning\n",
                                "        # raise TypeError(\n",
                                "        #     f\"The target column {y} has the dtype {df[y].dtype} which is not supported. A possible solution might be to convert {y} to a string column\"\n",
                                "        # )\n",
                                "        return df, \"target_is_datetime\"\n",
                                "\n",
                                "    # IDEA: show warning\n",
                                "    # raise Exception(\n",
                                "    #     f\"Could not infer a valid task based on the target {y}. The dtype {df[y].dtype} is not yet supported\"\n",
                                "    # )  # pragma: no cover\n",
                                "    return df, \"target_data_type_not_supported\"\n",
                                "\n",
                                "def _feature_is_id(df, x):\n",
                                "    \"Returns Boolean if the feature column x is an ID\"\n",
                                "    if not _dtype_represents_categories(df[x]):\n",
                                "        return False\n",
                                "\n",
                                "    category_count = df[x].value_counts().count()\n",
                                "    return category_count == len(df[x])\n",
                                "\n",
                                "def _maybe_sample(df, sample, random_seed=None):\n",
                                "    \"\"\"\n",
                                "    Maybe samples the rows of the given df to have at most `sample` rows\n",
                                "    If sample is `None` or falsy, there will be no sampling.\n",
                                "    If the df has fewer rows than the sample, there will be no sampling.\n",
                                "\n",
                                "    Parameters\n",
                                "    ----------\n",
                                "    df : pandas.DataFrame\n",
                                "        Dataframe that might be sampled\n",
                                "    sample : int or `None`\n",
                                "        Number of rows to be sampled\n",
                                "    random_seed : int or `None`\n",
                                "        Random seed that is forwarded to pandas.DataFrame.sample as `random_state`\n",
                                "\n",
                                "    Returns\n",
                                "    -------\n",
                                "    pandas.DataFrame\n",
                                "        DataFrame after potential sampling\n",
                                "    \"\"\"\n",
                                "    if sample and len(df) > sample:\n",
                                "        # this is a problem if x or y have more than sample=5000 categories\n",
                                "        # TODO: dont sample when the problem occurs and show warning\n",
                                "        df = df.sample(sample, random_state=random_seed, replace=False)\n",
                                "    return df\n",
                                "\n",
                                "def _is_column_in_df(column, df):\n",
                                "    try:\n",
                                "        return column in df.columns\n",
                                "    except:\n",
                                "        return False\n",
                                "\n",
                                "def _score(\n",
                                "    df, x, y, task, sample, cross_validation, random_seed, invalid_score, catch_errors\n",
                                "):\n",
                                "    df, case_type = _determine_case_and_prepare_df(\n",
                                "        df, x, y, sample=sample, random_seed=random_seed\n",
                                "    )\n",
                                "    task = _get_task(case_type, invalid_score)\n",
                                "\n",
                                "    if case_type in [\"classification\", \"regression\"]:\n",
                                "        model_score = _calculate_model_cv_score_(\n",
                                "            df,\n",
                                "            target=y,\n",
                                "            feature=x,\n",
                                "            task=task,\n",
                                "            cross_validation=cross_validation,\n",
                                "            random_seed=random_seed,\n",
                                "        )\n",
                                "        # IDEA: the baseline_scores do sometimes change significantly, e.g. for F1 and thus change the PPS\n",
                                "        # we might want to calculate the baseline_score 10 times and use the mean in order to have less variance\n",
                                "        ppscore, baseline_score = task[\"score_normalizer\"](\n",
                                "            df, y, model_score, random_seed=random_seed\n",
                                "        )\n",
                                "    else:\n",
                                "        model_score = task[\"model_score\"]\n",
                                "        baseline_score = task[\"baseline_score\"]\n",
                                "        ppscore = task[\"ppscore\"]\n",
                                "\n",
                                "    return {\n",
                                "        \"x\": x,\n",
                                "        \"y\": y,\n",
                                "        \"ppscore\": ppscore,\n",
                                "        \"case\": case_type,\n",
                                "        \"is_valid_score\": task[\"is_valid_score\"],\n",
                                "        \"metric\": task[\"metric_name\"],\n",
                                "        \"baseline_score\": baseline_score,\n",
                                "        \"model_score\": abs(model_score),  # sklearn returns negative mae\n",
                                "        \"model\": task[\"model\"],\n",
                                "    }\n",
                                "\n",
                                "def pps_score(\n",
                                "    df,\n",
                                "    x,\n",
                                "    y,\n",
                                "    task=\"NOT_SUPPORTED_ANYMORE\",\n",
                                "    sample=5_000,\n",
                                "    cross_validation=4,\n",
                                "    random_seed=123,\n",
                                "    invalid_score=0,\n",
                                "    catch_errors=True,\n",
                                "):\n",
                                "    \"\"\"\n",
                                "    Calculate the Predictive Power Score (PPS) for \"x predicts y\"\n",
                                "    The score always ranges from 0 to 1 and is data-type agnostic.\n",
                                "\n",
                                "    A score of 0 means that the column x cannot predict the column y better than a naive baseline model.\n",
                                "    A score of 1 means that the column x can perfectly predict the column y given the model.\n",
                                "    A score between 0 and 1 states the ratio of how much potential predictive power the model achieved compared to the baseline model.\n",
                                "\n",
                                "    Parameters\n",
                                "    ----------\n",
                                "    df : pandas.DataFrame\n",
                                "        Dataframe that contains the columns x and y\n",
                                "    x : str\n",
                                "        Name of the column x which acts as the feature\n",
                                "    y : str\n",
                                "        Name of the column y which acts as the target\n",
                                "    sample : int or `None`\n",
                                "        Number of rows for sampling. The sampling decreases the calculation time of the PPS.\n",
                                "        If `None` there will be no sampling.\n",
                                "    cross_validation : int\n",
                                "        Number of iterations during cross-validation. This has the following implications:\n",
                                "        For example, if the number is 4, then it is possible to detect patterns when there are at least 4 times the same observation. If the limit is increased, the required minimum observations also increase. This is important, because this is the limit when sklearn will throw an error and the PPS cannot be calculated\n",
                                "    random_seed : int or `None`\n",
                                "        Random seed for the parts of the calculation that require random numbers, e.g. shuffling or sampling.\n",
                                "        If the value is set, the results will be reproducible. If the value is `None` a new random number is drawn at the start of each calculation.\n",
                                "    invalid_score : any\n",
                                "        The score that is returned when a calculation is invalid, e.g. because the data type was not supported.\n",
                                "    catch_errors : bool\n",
                                "        If `True` all errors will be catched and reported as `unknown_error` which ensures convenience. If `False` errors will be raised. This is helpful for inspecting and debugging errors.\n",
                                "\n",
                                "    Returns\n",
                                "    -------\n",
                                "    Dict\n",
                                "        A dict that contains multiple fields about the resulting PPS.\n",
                                "        The dict enables introspection into the calculations that have been performed under the hood\n",
                                "    \"\"\"\n",
                                "\n",
                                "    if not isinstance(df, pd.DataFrame):\n",
                                "        raise TypeError(\n",
                                "            f\"The 'df' argument should be a pandas.DataFrame but you passed a {type(df)}\\nPlease convert your input to a pandas.DataFrame\"\n",
                                "        )\n",
                                "    if not _is_column_in_df(x, df):\n",
                                "        raise ValueError(\n",
                                "            f\"The 'x' argument should be the name of a dataframe column but the variable that you passed is not a column in the given dataframe.\\nPlease review the column name or your dataframe\"\n",
                                "        )\n",
                                "    if len(df[[x]].columns) >= 2:\n",
                                "        raise AssertionError(\n",
                                "            f\"The dataframe has {len(df[[x]].columns)} columns with the same column name {x}\\nPlease adjust the dataframe and make sure that only 1 column has the name {x}\"\n",
                                "        )\n",
                                "    if not _is_column_in_df(y, df):\n",
                                "        raise ValueError(\n",
                                "            f\"The 'y' argument should be the name of a dataframe column but the variable that you passed is not a column in the given dataframe.\\nPlease review the column name or your dataframe\"\n",
                                "        )\n",
                                "    if len(df[[y]].columns) >= 2:\n",
                                "        raise AssertionError(\n",
                                "            f\"The dataframe has {len(df[[y]].columns)} columns with the same column name {y}\\nPlease adjust the dataframe and make sure that only 1 column has the name {y}\"\n",
                                "        )\n",
                                "\n",
                                "    if random_seed is None:\n",
                                "        from random import random\n",
                                "\n",
                                "        random_seed = int(random() * 1000)\n",
                                "\n",
                                "    try:\n",
                                "        return _score(\n",
                                "            df,\n",
                                "            x,\n",
                                "            y,\n",
                                "            task,\n",
                                "            sample,\n",
                                "            cross_validation,\n",
                                "            random_seed,\n",
                                "            invalid_score,\n",
                                "            catch_errors,\n",
                                "        )\n",
                                "    except Exception as exception:\n",
                                "        if catch_errors:\n",
                                "            case_type = \"unknown_error\"\n",
                                "            task = _get_task(case_type, invalid_score)\n",
                                "            return {\n",
                                "                \"x\": x,\n",
                                "                \"y\": y,\n",
                                "                \"ppscore\": task[\"ppscore\"],\n",
                                "                \"case\": case_type,\n",
                                "                \"is_valid_score\": task[\"is_valid_score\"],\n",
                                "                \"metric\": task[\"metric_name\"],\n",
                                "                \"baseline_score\": task[\"baseline_score\"],\n",
                                "                \"model_score\": task[\"model_score\"],  # sklearn returns negative mae\n",
                                "                \"model\": task[\"model\"],\n",
                                "            }\n",
                                "        else:\n",
                                "            raise exception\n",
                                "\n",
                                "def _get_task(case_type, invalid_score):\n",
                                "    VALID_CALCULATIONS = {\n",
                                "        \"regression\": {\n",
                                "            \"type\": \"regression\",\n",
                                "            \"is_valid_score\": True,\n",
                                "            \"model_score\": -1,\n",
                                "            \"baseline_score\": -1,\n",
                                "            \"ppscore\": -1,\n",
                                "            \"metric_name\": \"mean absolute error\",\n",
                                "            \"metric_key\": \"neg_mean_absolute_error\",\n",
                                "            \"model\": tree.DecisionTreeRegressor(),\n",
                                "            \"score_normalizer\": _mae_normalizer,\n",
                                "        },\n",
                                "        \"classification\": {\n",
                                "            \"type\": \"classification\",\n",
                                "            \"is_valid_score\": True,\n",
                                "            \"model_score\": -1,\n",
                                "            \"baseline_score\": -1,\n",
                                "            \"ppscore\": -1,\n",
                                "            \"metric_name\": \"weighted F1\",\n",
                                "            \"metric_key\": \"f1_weighted\",\n",
                                "            \"model\": tree.DecisionTreeClassifier(),\n",
                                "            \"score_normalizer\": _f1_normalizer,\n",
                                "        },\n",
                                "        \"predict_itself\": {\n",
                                "            \"type\": \"predict_itself\",\n",
                                "            \"is_valid_score\": True,\n",
                                "            \"model_score\": 1,\n",
                                "            \"baseline_score\": 0,\n",
                                "            \"ppscore\": 1,\n",
                                "            \"metric_name\": None,\n",
                                "            \"metric_key\": None,\n",
                                "            \"model\": None,\n",
                                "            \"score_normalizer\": None,\n",
                                "        },\n",
                                "        \"target_is_constant\": {\n",
                                "            \"type\": \"target_is_constant\",\n",
                                "            \"is_valid_score\": True,\n",
                                "            \"model_score\": 1,\n",
                                "            \"baseline_score\": 1,\n",
                                "            \"ppscore\": 0,\n",
                                "            \"metric_name\": None,\n",
                                "            \"metric_key\": None,\n",
                                "            \"model\": None,\n",
                                "            \"score_normalizer\": None,\n",
                                "        },\n",
                                "        \"target_is_id\": {\n",
                                "            \"type\": \"target_is_id\",\n",
                                "            \"is_valid_score\": True,\n",
                                "            \"model_score\": 0,\n",
                                "            \"baseline_score\": 0,\n",
                                "            \"ppscore\": 0,\n",
                                "            \"metric_name\": None,\n",
                                "            \"metric_key\": None,\n",
                                "            \"model\": None,\n",
                                "            \"score_normalizer\": None,\n",
                                "        },\n",
                                "        \"feature_is_id\": {\n",
                                "            \"type\": \"feature_is_id\",\n",
                                "            \"is_valid_score\": True,\n",
                                "            \"model_score\": 0,\n",
                                "            \"baseline_score\": 0,\n",
                                "            \"ppscore\": 0,\n",
                                "            \"metric_name\": None,\n",
                                "            \"metric_key\": None,\n",
                                "            \"model\": None,\n",
                                "            \"score_normalizer\": None,\n",
                                "        },\n",
                                "    }\n",
                                "    INVALID_CALCULATIONS = [\n",
                                "        \"target_is_datetime\",\n",
                                "        \"target_data_type_not_supported\",\n",
                                "        \"empty_dataframe_after_dropping_na\",\n",
                                "        \"unknown_error\",\n",
                                "    ]\n",
                                "\n",
                                "    if case_type in VALID_CALCULATIONS.keys():\n",
                                "        return VALID_CALCULATIONS[case_type]\n",
                                "    elif case_type in INVALID_CALCULATIONS:\n",
                                "        return {\n",
                                "            \"type\": case_type,\n",
                                "            \"is_valid_score\": False,\n",
                                "            \"model_score\": invalid_score,\n",
                                "            \"baseline_score\": invalid_score,\n",
                                "            \"ppscore\": invalid_score,\n",
                                "            \"metric_name\": None,\n",
                                "            \"metric_key\": None,\n",
                                "            \"model\": None,\n",
                                "            \"score_normalizer\": None,\n",
                                "        }\n",
                                "    raise Exception(f\"case_type {case_type} is not supported\")\n",
                                "\n",
                                "def _format_list_of_dicts(scores, output, sorted):\n",
                                "    \"\"\"\n",
                                "    Format list of score dicts `scores`\n",
                                "    - maybe sort by ppscore\n",
                                "    - maybe return pandas.Dataframe\n",
                                "    - output can be one of [\"df\", \"list\"]\n",
                                "    \"\"\"\n",
                                "    if sorted:\n",
                                "        scores.sort(key=lambda item: item[\"ppscore\"], reverse=True)\n",
                                "\n",
                                "    if output == \"df\":\n",
                                "        df_columns = [\n",
                                "            \"x\",\n",
                                "            \"y\",\n",
                                "            \"ppscore\",\n",
                                "            \"case\",\n",
                                "            \"is_valid_score\",\n",
                                "            \"metric\",\n",
                                "            \"baseline_score\",\n",
                                "            \"model_score\",\n",
                                "            \"model\",\n",
                                "        ]\n",
                                "        data = {column: [score[column] for score in scores] for column in df_columns}\n",
                                "        scores = pd.DataFrame.from_dict(data)\n",
                                "\n",
                                "    return scores\n",
                                "\n",
                                "def pps_predictors(df, y, output=\"df\", sorted=True, **kwargs):\n",
                                "    \"\"\"\n",
                                "    Calculate the Predictive Power Score (PPS) of all the features in the dataframe\n",
                                "    against a target column\n",
                                "\n",
                                "    Parameters\n",
                                "    ----------\n",
                                "    df : pandas.DataFrame\n",
                                "        The dataframe that contains the data\n",
                                "    y : str\n",
                                "        Name of the column y which acts as the target\n",
                                "    output: str - potential values: \"df\", \"list\"\n",
                                "        Control the type of the output. Either return a pandas.DataFrame (df) or a list with the score dicts\n",
                                "    sorted: bool\n",
                                "        Whether or not to sort the output dataframe/list by the ppscore\n",
                                "    kwargs:\n",
                                "        Other key-word arguments that shall be forwarded to the pps.score method,\n",
                                "        e.g. `sample, `cross_validation, `random_seed, `invalid_score`, `catch_errors`\n",
                                "\n",
                                "    Returns\n",
                                "    -------\n",
                                "    pandas.DataFrame or list of Dict\n",
                                "        Either returns a tidy dataframe or a list of all the PPS dicts. This can be influenced\n",
                                "        by the output argument\n",
                                "    \"\"\"\n",
                                "    if not isinstance(df, pd.DataFrame):\n",
                                "        raise TypeError(\n",
                                "            f\"The 'df' argument should be a pandas.DataFrame but you passed a {type(df)}\\nPlease convert your input to a pandas.DataFrame\"\n",
                                "        )\n",
                                "    if not _is_column_in_df(y, df):\n",
                                "        raise ValueError(\n",
                                "            f\"The 'y' argument should be the name of a dataframe column but the variable that you passed is not a column in the given dataframe.\\nPlease review the column name or your dataframe\"\n",
                                "        )\n",
                                "    if len(df[[y]].columns) >= 2:\n",
                                "        raise AssertionError(\n",
                                "            f\"The dataframe has {len(df[[y]].columns)} columns with the same column name {y}\\nPlease adjust the dataframe and make sure that only 1 column has the name {y}\"\n",
                                "        )\n",
                                "    if not output in [\"df\", \"list\"]:\n",
                                "        raise ValueError(\n",
                                "            f\"\"\"The 'output' argument should be one of [\"df\", \"list\"] but you passed: {output}\\nPlease adjust your input to one of the valid values\"\"\"\n",
                                "        )\n",
                                "    if not sorted in [True, False]:\n",
                                "        raise ValueError(\n",
                                "            f\"\"\"The 'sorted' argument should be one of [True, False] but you passed: {sorted}\\nPlease adjust your input to one of the valid values\"\"\"\n",
                                "        )\n",
                                "\n",
                                "    scores = [pps_score(df, column, y, **kwargs) for column in df if column != y]\n",
                                "\n",
                                "    return _format_list_of_dicts(scores=scores, output=output, sorted=sorted)\n",
                                "\n",
                                "def pps_matrix(df, output=\"df\", sorted=False, **kwargs):\n",
                                "    \"\"\"\n",
                                "    Calculate the Predictive Power Score (PPS) matrix for all columns in the dataframe\n",
                                "\n",
                                "    Parameters\n",
                                "    ----------\n",
                                "    df : pandas.DataFrame\n",
                                "        The dataframe that contains the data\n",
                                "    output: str - potential values: \"df\", \"list\"\n",
                                "        Control the type of the output. Either return a pandas.DataFrame (df) or a list with the score dicts\n",
                                "    sorted: bool\n",
                                "        Whether or not to sort the output dataframe/list by the ppscore\n",
                                "    kwargs:\n",
                                "        Other key-word arguments that shall be forwarded to the pps.score method,\n",
                                "        e.g. `sample, `cross_validation, `random_seed, `invalid_score`, `catch_errors`\n",
                                "\n",
                                "    Returns\n",
                                "    -------\n",
                                "    pandas.DataFrame or list of Dict\n",
                                "        Either returns a tidy dataframe or a list of all the PPS dicts. This can be influenced\n",
                                "        by the output argument\n",
                                "    \"\"\"\n",
                                "    if not isinstance(df, pd.DataFrame):\n",
                                "        raise TypeError(\n",
                                "            f\"The 'df' argument should be a pandas.DataFrame but you passed a {type(df)}\\nPlease convert your input to a pandas.DataFrame\"\n",
                                "        )\n",
                                "    if not output in [\"df\", \"list\"]:\n",
                                "        raise ValueError(\n",
                                "            f\"\"\"The 'output' argument should be one of [\"df\", \"list\"] but you passed: {output}\\nPlease adjust your input to one of the valid values\"\"\"\n",
                                "        )\n",
                                "    if not sorted in [True, False]:\n",
                                "        raise ValueError(\n",
                                "            f\"\"\"The 'sorted' argument should be one of [True, False] but you passed: {sorted}\\nPlease adjust your input to one of the valid values\"\"\"\n",
                                "        )\n",
                                "\n",
                                "    scores = [pps_score(df, x, y, **kwargs) for x in df for y in df]\n",
                                "\n",
                                "    return _format_list_of_dicts(scores=scores, output=output, sorted=sorted)"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 10,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "\"\"\"PSM的工具函数\"\"\"\n",
                                "def get_propensity_scores(model, data, verbose = False):\n",
                                "    '''\n",
                                "    基于指定的逻辑回归模型计算倾向评分\n",
                                "\n",
                                "    Parameters\n",
                                "    ----------\n",
                                "    model : string\n",
                                "        a model specification in the form Y ~ X1 + X2 + ... + Xn\n",
                                "    data : Pandas DataFrame\n",
                                "        the data used to calculate propensity scores\n",
                                "    verbose : boolean\n",
                                "        verbosity of the model output\n",
                                "\n",
                                "    Returns\n",
                                "    -------\n",
                                "    An array of propensity scores.\n",
                                "    '''\n",
                                "    glm_binom = smf.glm(formula = model, data = data, family = sm.families.Binomial())\n",
                                "    result = glm_binom.fit()\n",
                                "    if verbose:\n",
                                "        print(result.summary)\n",
                                "    return result.fittedvalues\n",
                                "\n",
                                "def flatten_match_ids(df):\n",
                                "    '''\n",
                                "    Converts a Pandas DataFrame of matched IDs into a list of those IDs.\n",
                                "\n",
                                "    Parameters\n",
                                "    ----------\n",
                                "    df : Pandas Dataframe\n",
                                "        a dataframe consisting of 1 column of treated/case IDs and n columns\n",
                                "        of respective control(s) matched\n",
                                "\n",
                                "    Returns\n",
                                "    -------\n",
                                "    A list of treated case and matched control IDs.\n",
                                "    '''\n",
                                "    master_list = []\n",
                                "    master_list.append(df[df.columns[0]].tolist())\n",
                                "    for i in range(1, df.shape[1]):\n",
                                "        master_list.append(df[df.columns[i]].tolist())\n",
                                "    master_list = [item for sublist in master_list for item in sublist]\n",
                                "    return master_list\n",
                                "\n",
                                "def get_matched_data(match_ids, raw_data):\n",
                                "    '''\n",
                                "    Subsets the raw data to include data only from the treated cases and\n",
                                "    their respective matched control(s).\n",
                                "\n",
                                "    Parameters\n",
                                "    ----------\n",
                                "    match_ids : Pandas DataFrame\n",
                                "        a dataframe of treated case IDs and matched control(s)\n",
                                "    raw_data: Pandas DataFrame\n",
                                "        a dataframe of all of the raw data\n",
                                "\n",
                                "    Returns\n",
                                "    -------\n",
                                "    A dataframe containing data only from treated cases and matched control(s).\n",
                                "    '''\n",
                                "    match_ids = flatten_match_ids(match_ids)\n",
                                "    matched_data = raw_data[raw_data.index.isin(match_ids)]\n",
                                "    return matched_data\n",
                                "\n",
                                "def evaluate_KL_divergence(y_t1, y_t0, num_bins=30, epsilon=1e-10):\n",
                                "    \"\"\"\n",
                                "    计算 y_t1 和 y_t0 表示的概率分布之间的 KL 散度。\n",
                                "    \n",
                                "    参数:\n",
                                "    - y_t1: numpy 数组，表示处理组的数据，范围在 [-1, 1]\n",
                                "    - y_t0: numpy 数组，表示对照组的数据，范围在 [-1, 1]\n",
                                "    - num_bins: int，直方图的分箱数量\n",
                                "    - epsilon: float，为避免零概率而添加的平滑项\n",
                                "    \n",
                                "    返回:\n",
                                "    - KL 散度的值\n",
                                "    \"\"\"\n",
                                "    # 设置固定的分箱边界，从 -1 到 1\n",
                                "    bins = np.linspace(-1, 1, num_bins + 1)\n",
                                "    \n",
                                "    # 计算直方图\n",
                                "    hist_t1, _ = np.histogram(y_t1, bins=bins, density=True)\n",
                                "    hist_t0, _ = np.histogram(y_t0, bins=bins, density=True)\n",
                                "    \n",
                                "    # 将密度转换为概率分布\n",
                                "    bin_width = bins[1] - bins[0]\n",
                                "    p = hist_t1 * bin_width\n",
                                "    q = hist_t0 * bin_width\n",
                                "    \n",
                                "    # 添加平滑项以避免零概率\n",
                                "    p += epsilon\n",
                                "    q += epsilon\n",
                                "    \n",
                                "    # 重新归一化\n",
                                "    p /= p.sum()\n",
                                "    q /= q.sum()\n",
                                "    \n",
                                "    # 计算 KL 散度\n",
                                "    KL = stats.entropy(p, q)\n",
                                "    \n",
                                "    return KL\n",
                                "\n",
                                "####################################################\n",
                                "###################  Base Class  ###################\n",
                                "####################################################\n",
                                "\n",
                                "class PSMatch(object):\n",
                                "    '''\n",
                                "    Parameters\n",
                                "    ----------\n",
                                "    dataset : pd.DataFrame\n",
                                "        The dataset to be used for propensity score matching.\n",
                                "    model : string\n",
                                "        The model specification for calculating propensity scores; in the format T ~ X1 + X2 + ... + Xn\n",
                                "    k : string\n",
                                "        The number of controls to be matched to each treated case.\n",
                                "    '''\n",
                                "\n",
                                "    def __init__(self, dataset, Y_col, T_col, k):\n",
                                "        self.dataset = dataset.rename(columns={col: f'col_{col}' for col in dataset.columns})\n",
                                "        self.Y_col = f'col_{Y_col}'\n",
                                "        self.T_col = f'col_{T_col}'\n",
                                "        self.X_col = [f'col_{col}' for col in dataset.columns if col not in [Y_col, T_col]]\n",
                                "        self.model = f\"{self.T_col} ~ {' + '.join(self.X_col)}\"\n",
                                "        self.k = int(k)\n",
                                "\n",
                                "    def prepare_data(self, **kwargs):\n",
                                "        '''\n",
                                "        计算倾向得分\n",
                                "\n",
                                "        Returns\n",
                                "        -------\n",
                                "        A Pandas DataFrame containing raw data plus a column of propensity scores.\n",
                                "        '''\n",
                                "        df = self.dataset.copy()\n",
                                "        # 生成ID列\n",
                                "        df['ID'] = range(1, len(df) + 1)\n",
                                "        df = df.set_index('ID')\n",
                                "        # 将T_col从连续变量处理成0/1变量，按照85%和50%划分\n",
                                "        up_threshold = df[self.T_col].quantile(0.85)\n",
                                "        down_threshold = df[self.T_col].quantile(0.50)\n",
                                "        df['CASE'] = df[self.T_col].apply(lambda x: '处理组' if x >= up_threshold else ('控制组' if x < down_threshold else '剔除'))\n",
                                "        df = df[df['CASE'] != '剔除']\n",
                                "        # 更新 T_col 的值\n",
                                "        df[self.T_col] = df['CASE'].apply(lambda x: int(1) if x == '处理组' else int(0))\n",
                                "        # 计算倾向得分\n",
                                "        print(\"\\nCalculating propensity scores ...\", end = \" \")\n",
                                "        propensity_scores = get_propensity_scores(model = self.model, data = df, verbose = False)\n",
                                "        print(\"Preparing data ...\", end = \" \")\n",
                                "        df[\"PROPENSITY\"] = propensity_scores\n",
                                "        self.df = df\n",
                                "\n",
                                "    def match(self, caliper = None, **kwargs):\n",
                                "        '''\n",
                                "        执行倾向得分匹配\n",
                                "\n",
                                "        Returns\n",
                                "        -------\n",
                                "        matches : Pandas DataFrame\n",
                                "            the Match object attribute describing which control IDs are matched\n",
                                "            to a particular treatment case.\n",
                                "        matched_data: Pandas DataFrame\n",
                                "            the Match object attribute containing the raw data for only treatment\n",
                                "            cases and their matched controls.\n",
                                "        '''\n",
                                "        # 检查df是否已经初始化\n",
                                "        if not hasattr(self, 'df'):\n",
                                "            raise AttributeError(\"%s does not have a 'df' attribute.\" % (self))\n",
                                "\n",
                                "        # 获取倾向得分\n",
                                "        groups = self.df[f'{self.T_col}']\n",
                                "        propensity = self.df['PROPENSITY']\n",
                                "        # 处理组\n",
                                "        g_t1 = groups[groups==1].index\n",
                                "        p_t1 = propensity[groups==1]\n",
                                "        # 控制组\n",
                                "        g_t0 = groups[groups==0].index\n",
                                "        p_t0 = propensity[groups==0]\n",
                                "\n",
                                "        # 随机打乱T=1组的索引ID\n",
                                "        m_order = list(np.random.permutation(g_t1))\n",
                                "        matches = {}\n",
                                "        k = int(self.k)\n",
                                "\n",
                                "        # 基于倾向得分差异进行匹配\n",
                                "        print(\"\\nMatching [\" + str(k) + \"] controls to each case ... \", end = \" \")\n",
                                "        for m in m_order:\n",
                                "            # 计算得分差异\n",
                                "            difference = abs(p_t1[m]-p_t0)\n",
                                "            difference_array = np.array(difference)\n",
                                "            # 选取k个最小的得分差异\n",
                                "            k_smallest = np.partition(difference_array, k)[:k].tolist()\n",
                                "            if caliper:  # 如果设置了阈值\n",
                                "                caliper = float(caliper)\n",
                                "                keep_diffs = [i for i in k_smallest if i <= caliper]\n",
                                "                keep_ids = difference[difference.isin(keep_diffs)].index.tolist()\n",
                                "            else:\n",
                                "                keep_ids = difference.nsmallest(k).index.tolist()\n",
                                "\n",
                                "            # 如果匹配数大于 k，随机选择 k 个\n",
                                "            if len(keep_ids) > k:\n",
                                "                selected_ids = list(np.random.choice(keep_ids, k, replace=False))\n",
                                "                matches[m] = selected_ids\n",
                                "            elif len(keep_ids) < k:\n",
                                "                matches[m] = keep_ids.copy()\n",
                                "                while len(matches[m]) < k:\n",
                                "                    matches[m].append(\"NA\")\n",
                                "            else:\n",
                                "                matches[m] = keep_ids.copy()\n",
                                "\n",
                                "            # 只 drop 掉最小差异的控制组数据\n",
                                "            # 首先，确保当前匹配不包含 \"NA\"\n",
                                "            valid_matches = [ctrl for ctrl in matches[m] if ctrl != \"NA\"]\n",
                                "            if valid_matches:\n",
                                "                # 计算当前匹配中每个控制组的差异\n",
                                "                current_differences = difference[valid_matches]\n",
                                "                # 找到差异最小的控制组 ID\n",
                                "                best_control = current_differences.idxmin()\n",
                                "                # 从控制组池中删除该控制组\n",
                                "                g_t0 = g_t0.drop(best_control)\n",
                                "                p_t0 = p_t0.drop(best_control)\n",
                                "\n",
                                "        # 将匹配结果转换为DataFrame\n",
                                "        matches = pd.DataFrame.from_dict(matches, orient=\"index\")\n",
                                "        matches = matches.reset_index()\n",
                                "        column_names = {}\n",
                                "        column_names[\"index\"] = \"ID\"\n",
                                "        for i in range(k):\n",
                                "            column_names[i] = str(\"CONTROL_MATCH_\" + str(i+1))\n",
                                "        matches = matches.rename(columns = column_names)\n",
                                "\n",
                                "        # 根据匹配结果获取匹配数据\n",
                                "        matched_data = get_matched_data(matches, self.df)\n",
                                "        self.matches = matches\n",
                                "        self.matched_data = matched_data\n",
                                "\n",
                                "    def evaluate(self, **kwargs):\n",
                                "        '''\n",
                                "        Conducts chi-square tests to verify statistically that the cases/controls\n",
                                "        are well-matched on the variables of interest.\n",
                                "        '''\n",
                                "        # 检查是否已进行匹配\n",
                                "        if not hasattr(self, 'matches'):\n",
                                "            raise AttributeError(\"%s does not have a 'matches' attribute.\" % (self))\n",
                                "        if not hasattr(self, 'matched_data'):\n",
                                "            raise AttributeError(\"%s does not have a 'matched_data' attribute.\" % (self))\n",
                                "\n",
                                "        matched_data = self.matched_data\n",
                                "        y_t1 = matched_data[matched_data[f'{self.T_col}'] == 1][f'{self.Y_col}'].values\n",
                                "        y_t0 = matched_data[matched_data[f'{self.T_col}'] == 0][f'{self.Y_col}'].values\n",
                                "\n",
                                "        # 确保y_t1和y_t0的长度相同，如果不等，剪短为相同长度\n",
                                "        if len(y_t1) > len(y_t0):\n",
                                "            # 修改为随机抽取相同长度的样本\n",
                                "            y_t1 = np.random.choice(y_t1, len(y_t0), replace=False)\n",
                                "        elif len(y_t0) > len(y_t1):\n",
                                "            y_t0 = np.random.choice(y_t0, len(y_t1), replace=False)\n",
                                "\n",
                                "        # 确保 y_t1 和 y_t0 在 [-1, 1] 范围内\n",
                                "        y_t1 = np.clip(y_t1, -1, 1)\n",
                                "        y_t0 = np.clip(y_t0, -1, 1)\n",
                                "    \n",
                                "        # 计算 KL 散度\n",
                                "        KL_divergence = evaluate_KL_divergence(y_t1, y_t0)\n",
                                "\n",
                                "        return KL_divergence\n",
                                "\n",
                                "    def run(self, **kwargs):\n",
                                "        self.prepare_data()\n",
                                "        self.match()\n",
                                "        KL = self.evaluate()\n",
                                "        return KL"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 11,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "\"\"\"工具函数\"\"\"\n",
                                "###### 回归的工具函数\n",
                                "def Squared_term(dataset, variables):\n",
                                "    for var in variables:\n",
                                "        dataset[f'{var}_squared_term'] = dataset[var] ** 2\n",
                                "    return dataset\n",
                                "\n",
                                "def Interaction_term(dataset, variables):\n",
                                "    for i in range(len(variables)):\n",
                                "        for j in range(i + 1, len(variables)):\n",
                                "            dataset[f'{variables[i]}_{variables[j]}'] = dataset[variables[i]] * dataset[variables[j]]\n",
                                "    return dataset\n",
                                "\n",
                                "def Cos_Sin_term(dataset, variables):\n",
                                "    for var in variables:\n",
                                "        dataset[f'{var}_cos_term'] = np.cos(dataset[var])\n",
                                "        dataset[f'{var}_sin_term'] = np.sin(dataset[var])\n",
                                "    return dataset\n",
                                "\n",
                                "def Piecewise_term(dataset, variables):\n",
                                "    for var in variables:\n",
                                "        dataset[f'{var}_piecewise_term'] = np.maximum(dataset[var] - np.median(dataset[var]), 0)\n",
                                "    return dataset\n",
                                "\n",
                                "###### 因果发现算法的工具函数\n",
                                "def handle_multicollinearity(data, epsilon=1e-7, corr_threshold=0.99):\n",
                                "    \"\"\"\n",
                                "    检查数据中的多重共线性，并在需要时向存在多重共线性的列添加随机扰动。\n",
                                "    \n",
                                "    参数:\n",
                                "    - data (np.ndarray): 输入的数据矩阵，形状为 (样本数, 特征数)。\n",
                                "    - epsilon (float): 添加的随机扰动的尺度，默认值为1e-10。\n",
                                "    - corr_threshold (float): 判断高相关性的阈值，默认值为0.95。\n",
                                "    \n",
                                "    返回:\n",
                                "    - data (np.ndarray): 处理后的数据矩阵。\n",
                                "    \"\"\"\n",
                                "    # 计算相关系数矩阵\n",
                                "    corr_matrix = np.corrcoef(data, rowvar=False)\n",
                                "    n_cols = corr_matrix.shape[0]\n",
                                "\n",
                                "    # 使用集合存储所有涉及多重共线性的列索引，避免重复\n",
                                "    high_corr_indices = set()\n",
                                "    \n",
                                "    # 遍历相关系数矩阵的上三角部分，寻找高相关的列对\n",
                                "    for i in range(n_cols):\n",
                                "        for j in range(i+1, n_cols):\n",
                                "            if np.abs(corr_matrix[i, j]) > corr_threshold:\n",
                                "                high_corr_indices.add(i)\n",
                                "                high_corr_indices.add(j)\n",
                                "    \n",
                                "    if high_corr_indices:\n",
                                "        sorted_indices = sorted(high_corr_indices)\n",
                                "        # print(f\"检测到多重共线性，涉及的列索引: {sorted_indices}。正在添加随机扰动...\")\n",
                                "        \n",
                                "        # 生成与高相关性列对应的随机扰动\n",
                                "        noise = np.random.normal(0, epsilon, (data.shape[0], len(sorted_indices)))\n",
                                "        \n",
                                "        # 将扰动添加到相应的列\n",
                                "        data[:, sorted_indices] += noise\n",
                                "    \n",
                                "    return data\n",
                                "\n",
                                "def convert_bidirectional_to_dag(bidirectional_adj: pd.DataFrame) -> pd.DataFrame:\n",
                                "    \"\"\"\n",
                                "    将双向编码的邻接矩阵转换为单向编码的邻接矩阵（DAG表示）。\n",
                                "    \n",
                                "    在双向编码的邻接矩阵中：\n",
                                "    - [j, i] = 1 且 [i, j] = -1 表示 i → j\n",
                                "    - [j, i] = 1 且 [i, j] = 1 表示 i ↔ j（互为因果）\n",
                                "    - 其他情况 [i, j] = 0 表示无边\n",
                                "    \n",
                                "    转换后，单向编码的邻接矩阵将：\n",
                                "    - [i, j] = 1 表示 i → j\n",
                                "    - [i, j] = 1 且 [j, i] = 1 表示 i ↔ j\n",
                                "    - [i, j] = 0 表示无边\n",
                                "    \n",
                                "    参数:\n",
                                "    - bidirectional_adj (pd.DataFrame): 双向编码的邻接矩阵，使用 1 和 -1 表示有向边。\n",
                                "    \n",
                                "    返回:\n",
                                "    - pd.DataFrame: 单向编码的邻接矩阵（DAG表示）。\n",
                                "    \"\"\"\n",
                                "    # 确保输入是一个方阵\n",
                                "    if bidirectional_adj.shape[0] != bidirectional_adj.shape[1]:\n",
                                "        raise ValueError(\"输入的邻接矩阵必须是方阵（行数等于列数）。\")\n",
                                "    \n",
                                "    # 确保行列索引一致\n",
                                "    if not bidirectional_adj.index.equals(bidirectional_adj.columns):\n",
                                "        raise ValueError(\"邻接矩阵的行索引和列索引必须相同。\")\n",
                                "    \n",
                                "    # 初始化一个全零的邻接矩阵\n",
                                "    single_direction_adj = pd.DataFrame(0, index=bidirectional_adj.index, columns=bidirectional_adj.columns)\n",
                                "    \n",
                                "    # 遍历每一对变量，确定有向边\n",
                                "    for source in bidirectional_adj.columns:\n",
                                "        for target in bidirectional_adj.index:\n",
                                "            if source == target:\n",
                                "                # 根据DAG定义，通常不允许自环，因此设置为0\n",
                                "                single_direction_adj.at[source, target] = 0\n",
                                "                continue\n",
                                "            \n",
                                "            # 检查双向编码的邻接矩阵来确定有向边\n",
                                "            if bidirectional_adj.at[target, source] == 1:\n",
                                "                if bidirectional_adj.at[source, target] == -1:\n",
                                "                    # 表示 source → target\n",
                                "                    single_direction_adj.at[source, target] = 1\n",
                                "                elif bidirectional_adj.at[source, target] == 1:\n",
                                "                    # 表示 source ↔ target（互为因果）\n",
                                "                    single_direction_adj.at[source, target] = 1\n",
                                "                    single_direction_adj.at[target, source] = 1\n",
                                "                # 如果 [source, target] == 0 或其他情况，不设置边\n",
                                "    return single_direction_adj\n",
                                "\n",
                                "def convert_dag_with_strength_to_dag(adj: pd.DataFrame) -> pd.DataFrame:\n",
                                "    \"\"\"\n",
                                "    将表示因果影响强度的邻接矩阵数据框转换为二元邻接矩阵数据框。\n",
                                "    无论存在正向影响还是负向影响，都将其转换为1；不存在影响则为0。\n",
                                "    \n",
                                "    参数:\n",
                                "    - adj (pd.DataFrame): 原始的因果影响强度邻接矩阵，元素可以是正数、负数或0。\n",
                                "    \n",
                                "    返回:\n",
                                "    - pd.DataFrame: 二元邻接矩阵，元素为0或1。\n",
                                "    \"\"\"\n",
                                "    # 确保输入是一个方阵\n",
                                "    if adj.shape[0] != adj.shape[1]:\n",
                                "        raise ValueError(\"输入的邻接矩阵必须是方阵（行数等于列数）。\")\n",
                                "    \n",
                                "    # 确保行列索引一致\n",
                                "    if not adj.index.equals(adj.columns):\n",
                                "        raise ValueError(\"邻接矩阵的行索引和列索引必须相同。\")\n",
                                "    \n",
                                "    # 将所有非零元素转换为1，零元素保持为0\n",
                                "    return adj.ne(0).astype(int)"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 12,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "\"\"\"离散化统计指标的工具函数\"\"\"\n",
                                "def discretize_sequence(x, ffactor=10):\n",
                                "    \"\"\"\n",
                                "    将连续序列离散化。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 输入的连续变量，范围在 [-1, 1] 之间。\n",
                                "    ffactor (int): 离散化因子，用于缩放和离散化。\n",
                                "\n",
                                "    返回:\n",
                                "    np.ndarray: 离散化后的序列（整数类型）。\n",
                                "    \"\"\"\n",
                                "    # 确保输入在 [-1, 1] 范围内\n",
                                "    x = np.clip(x, -1, 1)\n",
                                "    # 缩放并四舍五入\n",
                                "    x = np.round(x * ffactor).astype(int)\n",
                                "    return x\n",
                                "\n",
                                "def compute_discrete_probability(x):\n",
                                "    \"\"\"\n",
                                "    计算离散概率分布。\n",
                                "\n",
                                "    参数:\n",
                                "    x (iterable): 输入的离散化后的序列，可以是单变量或联合变量。\n",
                                "\n",
                                "    返回:\n",
                                "    Counter: 元素及其计数。\n",
                                "    \"\"\"\n",
                                "    return Counter(x)\n",
                                "\n",
                                "def discrete_entropy(x, bias_factor=0.7):\n",
                                "    \"\"\"\n",
                                "    计算离散熵。\n",
                                "\n",
                                "    参数:\n",
                                "    x (iterable): 输入的离散化后的序列，可以是单变量或联合变量。\n",
                                "    bias_factor (float): 偏差因子，用于修正有限样本的熵估计。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 计算得到的熵值。\n",
                                "    \"\"\"\n",
                                "    c = compute_discrete_probability(x)\n",
                                "    pk = np.array(list(c.values()), dtype=float)\n",
                                "    pk_sum = pk.sum()\n",
                                "    if pk_sum == 0:\n",
                                "        return 0.0\n",
                                "    pk /= pk_sum\n",
                                "    # 避免 log(0) 问题，添加一个很小的常数\n",
                                "    vec = pk * np.log(pk + 1e-12)\n",
                                "    S = -np.sum(vec)\n",
                                "    # 添加偏差项\n",
                                "    bias = bias_factor * (len(pk) - 1) / (2.0 * len(x))\n",
                                "    return S + bias\n",
                                "\n",
                                "def discrete_joint_entropy(x, y):\n",
                                "    \"\"\"\n",
                                "    计算两个离散序列的联合熵 H(X, Y)。\n",
                                "\n",
                                "    参数:\n",
                                "    x, y (array-like): 输入的离散化后的序列。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 联合熵 H(X, Y)。\n",
                                "    \"\"\"\n",
                                "    joint = list(zip(x, y))  # 将 x 和 y 配对\n",
                                "    return discrete_entropy(joint)\n",
                                "\n",
                                "def normalized_error_probability(x, y):\n",
                                "    \"\"\"\n",
                                "    计算归一化的错误概率。\n",
                                "\n",
                                "    该函数通过构建联合概率矩阵，计算分类错误的概率，并将其归一化。\n",
                                "\n",
                                "    参数:\n",
                                "    x, y (array-like): 输入的离散化后的序列。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 归一化的错误概率。\n",
                                "    \"\"\"\n",
                                "    cx = Counter(x)\n",
                                "    cy = Counter(y)\n",
                                "\n",
                                "    sorted_cx = sorted(cx.keys())\n",
                                "    sorted_cy = sorted(cy.keys())\n",
                                "\n",
                                "    # 统计联合频数\n",
                                "    pxy = defaultdict(int)\n",
                                "    for a, b in zip(x, y):\n",
                                "        pxy[(a, b)] += 1\n",
                                "\n",
                                "    total = sum(pxy.values())\n",
                                "    if total == 0:\n",
                                "        return 0.0  # 或者根据需求返回其他值\n",
                                "\n",
                                "    # 构建联合概率矩阵\n",
                                "    pxy_matrix = np.array([\n",
                                "        [pxy.get((a, b), 0) for b in sorted_cy]\n",
                                "        for a in sorted_cx\n",
                                "    ], dtype=float)\n",
                                "\n",
                                "    # 归一化为概率\n",
                                "    pxy_matrix /= total\n",
                                "\n",
                                "    # 计算每行的最大概率\n",
                                "    max_per_row = pxy_matrix.max(axis=1)\n",
                                "    perr = 1 - np.sum(max_per_row)\n",
                                "\n",
                                "    # 计算每列的概率和的最大值\n",
                                "    sum_per_column = pxy_matrix.sum(axis=0)\n",
                                "    max_perr = 1 - np.max(sum_per_column)\n",
                                "\n",
                                "    # 归一化错误概率\n",
                                "    pnorm = perr / max_perr if max_perr > 0 else perr\n",
                                "    return pnorm\n",
                                "\n",
                                "def discrete_divergence(cx, cy):\n",
                                "    \"\"\"\n",
                                "    计算两个离散分布之间的KL散度（Kullback-Leibler Divergence）。\n",
                                "\n",
                                "    KL散度衡量了分布 cx 相对于分布 cy 的差异，是信息论中的一个重要概念。\n",
                                "\n",
                                "    参数:\n",
                                "    cx (Counter): 第一个离散分布的元素计数。\n",
                                "    cy (Counter): 第二个离散分布的元素计数。\n",
                                "\n",
                                "    返回:\n",
                                "    float: KL散度 D_KL(cx || cy)。\n",
                                "    \"\"\"\n",
                                "    # 创建 cy 的副本，避免修改原始对象\n",
                                "    cy = cy.copy()\n",
                                "\n",
                                "    # 为了避免 cy 中某些元素的概率为零，将它们的计数设为 1\n",
                                "    for a in cx:\n",
                                "        if cy[a] == 0:\n",
                                "            cy[a] = 1\n",
                                "\n",
                                "    # 计算概率\n",
                                "    nx = float(sum(cx.values()))\n",
                                "    ny = float(sum(cy.values()))\n",
                                "\n",
                                "    kl_div = 0.0\n",
                                "    for a, v in cx.items():\n",
                                "        px = v / nx\n",
                                "        py = cy[a] / ny\n",
                                "        kl_div += px * np.log(px / py)\n",
                                "    return kl_div\n",
                                "\n",
                                "def discrete_conditional_entropy(x, y):\n",
                                "    \"\"\"\n",
                                "    计算两个离散序列的条件熵 H(X|Y)。\n",
                                "\n",
                                "    条件熵衡量了在已知 Y 的情况下，X 的不确定性。\n",
                                "\n",
                                "    参数:\n",
                                "    x, y (array-like): 输入的离散化后的序列。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 条件熵 H(X|Y)。\n",
                                "    \"\"\"\n",
                                "    joint_entropy = discrete_joint_entropy(x, y)\n",
                                "    entropy_y = discrete_entropy(y)\n",
                                "    return joint_entropy - entropy_y\n",
                                "\n",
                                "def adjusted_mutual_information_score(x, y):\n",
                                "    \"\"\"\n",
                                "    计算两个离散序列的调整互信息（Adjusted Mutual Information, AMI）。\n",
                                "\n",
                                "    AMI 是互信息的一种调整版本，考虑了随机期望的互信息，通常用于聚类评估。\n",
                                "\n",
                                "    参数:\n",
                                "    x, y (array-like): 输入的离散化后的序列。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 调整后的互信息。\n",
                                "    \"\"\"\n",
                                "    return adjusted_mutual_info_score(x, y)\n",
                                "\n",
                                "def discrete_mutual_information(x, y):\n",
                                "    \"\"\"\n",
                                "    计算两个离散序列的互信息 I(X; Y)。\n",
                                "\n",
                                "    互信息衡量了两个变量之间共享的信息量，是信息论中的一个基本概念。\n",
                                "\n",
                                "    参数:\n",
                                "    x, y (array-like): 输入的离散化后的序列。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 互信息 I(X; Y)。\n",
                                "    \"\"\"\n",
                                "    entropy_x = discrete_entropy(x)\n",
                                "    entropy_y = discrete_entropy(y)\n",
                                "    joint_entropy = discrete_joint_entropy(x, y)\n",
                                "    mutual_info = entropy_x + entropy_y - joint_entropy\n",
                                "    # 避免由于数值误差导致的负值\n",
                                "    mutual_info = max(mutual_info, 0)\n",
                                "    return mutual_info\n",
                                "\n",
                                "# -------------------\n",
                                "# Helper Functions\n",
                                "# -------------------\n",
                                "\n",
                                "def normalize_discrete(x):\n",
                                "    \"\"\"\n",
                                "    对离散化后的序列进行标准化处理。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散化后的x序列。\n",
                                "\n",
                                "    返回:\n",
                                "    np.ndarray: 标准化后的x序列。\n",
                                "    \"\"\"\n",
                                "    if len(set(x)) < 2:\n",
                                "        return np.zeros_like(x, dtype=float)\n",
                                "    x_mean = np.mean(x)\n",
                                "    x_std = np.std(x)\n",
                                "    if x_std > 0:\n",
                                "        return (x - x_mean) / x_std\n",
                                "    else:\n",
                                "        return x - x_mean\n",
                                "\n",
                                "def to_numerical(x_discrete, y_continuous):\n",
                                "    \"\"\"\n",
                                "    将类别型的离散x转换为数值型，通过将每个唯一的x值替换为对应y的平均值。\n",
                                "\n",
                                "    参数:\n",
                                "    x_discrete (array-like): 离散化后的x数组。\n",
                                "    y_continuous (array-like): 与x对应的连续y数组。\n",
                                "\n",
                                "    返回:\n",
                                "    np.ndarray: 数值型的x数组，每个x值被替换为对应的y平均值。\n",
                                "    \"\"\"\n",
                                "    dx = defaultdict(lambda: [0.0, 0])\n",
                                "    for a, b in zip(x_discrete, y_continuous):\n",
                                "        dx[a][0] += b\n",
                                "        dx[a][1] += 1\n",
                                "    for a in dx:\n",
                                "        dx[a][0] /= dx[a][1] if dx[a][1] > 0 else 1e-12\n",
                                "    x_numerical = np.array([dx[a][0] for a in x_discrete], dtype=float)\n",
                                "    x_numerical = (x_numerical - np.mean(x_numerical)) / np.std(x_numerical) if np.std(x_numerical) > 0 else x_numerical\n",
                                "    return x_numerical\n",
                                "\n",
                                "def count_unique(x):\n",
                                "    \"\"\"\n",
                                "    计算数组中唯一元素的数量。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 输入数组。\n",
                                "\n",
                                "    返回:\n",
                                "    int: 唯一元素的数量。\n",
                                "    \"\"\"\n",
                                "    return len(set(x))\n",
                                "\n",
                                "# -------------------\n",
                                "# Feature Engineering Functions\n",
                                "# -------------------\n",
                                "\n",
                                "def normalized_entropy_baseline(x):\n",
                                "    \"\"\"\n",
                                "    计算给定归一化x的标准化熵基线。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 标准化熵基线值。\n",
                                "    \"\"\"\n",
                                "    if len(set(x)) < 2:\n",
                                "        return 0.0\n",
                                "    xs = np.sort(x)\n",
                                "    delta = xs[1:] - xs[:-1]\n",
                                "    delta = delta[delta != 0]\n",
                                "    if len(delta) == 0:\n",
                                "        return 0.0\n",
                                "    hx = np.mean(np.log(delta))\n",
                                "    hx += special.psi(len(delta))\n",
                                "    hx -= special.psi(1)\n",
                                "    return hx\n",
                                "\n",
                                "def normalized_entropy(x, m=2):\n",
                                "    \"\"\"\n",
                                "    计算标准化熵。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "    m (int): delta计算的参数。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 标准化熵值。\n",
                                "    \"\"\"\n",
                                "    cx = Counter(x)\n",
                                "    if len(cx) < 2:\n",
                                "        return 0.0\n",
                                "    xk = np.array(list(cx.keys()), dtype=float)\n",
                                "    xk.sort()\n",
                                "    if len(xk) < 2:\n",
                                "        return 0.0\n",
                                "    delta = (xk[1:] - xk[:-1]) / m\n",
                                "    counter = np.array([cx[i] for i in xk], dtype=float)\n",
                                "    hx = np.sum(counter[1:] * np.log(delta / counter[1:])) / len(x)\n",
                                "    hx += (special.psi(len(delta)) - np.log(len(delta)))\n",
                                "    hx += np.log(len(x))\n",
                                "    hx -= (special.psi(m) - np.log(m))\n",
                                "    return hx\n",
                                "\n",
                                "def igci(x, y):\n",
                                "    \"\"\"\n",
                                "    计算IGCI（信息几何因果推断）度量。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "    y (array-like): 离散且归一化的y序列。\n",
                                "\n",
                                "    返回:\n",
                                "    float: IGCI度量值。\n",
                                "    \"\"\"\n",
                                "    # 检查是否有足够的唯一值\n",
                                "    if len(set(x)) < 2:\n",
                                "        return 0.0\n",
                                "    \n",
                                "    # 判断是否有重复的x值\n",
                                "    if len(x) != len(set(x)):\n",
                                "        dx = defaultdict(lambda: [0.0, 0])\n",
                                "        for a, b in zip(x, y):\n",
                                "            dx[a][0] += b\n",
                                "            dx[a][1] += 1\n",
                                "        for a in dx:\n",
                                "            dx[a][0] /= dx[a][1] if dx[a][1] > 0 else 1e-12\n",
                                "        # 构建联合序列\n",
                                "        xy = np.array([[a, dx[a][0]] for a in dx.keys()], dtype=float)\n",
                                "        # 获取每个x的计数\n",
                                "        counter = np.array([dx[a][1] for a in xy[:, 0]], dtype=float)\n",
                                "    else:\n",
                                "        # 如果x没有重复，直接排序\n",
                                "        xy = np.array(sorted(zip(x, y)), dtype=float)\n",
                                "        counter = np.ones(len(x))\n",
                                "    \n",
                                "    # 计算相邻差值\n",
                                "    delta = xy[1:] - xy[:-1]\n",
                                "    # 选择y差值不为0的样本\n",
                                "    selec = delta[:, 1] != 0\n",
                                "    delta = delta[selec]\n",
                                "    counter = np.minimum(counter[1:], counter[:-1])[selec]\n",
                                "    \n",
                                "    if len(delta) == 0:\n",
                                "        return 0.0\n",
                                "    \n",
                                "    # 添加一个极小值epsilon，避免log(0)\n",
                                "    epsilon = 1e-12\n",
                                "    ratio = (delta[:, 0] + epsilon) / np.abs(delta[:, 1])\n",
                                "    ratio = np.where(ratio > 0, ratio, epsilon)\n",
                                "    \n",
                                "    # 计算 hxy，避免返回 NaN\n",
                                "    with np.errstate(divide='ignore', invalid='ignore'):\n",
                                "        hxy = np.sum(counter * np.log(ratio)) / len(x)\n",
                                "    \n",
                                "    # 检查 hxy 是否为有效数值\n",
                                "    if np.isnan(hxy):\n",
                                "        return 0.0\n",
                                "    \n",
                                "    return hxy\n",
                                "\n",
                                "def uniform_divergence(x, m=2):\n",
                                "    \"\"\"\n",
                                "    计算统一散度。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "    m (int): delta计算的参数。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 统一散度值。\n",
                                "    \"\"\"\n",
                                "    cx = Counter(x)\n",
                                "    xk = np.array(list(cx.keys()), dtype=float)\n",
                                "    xk.sort()\n",
                                "    delta = np.zeros(len(xk))\n",
                                "    if len(xk) > 1:\n",
                                "        delta[0] = xk[1] - xk[0]\n",
                                "        if len(xk) > m:\n",
                                "            delta[1:-1] = (xk[m:] - xk[:-m]) / m\n",
                                "        else:\n",
                                "            delta[1:-1] = (xk[-1] - xk[0]) / (len(xk) - 1)\n",
                                "        delta[-1] = xk[-1] - xk[-2]\n",
                                "    else:\n",
                                "        delta = np.array([np.sqrt(12)], dtype=float)  # 假设均匀分布在[-1,1]\n",
                                "\n",
                                "    counter = np.array([cx[i] for i in xk], dtype=float)\n",
                                "    delta_sum = np.sum(delta)\n",
                                "    if delta_sum > 0:\n",
                                "        delta = delta / delta_sum\n",
                                "    else:\n",
                                "        delta = delta\n",
                                "    if len(xk) > 1:\n",
                                "        hx = np.sum(counter * np.log(counter / delta)) / len(x)\n",
                                "    else:\n",
                                "        hx = 0.0\n",
                                "    hx -= np.log(len(x))\n",
                                "    hx += (special.psi(m) - np.log(m))\n",
                                "    return hx\n",
                                "\n",
                                "def normalized_skewness(x):\n",
                                "    \"\"\"\n",
                                "    计算x的标准化偏度。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 标准化偏度值。\n",
                                "    \"\"\"\n",
                                "    return stats.skew(x)\n",
                                "\n",
                                "def normalized_kurtosis(x):\n",
                                "    \"\"\"\n",
                                "    计算x的标准化峰度。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 标准化峰度值。\n",
                                "    \"\"\"\n",
                                "    return stats.kurtosis(x)\n",
                                "\n",
                                "def normalized_moment(x, y, n, m):\n",
                                "    \"\"\"\n",
                                "    计算x和y的标准化联合矩。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "    y (array-like): 离散且归一化的y序列。\n",
                                "    n (int): x的幂次。\n",
                                "    m (int): y的幂次。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 标准化的联合矩值。\n",
                                "    \"\"\"\n",
                                "    return np.mean((x ** n) * (y ** m))\n",
                                "\n",
                                "def moment21(x, y):\n",
                                "    \"\"\"\n",
                                "    计算标准化联合矩 I(X^2 * Y)。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "    y (array-like): 离散且归一化的y序列。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 联合矩 I(X^2 * Y)。\n",
                                "    \"\"\"\n",
                                "    return normalized_moment(x, y, 2, 1)\n",
                                "\n",
                                "def moment22(x, y):\n",
                                "    \"\"\"\n",
                                "    计算标准化联合矩 I(X^2 * Y^2)。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "    y (array-like): 离散且归一化的y序列。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 联合矩 I(X^2 * Y^2)。\n",
                                "    \"\"\"\n",
                                "    return normalized_moment(x, y, 2, 2)\n",
                                "\n",
                                "def moment31(x, y):\n",
                                "    \"\"\"\n",
                                "    计算标准化联合矩 I(X^3 * Y)。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "    y (array-like): 离散且归一化的y序列。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 联合矩 I(X^3 * Y)。\n",
                                "    \"\"\"\n",
                                "    return normalized_moment(x, y, 3, 1)\n",
                                "\n",
                                "def fit_pairwise(x, y):\n",
                                "    \"\"\"\n",
                                "    拟合多项式到x和y，并基于系数计算一个复杂的度量值。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列（数值型）。\n",
                                "    y (array-like): 离散且归一化的y序列（数值型）。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 拟合度量值。\n",
                                "    \"\"\"\n",
                                "    if count_unique(x) <= 2 or count_unique(y) <= 2:\n",
                                "        return 0.0\n",
                                "    x_std = x if np.std(x) == 1 else (x - np.mean(x)) / np.std(x) if np.std(x) > 0 else x\n",
                                "    y_std = y if np.std(y) == 1 else (y - np.mean(y)) / np.std(y) if np.std(y) > 0 else y\n",
                                "    try:\n",
                                "        xy1 = np.polyfit(x_std, y_std, 1)\n",
                                "        xy2 = np.polyfit(x_std, y_std, 2)\n",
                                "        return abs(2 * xy2[0]) + abs(xy2[1] - xy1[0])\n",
                                "    except np.RankWarning:\n",
                                "        return 0.0\n",
                                "    except Exception:\n",
                                "        return 0.0\n",
                                "\n",
                                "def fit_error(x, y, m=2):\n",
                                "    \"\"\"\n",
                                "    计算x和y之间的拟合误差。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "    y (array-like): 离散且归一化的y序列。\n",
                                "    m (int): 拟合时使用的多项式的阶数。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 拟合误差。\n",
                                "    \"\"\"\n",
                                "    if count_unique(x) <= m or count_unique(y) <= m:\n",
                                "        poly_degree = min(count_unique(x), count_unique(y)) - 1\n",
                                "    else:\n",
                                "        poly_degree = m\n",
                                "\n",
                                "    if poly_degree < 1:\n",
                                "        return 0.0\n",
                                "\n",
                                "    try:\n",
                                "        poly = np.polyfit(x, y, poly_degree)\n",
                                "        y_pred = np.polyval(poly, x)\n",
                                "        return np.std(y - y_pred)\n",
                                "    except np.RankWarning:\n",
                                "        return 0.0\n",
                                "    except Exception:\n",
                                "        return 0.0\n",
                                "\n",
                                "def fit_noise_entropy(x, y, minc=10):\n",
                                "    \"\"\"\n",
                                "    计算拟合噪声熵。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "    y (array-like): 离散且归一化的y序列。\n",
                                "    minc (int): 计算熵的最小计数阈值。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 拟合噪声熵。\n",
                                "    \"\"\"\n",
                                "    cx = Counter(x)\n",
                                "    entyx = []\n",
                                "    for a in cx:\n",
                                "        if cx[a] > minc:\n",
                                "            y_subset = y[x == a]\n",
                                "            entyx.append(discrete_entropy(y_subset))\n",
                                "    if len(entyx) == 0:\n",
                                "        return 0.0\n",
                                "    n = count_unique(y)\n",
                                "    return np.std(entyx) / np.log(n) if n > 0 else 0.0\n",
                                "\n",
                                "def fit_noise_skewness(x, y, minc=8):\n",
                                "    \"\"\"\n",
                                "    计算拟合噪声偏度的标准差。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "    y (array-like): 离散且归一化的y序列。\n",
                                "    minc (int): 计算偏度的最小计数阈值。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 拟合噪声偏度的标准差。\n",
                                "    \"\"\"\n",
                                "    cx = Counter(x)\n",
                                "    skewyx = []\n",
                                "    for a in cx:\n",
                                "        if cx[a] >= minc:\n",
                                "            y_subset = y[x == a]\n",
                                "            skewyx.append(normalized_skewness(y_subset))\n",
                                "    if len(skewyx) == 0:\n",
                                "        return 0.0\n",
                                "    return np.std(skewyx)\n",
                                "\n",
                                "def fit_noise_kurtosis(x, y, minc=8):\n",
                                "    \"\"\"\n",
                                "    计算拟合噪声峰度的标准差。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "    y (array-like): 离散且归一化的y序列。\n",
                                "    minc (int): 计算峰度的最小计数阈值。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 拟合噪声峰度的标准差。\n",
                                "    \"\"\"\n",
                                "    cx = Counter(x)\n",
                                "    kurtyx = []\n",
                                "    for a in cx:\n",
                                "        if cx[a] >= minc:\n",
                                "            y_subset = y[x == a]\n",
                                "            kurtyx.append(normalized_kurtosis(y_subset))\n",
                                "    if len(kurtyx) == 0:\n",
                                "        return 0.0\n",
                                "    return np.std(kurtyx)\n",
                                "\n",
                                "def conditional_distribution_similarity(x, y, minc=12):\n",
                                "    \"\"\"\n",
                                "    计算条件分布相似性。\n",
                                "\n",
                                "    参数:\n",
                                "    x (array-like): 离散且归一化的x序列。\n",
                                "    y (array-like): 离散且归一化的y序列。\n",
                                "    minc (int): 计算条件分布的最小计数阈值。\n",
                                "\n",
                                "    返回:\n",
                                "    float: 条件分布相似性度量。\n",
                                "    \"\"\"\n",
                                "    cx = Counter(x)\n",
                                "    cy = Counter(y)\n",
                                "    yrange = sorted(cy.keys())\n",
                                "    ny = len(yrange)\n",
                                "\n",
                                "    py = np.array([cy[i] for i in yrange], dtype=float)\n",
                                "    py = py / py.sum() if py.sum() > 0 else py\n",
                                "\n",
                                "    pyx = []\n",
                                "    for a in cx:\n",
                                "        if cx[a] > minc:\n",
                                "            yx = y[x == a]\n",
                                "            cyx = Counter(yx)\n",
                                "            pyxa = np.array([cyx.get(i, 0.0) for i in yrange], dtype=float)\n",
                                "            if pyxa.sum() == 0:\n",
                                "                continue\n",
                                "            pyxa = pyxa / pyxa.sum()\n",
                                "            pyx.append(py * pyxa)  # 修正这里，将 pyx * pyxa 改为 py * pyxa\n",
                                "\n",
                                "    if len(pyx) == 0:\n",
                                "        return 0.0\n",
                                "\n",
                                "    pyx = np.array(pyx)\n",
                                "    pyx = pyx - pyx.mean(axis=0)\n",
                                "    return np.std(pyx)"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 13,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:31:23.461861Z",
                                        "start_time": "2024-09-18T09:31:23.368017Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "###########################################################################\n",
                                "################################相关系数特征################################\n",
                                "###########################################################################\n",
                                "\"\"\"皮尔逊相关系数\"\"\"\n",
                                "def pearson_correlation(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the correlation-based features for each\n",
                                "    varibale, which are the correlation between that variable with X and Y,\n",
                                "    as well as summary statistics (max, min, mean, std) of all pairs\n",
                                "    of correlations.\n",
                                "    \"\"\"\n",
                                "\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = dataset.corr().drop([variable], axis=\"columns\").loc[variable].abs()\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"corr(v,X)\": dataset[[variable, \"X\"]].corr().loc[variable, \"X\"],\n",
                                "            \"corr(v,Y)\": dataset[[variable, \"Y\"]].corr().loc[variable, \"Y\"],\n",
                                "            \"max(corr(v, others))\": tmp.max(),\n",
                                "            \"min(corr(v, others))\": tmp.min(),\n",
                                "            \"mean(corr(v, others))\": tmp.mean(),\n",
                                "            \"std(corr(v, others))\": tmp.std(),\n",
                                "            \"25%(corr(v, others))\": tmp.quantile(0.25), \n",
                                "            \"75%(corr(v, others))\": tmp.quantile(0.75), \n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr().loc[\"X\", \"Y\"]\n",
                                "\n",
                                "    # pearsonr is NaN when the variance is 0, so we fill with 0\n",
                                "    df.fillna(0, inplace=True)\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"滚动皮尔逊相关系数绝对值\"\"\"\n",
                                "def rolling_abs_pearson_correlation(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = dataset[[variable, \"X\", \"Y\"]].copy()\n",
                                "        tmp = tmp.sort_values(by=variable, ascending=True).reset_index(drop=True)\n",
                                "        # 计算窗口内v-\"X\"的皮尔逊相关系数绝对值\n",
                                "        abs_rolling_corr_x = tmp[variable].rolling(window=300).corr(tmp['X']).abs()\n",
                                "        # 计算窗口内v-\"Y\"的皮尔逊相关系数绝对值\n",
                                "        abs_rolling_corr_y = tmp[variable].rolling(window=300).corr(tmp['Y']).abs()\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"max(abs_rolling_corr(v, X))\": abs_rolling_corr_x.max(),\n",
                                "            \"min(abs_rolling_corr(v, X))\": abs_rolling_corr_x.min(),\n",
                                "            \"mean(abs_rolling_corr(v, X))\": abs_rolling_corr_x.mean(),\n",
                                "            # \"std(abs_rolling_corr(v, X))\": abs_rolling_corr_x.std(),  # 掉分\n",
                                "            # \"25%(abs_rolling_corr(v, X))\": abs_rolling_corr_x.quantile(0.25),\n",
                                "            # \"75%(abs_rolling_corr(v, X))\": abs_rolling_corr_x.quantile(0.75),\n",
                                "            \"max(abs_rolling_corr(v, Y))\": abs_rolling_corr_y.max(),\n",
                                "            \"min(abs_rolling_corr(v, Y))\": abs_rolling_corr_y.min(),\n",
                                "            \"mean(abs_rolling_corr(v, Y))\": abs_rolling_corr_y.mean(),\n",
                                "            # \"std(abs_rolling_corr(v, Y))\": abs_rolling_corr_y.std(),\n",
                                "            # \"25%(abs_rolling_corr(v, Y))\": abs_rolling_corr_y.quantile(0.25),\n",
                                "            # \"75%(abs_rolling_corr(v, Y))\": abs_rolling_corr_y.quantile(0.75)\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"斯皮尔曼相关系数\"\"\"\n",
                                "def spearman_correlation(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the Spearman rank correlation-based features for each\n",
                                "    variable, which are the Spearman correlation between that variable with X and Y,\n",
                                "    as well as summary statistics (max, min, mean, std) of all pairs of Spearman correlations.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = dataset.corr(method='spearman').drop([variable], axis=\"columns\").loc[variable].abs()\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"spearman_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='spearman').loc[variable, \"X\"],\n",
                                "            \"spearman_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='spearman').loc[variable, \"Y\"],\n",
                                "            \"max(spearman_corr(v, others))\": tmp.max(),\n",
                                "            \"min(spearman_corr(v, others))\": tmp.min(),\n",
                                "            \"mean(spearman_corr(v, others))\": tmp.mean(),\n",
                                "            \"std(spearman_corr(v, others))\": tmp.std(),\n",
                                "            #TODO 分位数\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"spearman_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='spearman').loc[\"X\", \"Y\"]\n",
                                "\n",
                                "    # Spearman correlation is NaN when there are ties in rank, so we fill with 0\n",
                                "    df.fillna(0, inplace=True)\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"肯德尔相关系数\"\"\"\n",
                                "def kendall_correlation(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the Kendall's tau correlation-based features for each\n",
                                "    variable, which are the Kendall's tau correlation between that variable with X and Y,\n",
                                "    as well as summary statistics (max, min, mean, std) of all pairs of Kendall's tau correlations.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = dataset.corr(method='kendall').drop([variable], axis=\"columns\").loc[variable].abs()\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"kendall_corr(v,X)\": dataset[[variable, \"X\"]].corr(method='kendall').loc[variable, \"X\"],\n",
                                "            \"kendall_corr(v,Y)\": dataset[[variable, \"Y\"]].corr(method='kendall').loc[variable, \"Y\"],\n",
                                "            \"max(kendall_corr(v, others))\": tmp.max(),\n",
                                "            \"min(kendall_corr(v, others))\": tmp.min(),\n",
                                "            \"mean(kendall_corr(v, others))\": tmp.mean(),\n",
                                "            \"std(kendall_corr(v, others))\": tmp.std(),\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"kendall_corr(X,Y)\"] = dataset[[\"X\", \"Y\"]].corr(method='kendall').loc[\"X\", \"Y\"]\n",
                                "\n",
                                "    # Kendall's tau correlation can be NaN in some cases, so we fill with 0\n",
                                "    df.fillna(0, inplace=True)\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"互信息\"\"\"\n",
                                "def mutual_information(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the mutual-information-based features\n",
                                "    for each variable, which are the MI between that variable\n",
                                "    and X and Y, as well as summary statistics (max, min, mean, std) of\n",
                                "    all pairs of MI.\n",
                                "    \"\"\"\n",
                                "\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = mutual_info_regression(dataset.drop(columns=[variable]), dataset[variable])\n",
                                "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"MI(v,X)\": mutual_info_regression(dataset[[variable]], dataset[\"X\"], discrete_features=False)[0],\n",
                                "            \"MI(v,Y)\": mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0],\n",
                                "            \"max(MI(v, others))\": tmp.max(),\n",
                                "            \"min(MI(v, others))\": tmp.min(),\n",
                                "            \"mean(MI(v, others))\": tmp.mean(),\n",
                                "            \"std(MI(v, others))\": tmp.std(),\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"MI(X,Y)\"] = mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"条件互信息\"\"\"\n",
                                "def conditional_mutual_information(dataset):\n",
                                "    \"\"\"\n",
                                "    Calculate conditional mutual information for each variable with X and Y.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "    \n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # Calculate conditional MI(v, X | Y)\n",
                                "        mi_vx_given_y = mutual_info_regression(dataset[[variable, \"Y\"]], dataset[\"X\"], discrete_features=False)[0] - \\\n",
                                "                        mutual_info_regression(dataset[[\"Y\"]], dataset[\"X\"], discrete_features=False)[0]\n",
                                "        \n",
                                "        # Calculate conditional MI(v, Y | X)\n",
                                "        mi_vy_given_x = mutual_info_regression(dataset[[variable, \"X\"]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
                                "                        mutual_info_regression(dataset[[\"X\"]], dataset[\"Y\"], discrete_features=False)[0]\n",
                                "        \n",
                                "        # Calculate conditional MI(X, Y | v)\n",
                                "        mi_xy_given_v = mutual_info_regression(dataset[[\"X\", variable]], dataset[\"Y\"], discrete_features=False)[0] - \\\n",
                                "                        mutual_info_regression(dataset[[variable]], dataset[\"Y\"], discrete_features=False)[0]\n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"conditional_MI(v,X|Y)\": mi_vx_given_y,\n",
                                "            \"conditional_MI(v,Y|X)\": mi_vy_given_x,\n",
                                "            \"conditional_MI(X,Y|v)\": mi_xy_given_v,\n",
                                "        })\n",
                                "    \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "    \n",
                                "    return df\n",
                                "\n",
                                "\"\"\"距离相关系数\"\"\"\n",
                                "def distance_correlation(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the distance correlation-based features for each\n",
                                "    variable, which are the distance correlation between that variable with X and Y,\n",
                                "    as well as summary statistics (max, min, mean, std) of all pairs of distance correlations.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = []\n",
                                "        # Compute distance correlation between 'variable' and all other variables (excluding itself)\n",
                                "        other_variables = dataset.columns.drop([variable])\n",
                                "        for other_var in other_variables:\n",
                                "            corr = dcor.distance_correlation(dataset[variable], dataset[other_var])\n",
                                "            tmp.append(corr)\n",
                                "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
                                "\n",
                                "        distance_correlation_v_X = dcor.distance_correlation(dataset[variable], dataset[\"X\"])\n",
                                "        distance_correlation_v_Y = dcor.distance_correlation(dataset[variable], dataset[\"Y\"])\n",
                                "        distance_correlation_X_Y = dcor.distance_correlation(dataset[\"X\"], dataset[\"Y\"])\n",
                                "        distance_correlation_v_X_square = distance_correlation_v_X ** 2\n",
                                "        distance_correlation_v_Y_square = distance_correlation_v_Y ** 2\n",
                                "        distance_correlation_X_Y_square = distance_correlation_X_Y ** 2\n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"dcor(v,X)\": distance_correlation_v_X,\n",
                                "            \"dcor(v,Y)\": distance_correlation_v_Y,\n",
                                "            \"dcor(v,X)^2\": distance_correlation_v_X_square,\n",
                                "            \"dcor(v,Y)^2\": distance_correlation_v_Y_square,\n",
                                "            \"max(dcor(v, others))\": tmp.max(),\n",
                                "            \"min(dcor(v, others))\": tmp.min(),\n",
                                "            \"mean(dcor(v, others))\": tmp.mean(),\n",
                                "            \"std(dcor(v, others))\": tmp.std(),\n",
                                "            \"25%(dcor(v, others))\": tmp.quantile(0.25),\n",
                                "            \"75%(dcor(v, others))\": tmp.quantile(0.75),\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"dcor(X,Y)\"] = distance_correlation_X_Y\n",
                                "    df[\"dcor(X,Y)^2\"] = distance_correlation_X_Y_square\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"距离相关系数-尝试不同的欧几里得距离指数\"\"\"\n",
                                "def distance_correlation_different_exponent(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the distance correlation-based features for each\n",
                                "    variable, which are the distance correlation between that variable with X and Y,\n",
                                "    with different exponent, as well as summary statistics (max, min, mean, std) of all pairs of distance correlations.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        tmp = []\n",
                                "        # Compute distance correlation between 'variable' and all other variables (excluding itself)\n",
                                "        other_variables = dataset.columns.drop([variable])\n",
                                "        for other_var in other_variables:\n",
                                "            corr = dcor.distance_correlation(dataset[variable], dataset[other_var])\n",
                                "            tmp.append(corr)\n",
                                "        tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
                                "\n",
                                "        distance_correlation_v_X = dcor.distance_correlation(dataset[variable], dataset[\"X\"], exponent=0.5)\n",
                                "        distance_correlation_v_Y = dcor.distance_correlation(dataset[variable], dataset[\"Y\"], exponent=0.5)\n",
                                "        distance_correlation_X_Y = dcor.distance_correlation(dataset[\"X\"], dataset[\"Y\"], exponent=0.5)\n",
                                "        distance_correlation_v_X_square = distance_correlation_v_X ** 2\n",
                                "        distance_correlation_v_Y_square = distance_correlation_v_Y ** 2\n",
                                "        distance_correlation_X_Y_square = distance_correlation_X_Y ** 2\n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"dcor_0.5exp(v,X)\": distance_correlation_v_X,\n",
                                "            \"dcor_0.5exp(v,Y)\": distance_correlation_v_Y,\n",
                                "            \"dcor_0.5exp(v,X)^2\": distance_correlation_v_X_square,\n",
                                "            \"dcor_0.5exp(v,Y)^2\": distance_correlation_v_Y_square,\n",
                                "            \"max(dcor_0.5exp(v, others))\": tmp.max(),\n",
                                "            \"min(dcor_0.5exp(v, others))\": tmp.min(),\n",
                                "            \"mean(dcor_0.5exp(v, others))\": tmp.mean(),\n",
                                "            \"std(dcor_0.5exp(v, others))\": tmp.std(),\n",
                                "            \"25%(dcor_0.5exp(v, others))\": tmp.quantile(0.25),\n",
                                "            \"75%(dcor_0.5exp(v, others))\": tmp.quantile(0.75),\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"dcor_0.5exp(X,Y)\"] = distance_correlation_X_Y\n",
                                "    df[\"dcor_0.5exp(X,Y)^2\"] = distance_correlation_X_Y_square\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"能量距离\"\"\"\n",
                                "def energy_distance_features(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the energy distance-based features for each\n",
                                "    variable, which are the energy distance between that variable with X and Y,\n",
                                "    as well as summary statistics (max, min, mean, std) of all pairs of energy distances.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # tmp = []\n",
                                "        # # Compute energy distance between 'variable' and all other variables (excluding itself)\n",
                                "        # other_variables = dataset.columns.drop([variable])\n",
                                "        # for other_var in other_variables:\n",
                                "        #     energy_dist = dcor.energy_distance(dataset[variable], dataset[other_var])\n",
                                "        #     tmp.append(energy_dist)\n",
                                "        # tmp = pd.Series(tmp)  # Convert tmp to a Pandas Series\n",
                                "\n",
                                "        energy_distance_v_X = dcor.energy_distance(dataset[variable], dataset[\"X\"])\n",
                                "        energy_distance_v_Y = dcor.energy_distance(dataset[variable], dataset[\"Y\"])\n",
                                "        energy_distance_X_Y = dcor.energy_distance(dataset[\"X\"], dataset[\"Y\"])\n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"energy_dist(v,X)\": energy_distance_v_X,\n",
                                "            \"energy_dist(v,Y)\": energy_distance_v_Y,\n",
                                "            # \"max(energy_dist(v, others))\": tmp.max(),\n",
                                "            # \"min(energy_dist(v, others))\": tmp.min(),\n",
                                "            # \"mean(energy_dist(v, others))\": tmp.mean(),\n",
                                "            # \"std(energy_dist(v, others))\": tmp.std(),\n",
                                "            # \"25%(energy_dist(v, others))\": tmp.quantile(0.25),\n",
                                "            # \"75%(energy_dist(v, others))\": tmp.quantile(0.75),\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    df[\"energy_dist(X,Y)\"] = energy_distance_X_Y\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"偏相关系数\"\"\"\n",
                                "def partial_correlation(dataset):\n",
                                "    \"\"\"\n",
                                "    Compute partial correlation coefficients for each variable with X and Y,\n",
                                "    controlling for the other variable, as well as the partial correlation\n",
                                "    between X and Y controlling for each variable.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "    \n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # Compute partial correlations\n",
                                "        pcorr_vX_Y = pg.partial_corr(data=dataset, x=variable, y='X', covar='Y')['r'].iloc[0]\n",
                                "        pcorr_vY_X = pg.partial_corr(data=dataset, x=variable, y='Y', covar='X')['r'].iloc[0]\n",
                                "        pcorr_XY_v = pg.partial_corr(data=dataset, x='X', y='Y', covar=variable)['r'].iloc[0]\n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"partial_corr(v,X|Y)\": pcorr_vX_Y,\n",
                                "            \"partial_corr(v,Y|X)\": pcorr_vY_X,\n",
                                "            \"partial_corr(X,Y|v)\": pcorr_XY_v,\n",
                                "        })\n",
                                "    \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "    \n",
                                "    return df\n",
                                "\n",
                                "\"\"\"Copula Entropy\"\"\"\n",
                                "def copula_entropy(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute the Copula entropy-based features for each\n",
                                "    variable, which are the Copula entropy between that variable with X and Y,\n",
                                "    as well as summary statistics of all pairs of Copula entropies.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # Compute copula entropy between v and X\n",
                                "        ce_v_X = copent(dataset[[variable, \"X\"]].values)\n",
                                "        \n",
                                "        # Compute copula entropy between v and Y\n",
                                "        ce_v_Y = copent(dataset[[variable, \"Y\"]].values)\n",
                                "        \n",
                                "        # # Compute transfer entropy from v to X and X to v\n",
                                "        # te_v_X = transent(dataset[variable].values, dataset[\"X\"].values)\n",
                                "        # te_X_v = transent(dataset[\"X\"].values, dataset[variable].values)\n",
                                "        \n",
                                "        # # Compute transfer entropy from v to Y and Y to v\n",
                                "        # te_v_Y = transent(dataset[variable].values, dataset[\"Y\"].values)\n",
                                "        # te_Y_v = transent(dataset[\"Y\"].values, dataset[variable].values)\n",
                                "        \n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"copula_entropy(v,X)\": ce_v_X,\n",
                                "            \"copula_entropy(v,Y)\": ce_v_Y,\n",
                                "            # \"transfer_entropy(v->X)\": te_v_X,\n",
                                "            # \"transfer_entropy(X->v)\": te_X_v,\n",
                                "            # \"transfer_entropy(v->Y)\": te_v_Y,\n",
                                "            # \"transfer_entropy(Y->v)\": te_Y_v,\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Compute copula entropy between X and Y\n",
                                "    df[\"copula_entropy(X,Y)\"] = copent(dataset[[\"X\", \"Y\"]].values)\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"Predictive Power Score\"\"\"\n",
                                "def PPS_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    matrix_df = pps_matrix(dataset)\n",
                                "    pivot_df = pd.pivot_table(matrix_df, index='x', columns='y', values='ppscore')\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"PPS(v,X)\": pivot_df.loc[variable, 'X'],\n",
                                "            \"PPS(X,v)\": pivot_df.loc['X', variable],\n",
                                "            \"PPS(v,Y)\": pivot_df.loc[variable, 'Y'],\n",
                                "            \"PPS(Y,v)\": pivot_df.loc['Y', variable],\n",
                                "            \"PPS(X,Y)\": pivot_df.loc['X', 'Y'],\n",
                                "            \"max(PPS(v,others))\": pivot_df.loc[variable, variables].max(),\n",
                                "            \"mean(PPS(v,others))\": pivot_df.loc[variable, variables].mean(),\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "###########################################################################\n",
                                "################################其他类型特征################################\n",
                                "###########################################################################\n",
                                "\"\"\"维度信息\"\"\"\n",
                                "def add_dimension_feature(dataset):\n",
                                "    \"\"\"\n",
                                "    Add a dimension feature to the dataset.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "    dimension = len(variables)\n",
                                "    square_dimension = dimension * dimension\n",
                                "    df = pd.DataFrame({\n",
                                "        \"variable\": variables,\n",
                                "        \"dimension\": dimension,\n",
                                "        \"square_dimension\": square_dimension\n",
                                "    })\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    return df\n",
                                "\n",
                                "\"\"\"\"用于剔除离群样本\"\"\"\n",
                                "def descirbe_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "    X_mean = dataset[\"X\"].mean()\n",
                                "    Y_mean = dataset[\"Y\"].mean()\n",
                                "    \n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"variable_mean\": np.mean(dataset[variable]),\n",
                                "            \"variable_std\": np.std(dataset[variable])\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    df[\"X_mean\"] = X_mean\n",
                                "    df[\"Y_mean\"] = Y_mean\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"网格化信息\"\"\"\n",
                                "def grid_describe(df, var_col, dir_col, grid=5, xlim=[-1.0, 1.0], ylim=[-1.0, 1.0]):\n",
                                "    \"\"\"\n",
                                "    将指定的空间根据 xlim 和 ylim 划分为 grid x grid 的小窗格，并统计每个窗格内的数据点数量。\n",
                                "    确保所有划分出的区域都被记录，即使某些区域内没有数据点，计数为0。\n",
                                "\n",
                                "    参数:\n",
                                "    - df: pandas.DataFrame 包含 var_col 和 dir_col 两列\n",
                                "    - var_col: 用于 X 轴的列名（例如 'X'）\n",
                                "    - dir_col: 用于 Y 轴的列名（例如 'Y'）\n",
                                "    - grid: 网格的数量，默认为 5\n",
                                "    - xlim: X 轴的范围，默认为 [-1.0, 1.0]\n",
                                "    - ylim: Y 轴的范围，默认为 [-1.0, 1.0]\n",
                                "\n",
                                "    返回:\n",
                                "    - 一个字典，键为 (x_bin, y_bin) 的元组，值为对应窗格内的数据点数量\n",
                                "    \"\"\"\n",
                                "    # 定义网格边界\n",
                                "    x_bins = np.linspace(xlim[0], xlim[1], grid + 1)\n",
                                "    y_bins = np.linspace(ylim[0], ylim[1], grid + 1)\n",
                                "\n",
                                "    # 使用 numpy.histogram2d 计算2D直方图\n",
                                "    counts, _, _ = np.histogram2d(df[var_col], df[dir_col], bins=[x_bins, y_bins])\n",
                                "\n",
                                "    # 将计数结果转换为字典，键为 (x_bin, y_bin)，值为计数\n",
                                "    grid_counts = {}\n",
                                "    for x in range(grid):\n",
                                "        for y in range(grid):\n",
                                "            grid_counts[(x, y)] = int(counts[x, y])  # 转换为整数\n",
                                "\n",
                                "    return grid_counts\n",
                                "\n",
                                "def grid_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    grid = 5\n",
                                "    xlim = [-1.0, 1.0]\n",
                                "    ylim = [-1.0, 1.0]\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        temp = dataset[[variable, \"X\"]].sort_values(by=variable, ascending=True)\n",
                                "        grid_features_v_to_X = grid_describe(temp, variable, 'X', grid, xlim, ylim)\n",
                                "        temp = dataset[[variable, \"Y\"]].sort_values(by=variable, ascending=True)\n",
                                "        grid_features_v_to_Y = grid_describe(temp, variable, 'Y', grid, xlim, ylim)\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"grid(v,X)(0,0)\": grid_features_v_to_X[(0, 0)],\n",
                                "            \"grid(v,X)(0,1)\": grid_features_v_to_X[(0, 1)],\n",
                                "            \"grid(v,X)(0,2)\": grid_features_v_to_X[(0, 2)],\n",
                                "            \"grid(v,X)(0,3)\": grid_features_v_to_X[(0, 3)],\n",
                                "            \"grid(v,X)(0,4)\": grid_features_v_to_X[(0, 4)],\n",
                                "            \"grid(v,X)(1,0)\": grid_features_v_to_X[(1, 0)],\n",
                                "            \"grid(v,X)(1,1)\": grid_features_v_to_X[(1, 1)],\n",
                                "            \"grid(v,X)(1,2)\": grid_features_v_to_X[(1, 2)],\n",
                                "            \"grid(v,X)(1,3)\": grid_features_v_to_X[(1, 3)],\n",
                                "            \"grid(v,X)(1,4)\": grid_features_v_to_X[(1, 4)],\n",
                                "            \"grid(v,X)(2,0)\": grid_features_v_to_X[(2, 0)],\n",
                                "            \"grid(v,X)(2,1)\": grid_features_v_to_X[(2, 1)],\n",
                                "            \"grid(v,X)(2,2)\": grid_features_v_to_X[(2, 2)],\n",
                                "            \"grid(v,X)(2,3)\": grid_features_v_to_X[(2, 3)],\n",
                                "            \"grid(v,X)(2,4)\": grid_features_v_to_X[(2, 4)],\n",
                                "            \"grid(v,X)(3,0)\": grid_features_v_to_X[(3, 0)],\n",
                                "            \"grid(v,X)(3,1)\": grid_features_v_to_X[(3, 1)],\n",
                                "            \"grid(v,X)(3,2)\": grid_features_v_to_X[(3, 2)],\n",
                                "            \"grid(v,X)(3,3)\": grid_features_v_to_X[(3, 3)],\n",
                                "            \"grid(v,X)(3,4)\": grid_features_v_to_X[(3, 4)],\n",
                                "            \"grid(v,X)(4,0)\": grid_features_v_to_X[(4, 0)],\n",
                                "            \"grid(v,X)(4,1)\": grid_features_v_to_X[(4, 1)],\n",
                                "            \"grid(v,X)(4,2)\": grid_features_v_to_X[(4, 2)],\n",
                                "            \"grid(v,X)(4,3)\": grid_features_v_to_X[(4, 3)],\n",
                                "            \"grid(v,X)(4,4)\": grid_features_v_to_X[(4, 4)],\n",
                                "            \"grid(v,Y)(0,0)\": grid_features_v_to_Y[(0, 0)],\n",
                                "            \"grid(v,Y)(0,1)\": grid_features_v_to_Y[(0, 1)],\n",
                                "            \"grid(v,Y)(0,2)\": grid_features_v_to_Y[(0, 2)],\n",
                                "            \"grid(v,Y)(0,3)\": grid_features_v_to_Y[(0, 3)],\n",
                                "            \"grid(v,Y)(0,4)\": grid_features_v_to_Y[(0, 4)],\n",
                                "            \"grid(v,Y)(1,0)\": grid_features_v_to_Y[(1, 0)],\n",
                                "            \"grid(v,Y)(1,1)\": grid_features_v_to_Y[(1, 1)],\n",
                                "            \"grid(v,Y)(1,2)\": grid_features_v_to_Y[(1, 2)],\n",
                                "            \"grid(v,Y)(1,3)\": grid_features_v_to_Y[(1, 3)],\n",
                                "            \"grid(v,Y)(1,4)\": grid_features_v_to_Y[(1, 4)],\n",
                                "            \"grid(v,Y)(2,0)\": grid_features_v_to_Y[(2, 0)],\n",
                                "            \"grid(v,Y)(2,1)\": grid_features_v_to_Y[(2, 1)],\n",
                                "            \"grid(v,Y)(2,2)\": grid_features_v_to_Y[(2, 2)],\n",
                                "            \"grid(v,Y)(2,3)\": grid_features_v_to_Y[(2, 3)],\n",
                                "            \"grid(v,Y)(2,4)\": grid_features_v_to_Y[(2, 4)],\n",
                                "            \"grid(v,Y)(3,0)\": grid_features_v_to_Y[(3, 0)],\n",
                                "            \"grid(v,Y)(3,1)\": grid_features_v_to_Y[(3, 1)],\n",
                                "            \"grid(v,Y)(3,2)\": grid_features_v_to_Y[(3, 2)],\n",
                                "            \"grid(v,Y)(3,3)\": grid_features_v_to_Y[(3, 3)],\n",
                                "            \"grid(v,Y)(3,4)\": grid_features_v_to_Y[(3, 4)],\n",
                                "            \"grid(v,Y)(4,0)\": grid_features_v_to_Y[(4, 0)],\n",
                                "            \"grid(v,Y)(4,1)\": grid_features_v_to_Y[(4, 1)],\n",
                                "            \"grid(v,Y)(4,2)\": grid_features_v_to_Y[(4, 2)],\n",
                                "            \"grid(v,Y)(4,3)\": grid_features_v_to_Y[(4, 3)],\n",
                                "            \"grid(v,Y)(4,4)\": grid_features_v_to_Y[(4, 4)],\n",
                                "        })\n",
                                "    \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"离散化统计指标\"\"\"\n",
                                "def discrete_statistic_features(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    ffactor = 10\n",
                                "    x = dataset[\"X\"].values\n",
                                "    y = dataset[\"Y\"].values\n",
                                "    x_discrete = discretize_sequence(x, ffactor=ffactor)\n",
                                "    y_discrete = discretize_sequence(y, ffactor=ffactor)\n",
                                "    x_normalized = normalize_discrete(x_discrete)\n",
                                "    y_normalized = normalize_discrete(y_discrete)\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        v = dataset[variable].values\n",
                                "        v_discrete = discretize_sequence(v, ffactor=ffactor)\n",
                                "        v_normalized = normalize_discrete(v_discrete)   \n",
                                "        \n",
                                "        # 计算联合熵 H(v,x) 和 H(v,y)\n",
                                "        H_vx = discrete_joint_entropy(v_discrete, x_discrete)\n",
                                "        H_vy = discrete_joint_entropy(v_discrete, y_discrete)\n",
                                "        # 计算条件熵 H(v|x) 和 H(v|y)\n",
                                "        H_v_given_x = discrete_conditional_entropy(v_discrete, x_discrete)\n",
                                "        H_v_given_y = discrete_conditional_entropy(v_discrete, y_discrete)\n",
                                "        # 计算互信息 I(v,x) 和 I(v,y)\n",
                                "        I_vx = discrete_mutual_information(v_discrete, x_discrete)\n",
                                "        I_vy = discrete_mutual_information(v_discrete, y_discrete)\n",
                                "        # 计算调整互信息 AMI(v,x) 和 AMI(v,y)\n",
                                "        AMI_vx = adjusted_mutual_information_score(v_discrete, x_discrete)\n",
                                "        AMI_vy = adjusted_mutual_information_score(v_discrete, y_discrete)\n",
                                "        # 计算归一化的错误概率 NPE(v,x) 和 NPE(v,y)\n",
                                "        error_prob_vx = normalized_error_probability(v_discrete, x_discrete)\n",
                                "        error_prob_vy = normalized_error_probability(v_discrete, y_discrete)\n",
                                "\n",
                                "        # 计算归一化熵基线 H_baseline 和归一化熵 H_entropy 和均匀散度 uniform_div\n",
                                "        H_baseline = normalized_entropy_baseline(v_normalized)\n",
                                "        H_entropy = normalized_entropy(v_normalized)\n",
                                "        uniform_div = uniform_divergence(v_normalized)\n",
                                "        # 计算IGCI (v,x) 和 IGCI(v,y) \n",
                                "        igci_vx = igci(v_normalized, x_normalized)\n",
                                "        igci_vy = igci(v_normalized, y_normalized)\n",
                                "        # 计算IGCI (x,v) 和 IGCI(y,v) \n",
                                "        igci_xv = igci(x_normalized, v_normalized)\n",
                                "        igci_yv = igci(y_normalized, v_normalized)\n",
                                "        # 计算矩特征 MM(v^2 * x) 和 MM(v^2 * y) \n",
                                "        moment_21_vx = moment21(v_normalized, x_normalized)\n",
                                "        moment_21_vy = moment21(v_normalized, y_normalized)\n",
                                "        # 计算矩特征 MM(v^2 * x^2) 和 MM(v^2 * y^2) \n",
                                "        moment_22_vx = moment22(v_normalized, x_normalized)\n",
                                "        moment_22_vy = moment22(v_normalized, y_normalized)\n",
                                "        # 计算矩特征 MM(v^3 * x) 和 MM(v^3 * y) \n",
                                "        moment_31_vx = moment31(v_normalized, x_normalized)\n",
                                "        moment_31_vy = moment31(v_normalized, y_normalized)\n",
                                "        # 计算拟合度量值 fit 和拟合误差 fit_error 和拟合噪声熵 fit_noise_entropy 和拟合噪声偏度标准差 fit_noise_skewness 和拟合噪声峰度标准差 fit_noise_kurtosis\n",
                                "        fit_val_vx = fit_pairwise(v_normalized, x_normalized)\n",
                                "        fit_err_vx = fit_error(v_normalized, x_normalized, m=2)\n",
                                "        fit_noise_ent_vx = fit_noise_entropy(v_normalized, x_normalized, minc=8)\n",
                                "        fit_noise_skew_vx = fit_noise_skewness(v_normalized, x_normalized, minc=8)\n",
                                "        fit_noise_kurt_vx = fit_noise_kurtosis(v_normalized, x_normalized, minc=8)\n",
                                "        fit_val_vy = fit_pairwise(v_normalized, y_normalized)\n",
                                "        fit_err_vy = fit_error(v_normalized, y_normalized, m=2)\n",
                                "        fit_noise_ent_vy = fit_noise_entropy(v_normalized, y_normalized, minc=8)\n",
                                "        fit_noise_skew_vy = fit_noise_skewness(v_normalized, y_normalized, minc=8)\n",
                                "        fit_noise_kurt_vy = fit_noise_kurtosis(v_normalized, y_normalized, minc=8)\n",
                                "        # 计算条件分布相似度 cond_dist_sim\n",
                                "        cond_dist_sim_vx = conditional_distribution_similarity(v_normalized, x_normalized, minc=8)\n",
                                "        cond_dist_sim_vy = conditional_distribution_similarity(v_normalized, y_normalized, minc=8)\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"JH(v,x)\": H_vx,\n",
                                "            \"JH(v,y)\": H_vy,\n",
                                "            \"CH(v|x)\": H_v_given_x,\n",
                                "            \"CH(v|y)\": H_v_given_y,\n",
                                "            \"I(v,x)\": I_vx,\n",
                                "            \"I(v,y)\": I_vy,\n",
                                "            \"AMI(v,x)\": AMI_vx,\n",
                                "            \"AMI(v,y)\": AMI_vy,\n",
                                "            \"NPE(v,x)\": error_prob_vx,\n",
                                "            \"NPE(v,y)\": error_prob_vy,\n",
                                "            \"H_baseline\": H_baseline,\n",
                                "            \"H_entropy\": H_entropy,\n",
                                "            \"uniform_div\": uniform_div,\n",
                                "            \"IGCI(v,x)\": igci_vx,\n",
                                "            \"IGCI(v,y)\": igci_vy,\n",
                                "            \"IGCI(x,v)\": igci_xv,\n",
                                "            \"IGCI(y,v)\": igci_yv,\n",
                                "            \"MM(v^2 * x)\": moment_21_vx,\n",
                                "            \"MM(v^2 * y)\": moment_21_vy,\n",
                                "            \"MM(v^2 * x^2)\": moment_22_vx,\n",
                                "            \"MM(v^2 * y^2)\": moment_22_vy,\n",
                                "            \"MM(v^3 * x)\": moment_31_vx,\n",
                                "            \"MM(v^3 * y)\": moment_31_vy,\n",
                                "            \"fit(v,x)\": fit_val_vx,\n",
                                "            \"fit(v,y)\": fit_val_vy,\n",
                                "            \"fit_error(v,x)\": fit_err_vx,\n",
                                "            \"fit_error(v,y)\": fit_err_vy,\n",
                                "            \"fit_noise_entropy(v,x)\": fit_noise_ent_vx,\n",
                                "            \"fit_noise_entropy(v,y)\": fit_noise_ent_vy,\n",
                                "            \"fit_noise_skewness(v,x)\": fit_noise_skew_vx,\n",
                                "            \"fit_noise_skewness(v,y)\": fit_noise_skew_vy,\n",
                                "            \"fit_noise_kurtosis(v,x)\": fit_noise_kurt_vx,\n",
                                "            \"fit_noise_kurtosis(v,y)\": fit_noise_kurt_vy,\n",
                                "            \"cond_dist_sim(v,x)\": cond_dist_sim_vx,\n",
                                "            \"cond_dist_sim(v,y)\": cond_dist_sim_vy,\n",
                                "        })\n",
                                "    \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "    \n",
                                "###########################################################################\n",
                                "################################回归系数特征################################\n",
                                "###########################################################################\n",
                                "\"\"\"线性回归\"\"\"\n",
                                "def linear_regression_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "    scaler = StandardScaler()\n",
                                "\n",
                                "    # model1: Fit X, v, v^2, v_i*v_j, v_i*X, cos(v), sin(v) ~ Y\n",
                                "    model1_features = [\"X\"] + variables\n",
                                "    d1 = Squared_term(dataset[model1_features], model1_features)\n",
                                "    d1 = Interaction_term(d1, model1_features)\n",
                                "    d1 = Cos_Sin_term(d1, variables)\n",
                                "    model1_features = d1.columns.tolist()\n",
                                "    d1_scaled = scaler.fit_transform(d1)\n",
                                "    model1 = LinearRegression().fit(d1_scaled, dataset[[\"Y\"]])\n",
                                "    model1_coefs = model1.coef_[0].tolist()\n",
                                "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
                                "    \n",
                                "    # model2: Fit v, v^2, v_i*v_j, cos(v), sin(v) ~ X\n",
                                "    model2_features = variables\n",
                                "    d2 = Squared_term(dataset[model2_features], model2_features)\n",
                                "    d2 = Interaction_term(d2, model2_features)\n",
                                "    d2 = Cos_Sin_term(d2, variables)\n",
                                "    model2_features = d2.columns.tolist()\n",
                                "    d2_scaled = scaler.fit_transform(d2)\n",
                                "    model2 = LinearRegression().fit(d2_scaled, dataset[[\"X\"]])\n",
                                "    model2_coefs = model2.coef_[0].tolist()\n",
                                "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
                                "    \n",
                                "    df = []\n",
                                "    for i, variable in enumerate(variables):\n",
                                "        # model3: Fit other v, X, Y ~ v\n",
                                "        model3_features = [\"X\", \"Y\"] + dataset.columns.drop([\"X\", \"Y\", variable]).tolist()\n",
                                "        d3 = Squared_term(dataset[model3_features], model3_features)\n",
                                "        d3 = Interaction_term(d3, model3_features)\n",
                                "        d3 = Cos_Sin_term(d3, model3_features)\n",
                                "        model3_features = d3.columns.tolist()\n",
                                "        d3_scaled = scaler.fit_transform(d3)\n",
                                "        model3 = LinearRegression().fit(d3_scaled, dataset[[variable]])\n",
                                "        model3_coefs = model3.coef_[0].tolist()\n",
                                "        model3_dict = {name: coef for name, coef in zip(model3_features, model3_coefs)}\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"v~Y_coefficient\": model1_dict[variable],     # <--- model1\n",
                                "            \"v_squared~Y_coefficient\": model1_dict[f\"{variable}_squared_term\"],\n",
                                "            \"v*X~Y_coefficient\": model1_dict[f\"X_{variable}\"],\n",
                                "            \"v_cos~Y_coefficient\": model1_dict[f\"{variable}_cos_term\"],\n",
                                "            \"v_sin~Y_coefficient\": model1_dict[f\"{variable}_sin_term\"],\n",
                                "            \"v~X_coefficient\": model2_dict[variable],     # <--- model2\n",
                                "            \"v_squared~X_coefficient\": model2_dict[f\"{variable}_squared_term\"],\n",
                                "            \"v_cos~X_coefficient\": model2_dict[f\"{variable}_cos_term\"], \n",
                                "            \"v_sin~X_coefficient\": model2_dict[f\"{variable}_sin_term\"],  \n",
                                "            \"X~v_coefficient\": model3_dict[\"X\"],          # <--- model3\n",
                                "            \"X_squared~v_coefficient\": model3_dict[\"X_squared_term\"],\n",
                                "            \"X_cos~v_coefficient\": model3_dict[\"X_cos_term\"],\n",
                                "            \"X_sin~v_coefficient\": model3_dict[\"X_sin_term\"],\n",
                                "            \"Y~v_coefficient\": model3_dict[\"Y\"],\n",
                                "            \"Y_squared~v_coefficient\": model3_dict[\"Y_squared_term\"],\n",
                                "            \"Y_cos~v_coefficient\": model3_dict[\"Y_cos_term\"],\n",
                                "            \"Y_sin~v_coefficient\": model3_dict[\"Y_sin_term\"],\n",
                                "            \"X*Y~v_coefficient\": model3_dict[\"X_Y\"]\n",
                                "        })\n",
                                "        \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    df[\"X~Y_coefficient\"] = model1_dict[\"X\"]\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"分段线性回归\"\"\"\n",
                                "def piecewise_linear_regression_estimate(dataset, X_col, y_col):\n",
                                "    X = dataset[X_col].values.reshape(-1, 1)\n",
                                "    y = dataset[y_col].values\n",
                                "\n",
                                "    break_point = np.median(X)\n",
                                "    X_piecewise = np.column_stack((X, np.maximum(X - break_point, 0)))\n",
                                "    model = LinearRegression().fit(X_piecewise, y)\n",
                                "    return model.coef_\n",
                                "\n",
                                "def piecewise_linear_regression_feature(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute piecewise linear regression features for each\n",
                                "    variable with X and Y, using a single breakpoint at the median.\n",
                                "    We also include interaction terms between v and X for predicting Y.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # For v ~ X\n",
                                "        v2X_coef = piecewise_linear_regression_estimate(dataset, variable, \"X\")\n",
                                "        # For v ~ Y\n",
                                "        v2Y_coef = piecewise_linear_regression_estimate(dataset, variable, \"Y\")\n",
                                "        # For X ~ v\n",
                                "        X2v_coef = piecewise_linear_regression_estimate(dataset, \"X\", variable)\n",
                                "        # For Y ~ v\n",
                                "        Y2v_coef = piecewise_linear_regression_estimate(dataset, \"Y\", variable)\n",
                                "        \n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"v~X_piecewise_coef1\": v2X_coef[0],\n",
                                "            \"v~X_piecewise_coef2\": v2X_coef[1],\n",
                                "            \"v~Y_piecewise_coef1\": v2Y_coef[0],\n",
                                "            \"v~Y_piecewise_coef2\": v2Y_coef[1],\n",
                                "            \"X~v_piecewise_coef1\": X2v_coef[0],\n",
                                "            \"X~v_piecewise_coef2\": X2v_coef[1],\n",
                                "            \"Y~v_piecewise_coef1\": Y2v_coef[0],\n",
                                "            \"Y~v_piecewise_coef2\": Y2v_coef[1],\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def piecewise_quadratic_regression_feature(dataset):\n",
                                "    \"\"\"\n",
                                "    Compute piecewise quadratic regression features for each variable with X and Y,\n",
                                "    using a single breakpoint at the median.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # Prepare data\n",
                                "        v = dataset[variable].values.reshape(-1, 1)\n",
                                "        X = dataset[\"X\"].values.reshape(-1, 1)\n",
                                "        Y = dataset[\"Y\"].values.reshape(-1, 1)\n",
                                "\n",
                                "        # Create piecewise quadratic features\n",
                                "        v_breakpoint = np.median(v)\n",
                                "        X_breakpoint = np.median(X)\n",
                                "        Y_breakpoint = np.median(Y)\n",
                                "\n",
                                "        v_piecewise = np.column_stack((v, v**2, np.maximum(v - v_breakpoint, 0), np.maximum(v - v_breakpoint, 0)**2))\n",
                                "        X_piecewise = np.column_stack((X, X**2, np.maximum(X - X_breakpoint, 0), np.maximum(X - X_breakpoint, 0)**2))\n",
                                "        Y_piecewise = np.column_stack((Y, Y**2, np.maximum(Y - Y_breakpoint, 0), np.maximum(Y - Y_breakpoint, 0)**2))\n",
                                "\n",
                                "        # Fit models\n",
                                "        model_v_X = LinearRegression().fit(v_piecewise, X)\n",
                                "        model_v_Y = LinearRegression().fit(v_piecewise, Y)\n",
                                "        model_X_v = LinearRegression().fit(X_piecewise, v)\n",
                                "        model_Y_v = LinearRegression().fit(Y_piecewise, v)\n",
                                "\n",
                                "        # Store coefficients\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"v~X_piecewise_coef1\": model_v_X.coef_[0][0],\n",
                                "            \"v~X_piecewise_coef2\": model_v_X.coef_[0][1],\n",
                                "            \"v~X_piecewise_coef3\": model_v_X.coef_[0][2],\n",
                                "            \"v~X_piecewise_coef4\": model_v_X.coef_[0][3],\n",
                                "            \"v~Y_piecewise_coef1\": model_v_Y.coef_[0][0],\n",
                                "            \"v~Y_piecewise_coef2\": model_v_Y.coef_[0][1],\n",
                                "            \"v~Y_piecewise_coef3\": model_v_Y.coef_[0][2],\n",
                                "            \"v~Y_piecewise_coef4\": model_v_Y.coef_[0][3],\n",
                                "            \"X~v_piecewise_coef1\": model_X_v.coef_[0][0],\n",
                                "            \"X~v_piecewise_coef2\": model_X_v.coef_[0][1],\n",
                                "            \"X~v_piecewise_coef3\": model_X_v.coef_[0][2],\n",
                                "            \"X~v_piecewise_coef4\": model_X_v.coef_[0][3],\n",
                                "            \"Y~v_piecewise_coef1\": model_Y_v.coef_[0][0],\n",
                                "            \"Y~v_piecewise_coef2\": model_Y_v.coef_[0][1],\n",
                                "            \"Y~v_piecewise_coef3\": model_Y_v.coef_[0][2],\n",
                                "            \"Y~v_piecewise_coef4\": model_Y_v.coef_[0][3],\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def piecewise_quadratic_regression_feature_4_improved(dataset):\n",
                                "    \"\"\"\n",
                                "    Compute piecewise quadratic regression features for each variable with X and Y,\n",
                                "    using three breakpoints to create four segments, and only using quadratic terms.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # Prepare data\n",
                                "        v = dataset[variable].values.reshape(-1, 1)\n",
                                "        X = dataset[\"X\"].values.reshape(-1, 1)\n",
                                "        Y = dataset[\"Y\"].values.reshape(-1, 1)\n",
                                "\n",
                                "        # Create piecewise quadratic features with three breakpoints\n",
                                "        v_breakpoints = np.percentile(v, [25, 50, 75])\n",
                                "        X_breakpoints = np.percentile(X, [25, 50, 75])\n",
                                "        Y_breakpoints = np.percentile(Y, [25, 50, 75])\n",
                                "\n",
                                "        v_piecewise = np.column_stack((\n",
                                "            v**2,\n",
                                "            np.maximum(v - v_breakpoints[0], 0)**2,\n",
                                "            np.maximum(v - v_breakpoints[1], 0)**2,\n",
                                "            np.maximum(v - v_breakpoints[2], 0)**2\n",
                                "        ))\n",
                                "        X_piecewise = np.column_stack((\n",
                                "            X**2,\n",
                                "            np.maximum(X - X_breakpoints[0], 0)**2,\n",
                                "            np.maximum(X - X_breakpoints[1], 0)**2,\n",
                                "            np.maximum(X - X_breakpoints[2], 0)**2\n",
                                "        ))\n",
                                "        Y_piecewise = np.column_stack((\n",
                                "            Y**2,\n",
                                "            np.maximum(Y - Y_breakpoints[0], 0)**2,\n",
                                "            np.maximum(Y - Y_breakpoints[1], 0)**2,\n",
                                "            np.maximum(Y - Y_breakpoints[2], 0)**2\n",
                                "        ))\n",
                                "\n",
                                "        # Fit models\n",
                                "        model_v_X = LinearRegression(fit_intercept=False).fit(v_piecewise, X)\n",
                                "        model_v_Y = LinearRegression(fit_intercept=False).fit(v_piecewise, Y)\n",
                                "        model_X_v = LinearRegression(fit_intercept=False).fit(X_piecewise, v)\n",
                                "        model_Y_v = LinearRegression(fit_intercept=False).fit(Y_piecewise, v)\n",
                                "\n",
                                "        # Store coefficients\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"v~X_quadratic_coef1\": model_v_X.coef_[0][0],\n",
                                "            \"v~X_quadratic_coef2\": model_v_X.coef_[0][1],\n",
                                "            \"v~X_quadratic_coef3\": model_v_X.coef_[0][2],\n",
                                "            \"v~X_quadratic_coef4\": model_v_X.coef_[0][3],\n",
                                "            \"v~Y_quadratic_coef1\": model_v_Y.coef_[0][0],\n",
                                "            \"v~Y_quadratic_coef2\": model_v_Y.coef_[0][1],\n",
                                "            \"v~Y_quadratic_coef3\": model_v_Y.coef_[0][2],\n",
                                "            \"v~Y_quadratic_coef4\": model_v_Y.coef_[0][3],\n",
                                "            \"X~v_quadratic_coef1\": model_X_v.coef_[0][0],\n",
                                "            \"X~v_quadratic_coef2\": model_X_v.coef_[0][1],\n",
                                "            \"X~v_quadratic_coef3\": model_X_v.coef_[0][2],\n",
                                "            \"X~v_quadratic_coef4\": model_X_v.coef_[0][3],\n",
                                "            \"Y~v_quadratic_coef1\": model_Y_v.coef_[0][0],\n",
                                "            \"Y~v_quadratic_coef2\": model_Y_v.coef_[0][1],\n",
                                "            \"Y~v_quadratic_coef3\": model_Y_v.coef_[0][2],\n",
                                "            \"Y~v_quadratic_coef4\": model_Y_v.coef_[0][3],\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def sliding_window_linear_regression_estimate(dataset, X_col, y_col, window_size=None, step_size=None):\n",
                                "    if window_size is None:\n",
                                "        window_size = len(dataset) // 3\n",
                                "    if step_size is None:\n",
                                "        step_size = len(dataset) // 6\n",
                                "\n",
                                "    X = dataset[X_col].values\n",
                                "    y = dataset[y_col].values\n",
                                "\n",
                                "    coefficients = []\n",
                                "    for start in range(0, len(dataset) - window_size + 1, step_size):\n",
                                "        end = start + window_size\n",
                                "        X_window = X[start:end].reshape(-1, 1)\n",
                                "        y_window = y[start:end]\n",
                                "        \n",
                                "        model = LinearRegression().fit(X_window, y_window)\n",
                                "        coefficients.append(model.coef_[0])\n",
                                "\n",
                                "    return coefficients\n",
                                "\n",
                                "def sliding_window_linear_regression_feature(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, we compute sliding window linear regression features for each\n",
                                "    variable with X and Y, using a window size of 1/3 of the data and step size of 1/6.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # For v ~ X\n",
                                "        v2X_coef = sliding_window_linear_regression_estimate(dataset, variable, \"X\")\n",
                                "        # For v ~ Y\n",
                                "        v2Y_coef = sliding_window_linear_regression_estimate(dataset, variable, \"Y\")\n",
                                "        # For X ~ v\n",
                                "        X2v_coef = sliding_window_linear_regression_estimate(dataset, \"X\", variable)\n",
                                "        # For Y ~ v\n",
                                "        Y2v_coef = sliding_window_linear_regression_estimate(dataset, \"Y\", variable)\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"v~X_sliding_coef1\": v2X_coef[0],\n",
                                "            \"v~X_sliding_coef2\": v2X_coef[1],\n",
                                "            \"v~X_sliding_coef3\": v2X_coef[2],\n",
                                "            \"v~X_sliding_coef4\": v2X_coef[3],\n",
                                "            \"v~X_sliding_coef5\": v2X_coef[4],\n",
                                "            \"v~Y_sliding_coef1\": v2Y_coef[0],\n",
                                "            \"v~Y_sliding_coef2\": v2Y_coef[1],\n",
                                "            \"v~Y_sliding_coef3\": v2Y_coef[2],\n",
                                "            \"v~Y_sliding_coef4\": v2Y_coef[3],\n",
                                "            \"v~Y_sliding_coef5\": v2Y_coef[4],\n",
                                "            \"X~v_sliding_coef1\": X2v_coef[0],\n",
                                "            \"X~v_sliding_coef2\": X2v_coef[1],\n",
                                "            \"X~v_sliding_coef3\": X2v_coef[2],\n",
                                "            \"X~v_sliding_coef4\": X2v_coef[3],\n",
                                "            \"X~v_sliding_coef5\": X2v_coef[4],\n",
                                "            \"Y~v_sliding_coef1\": Y2v_coef[0],\n",
                                "            \"Y~v_sliding_coef2\": Y2v_coef[1],\n",
                                "            \"Y~v_sliding_coef3\": Y2v_coef[2],\n",
                                "            \"Y~v_sliding_coef4\": Y2v_coef[3],\n",
                                "            \"Y~v_sliding_coef5\": Y2v_coef[4],\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"岭回归\"\"\"\n",
                                "def ridge_regression_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "    # 使用GridSearchCV来选择最佳的alpha值\n",
                                "    param_grid = {'alpha': np.logspace(-6, 6, 13)}\n",
                                "    \n",
                                "    # model1: Fit X, v ~ Y\n",
                                "    model1_features = [\"X\"] + variables\n",
                                "    d1 = Squared_term(dataset[model1_features], model1_features)\n",
                                "    # d1 = Interaction_term(d1, model1_features) # 掉分\n",
                                "    model1_features = d1.columns.tolist()\n",
                                "    scaler1 = StandardScaler()\n",
                                "    d1_scaled = scaler1.fit_transform(d1)\n",
                                "    model1 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
                                "    model1.fit(d1_scaled, dataset[\"Y\"])\n",
                                "    model1_coefs = model1.best_estimator_.coef_.tolist()\n",
                                "    model1_dict = {name: coef for name, coef in zip(model1_features, model1_coefs)}\n",
                                "    \n",
                                "    # model2: Fit v ~ X\n",
                                "    model2_features = variables\n",
                                "    d2 = Squared_term(dataset[model2_features], model2_features)\n",
                                "    d2 = Interaction_term(d2, model2_features)\n",
                                "    # d2 = Cos_Sin_term(d2, model2_features)  # 掉分\n",
                                "    model2_features = d2.columns.tolist()\n",
                                "    scaler2 = StandardScaler()\n",
                                "    d2_scaled = scaler2.fit_transform(d2)\n",
                                "    model2 = GridSearchCV(Ridge(random_state=42), param_grid, cv=5)\n",
                                "    model2.fit(d2_scaled, dataset[\"X\"])\n",
                                "    model2_coefs = model2.best_estimator_.coef_.tolist()\n",
                                "    model2_dict = {name: coef for name, coef in zip(model2_features, model2_coefs)}\n",
                                "\n",
                                "    # # 获取最优的 alpha 值\n",
                                "    # best_alpha_model1 = model1.best_params_['alpha']   # 0.4730-0.4727\n",
                                "    # best_alpha_model2 = model2.best_params_['alpha']\n",
                                "    \n",
                                "    df = []\n",
                                "    for i, variable in enumerate(variables):\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"v~Y_ridge_coefficient\": model1_dict[variable],\n",
                                "            # \"v_squared~Y_ridge_coefficient\": model1_dict[f\"{variable}_squared_term\"],  # 掉分\n",
                                "            \"v~X_ridge_coefficient\": model2_dict[variable],\n",
                                "            # \"v_squared~X_ridge_coefficient\": model2_dict[f\"{variable}_squared_term\"],  # 掉分\n",
                                "            # \"v_cos~X_ridge_coefficient\": model2_dict[f\"{variable}_cos_term\"],  # 掉分\n",
                                "            # \"v_sin~X_ridge_coefficient\": model2_dict[f\"{variable}_sin_term\"],  # 掉分\n",
                                "            # \"v~Y_ridge_alpha\": best_alpha_model1,\n",
                                "            # \"v~X_ridge_alpha\": best_alpha_model2\n",
                                "        })\n",
                                "        \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    df[\"X~Y_ridge_coefficient\"] = model1_dict[\"X\"]\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "###########################################################################\n",
                                "################################因果发现特征################################\n",
                                "###########################################################################\n",
                                "\"\"\"因果发现算法\"\"\"\n",
                                "def ExactSearch_estimate(dataset, search_method='astar', use_path_extension=True, \n",
                                "                        use_k_cycle_heuristic=False, k=3, max_parents=None):\n",
                                "    \"\"\"\n",
                                "    使用Exact Search算法进行因果发现。\n",
                                "    \n",
                                "    参数:\n",
                                "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
                                "    - search_method (str): Exact Search方法，'astar'或'dp'。默认值为'astar'。\n",
                                "    - use_path_extension (bool): 是否使用路径扩展。默认值为True。\n",
                                "    - use_k_cycle_heuristic (bool): 是否使用k-cycle冲突启发式。仅适用于'astar'方法。默认值为False。\n",
                                "    - k (int): k-cycle启发式的参数。默认值为3。\n",
                                "    - max_parents (int or None): 节点的最大父节点数量。默认值为None。\n",
                                "    \n",
                                "    返回:\n",
                                "    - adj_df (pd.DataFrame): 因果图的邻接矩阵，格式为DataFrame。\n",
                                "    \"\"\"\n",
                                "    \n",
                                "    # 1. 将数据框转换为numpy.ndarray\n",
                                "    data = dataset.values\n",
                                "\n",
                                "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
                                "    data = handle_multicollinearity(data)\n",
                                "\n",
                                "    # 2. 定义X→Y的超级图\n",
                                "    d = dataset.shape[1]   \n",
                                "    super_graph = np.ones((d, d), dtype=int)  # 初始化为全1，表示所有边默认允许\n",
                                "    columns = dataset.columns.tolist()\n",
                                "    x_idx = columns.index('X')\n",
                                "    y_idx = columns.index('Y')\n",
                                "    super_graph[y_idx][x_idx] = 0     # 禁止'Y'→'X'\n",
                                "    np.fill_diagonal(super_graph, 0)  # 禁止自环：确保对角线为0\n",
                                "    \n",
                                "    # 3. 运行Exact Search算法，使用指定的参数\n",
                                "    dag_est, search_stats = bic_exact_search(X=data, super_graph=super_graph, search_method=search_method,\n",
                                "        use_path_extension=use_path_extension, use_k_cycle_heuristic=use_k_cycle_heuristic,\n",
                                "        k=k, verbose=False, max_parents=max_parents)\n",
                                "    \n",
                                "    # 4. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
                                "    adj_df = pd.DataFrame(dag_est, index=dataset.columns, columns=dataset.columns)\n",
                                "    \n",
                                "    return adj_df\n",
                                "\n",
                                "def ExactSearch_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    estimate_adj_df = ExactSearch_estimate(dataset)\n",
                                "    estimate_adj_df_dag = estimate_adj_df.astype(int)  # 转换为整型\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # 检查变量与'X'和'Y'之间的边\n",
                                "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
                                "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
                                "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
                                "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
                                "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
                                "\n",
                                "        # # 检查是否存在中介路径\n",
                                "        # v_to_others_to_X = int(any(\n",
                                "        #     estimate_adj_df_dag.loc[variable, other] and estimate_adj_df_dag.loc[other, 'X']\n",
                                "        #     for other in variables if other != variable\n",
                                "        # ))\n",
                                "        # X_to_others_to_v = int(any(\n",
                                "        #     estimate_adj_df_dag.loc['X', other] and estimate_adj_df_dag.loc[other, variable]\n",
                                "        #     for other in variables if other != variable\n",
                                "        # ))\n",
                                "        # v_to_others_to_Y = int(any(\n",
                                "        #     estimate_adj_df_dag.loc[variable, other] and estimate_adj_df_dag.loc[other, 'Y']\n",
                                "        #     for other in variables if other != variable\n",
                                "        # ))\n",
                                "        # Y_to_others_to_v = int(any(\n",
                                "        #     estimate_adj_df_dag.loc['Y', other] and estimate_adj_df_dag.loc[other, variable]\n",
                                "        #     for other in variables if other != variable\n",
                                "        # ))\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"ExactSearch(v,X)\": v_to_X,\n",
                                "            \"ExactSearch(X,v)\": X_to_v,\n",
                                "            \"ExactSearch(v,Y)\": v_to_Y,\n",
                                "            \"ExactSearch(Y,v)\": Y_to_v,\n",
                                "            \"ExactSearch(X,Y)\": X_to_Y,\n",
                                "            # \"ExactSearch(v,others,X)\": v_to_others_to_X,\n",
                                "            # \"ExactSearch(X,others,v)\": X_to_others_to_v,\n",
                                "            # \"ExactSearch(v,others,Y)\": v_to_others_to_Y,\n",
                                "            # \"ExactSearch(Y,others,v)\": Y_to_others_to_v\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def PC_estimate(dataset, alpha=0.05, indep_test='fisherz', kernel=None, stable=True, \n",
                                "               uc_rule=0, uc_priority=2, verbose=False, show_progress=False):\n",
                                "    \"\"\"\n",
                                "    使用PC算法进行因果发现，并应用先验知识。\n",
                                "\n",
                                "    参数:\n",
                                "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
                                "    - alpha (float): 显著性水平，默认值为0.05。\n",
                                "    - indep_test (str): 独立性检验方法，默认值为'fisherz'。\n",
                                "    - kernel (str): 核函数类型，默认值为'linear'。\n",
                                "    - stable (bool): 是否运行稳定的骨架发现，默认值为True。\n",
                                "    - uc_rule (int): 未屏蔽碰撞点的定向规则，默认值为0。\n",
                                "    - uc_priority (int): 解决未屏蔽碰撞点冲突的优先规则，默认值为2。\n",
                                "    - verbose (bool): 是否打印详细输出，默认值为False。\n",
                                "    - show_progress (bool): 是否显示算法进度，默认值为False。\n",
                                "\n",
                                "    返回:\n",
                                "    - adj_df (pd.DataFrame): 因果图的邻接矩阵，格式为pivot_table。\n",
                                "    \"\"\"\n",
                                "    \n",
                                "    # 1. 将数据框转换为numpy.ndarray\n",
                                "    data = dataset.values\n",
                                "\n",
                                "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
                                "    data = handle_multicollinearity(data)\n",
                                "    \n",
                                "    # 2. 定义先验知识：'X' → 'Y'\n",
                                "    # 创建GraphNode对象\n",
                                "    try:\n",
                                "        node_X = GraphNode('X')\n",
                                "        node_Y = GraphNode('Y')\n",
                                "    except Exception as e:\n",
                                "        raise ValueError(\"确保数据框中包含名为'X'和'Y'的列。\") from e\n",
                                "    \n",
                                "    # 初始化BackgroundKnowledge对象并添加先验知识\n",
                                "    bk = BackgroundKnowledge().add_required_by_node(node_X, node_Y)\n",
                                "\n",
                                "    # 3. 配置核参数\n",
                                "    if indep_test == 'kci':\n",
                                "        if kernel is None:\n",
                                "            kernel = 'linear'\n",
                                "        if kernel == 'linear':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Linear', \n",
                                "                'kernelY': 'Linear', \n",
                                "                'kernelZ': 'Linear', \n",
                                "                'approx': True,           # 使用伽玛近似\n",
                                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        elif kernel == 'polynomial':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Polynomial', \n",
                                "                'kernelY': 'Polynomial', \n",
                                "                'kernelZ': 'Polynomial', \n",
                                "                'polyd': 3,               # 多项式次数设置为3\n",
                                "                'approx': True,           # 使用伽玛近似\n",
                                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        elif kernel == 'gaussian':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Gaussian', \n",
                                "                'kernelY': 'Gaussian', \n",
                                "                'kernelZ': 'Gaussian', \n",
                                "                'est_width': 'empirical', # 使用经验宽度\n",
                                "                'approx': True,           # 使用伽玛近似\n",
                                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        elif kernel == 'mix':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Polynomial', \n",
                                "                'kernelY': 'Polynomial', \n",
                                "                'kernelZ': 'Gaussian',     # Z使用高斯核\n",
                                "                'polyd': 3,                # 多项式次数设置为3\n",
                                "                'est_width': 'median',     # Z的高斯核带宽使用中位数技巧\n",
                                "                'approx': True,            # 使用伽玛近似\n",
                                "                'nullss': 1000,           # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        else:\n",
                                "            raise ValueError(f'Unknown kernel: {kernel}')\n",
                                "    else:\n",
                                "        kernel_kwargs = {}\n",
                                "\n",
                                "    # 4. 运行PC算法，传入先验知识\n",
                                "    cg = pc(data, alpha=alpha, indep_test=indep_test, stable=stable, uc_rule=uc_rule, uc_priority=uc_priority, \n",
                                "            background_knowledge=bk, verbose=verbose, show_progress=show_progress, **kernel_kwargs)\n",
                                "    \n",
                                "    # 5. 提取邻接矩阵\n",
                                "    adj_matrix = cg.G.graph\n",
                                "    # 6. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
                                "    adj_df = pd.DataFrame(adj_matrix, index=dataset.columns, columns=dataset.columns)\n",
                                "    \n",
                                "    return adj_df\n",
                                "\n",
                                "def PC_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    estimate_adj_df_bidirectional = PC_estimate(dataset)  # 双向的估计因果图\n",
                                "    estimate_adj_df_dag = convert_bidirectional_to_dag(estimate_adj_df_bidirectional)  # 将双向图转换为有向图\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # 检查变量与'X'和'Y'之间的边\n",
                                "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
                                "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
                                "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
                                "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
                                "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"PC(v,X)\": v_to_X,\n",
                                "            \"PC(X,v)\": X_to_v,\n",
                                "            \"PC(v,Y)\": v_to_Y,\n",
                                "            \"PC(Y,v)\": Y_to_v,\n",
                                "            \"PC(X,Y)\": X_to_Y\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def FCI_estimate(dataset, alpha=0.05, indep_test='fisherz', kernel=None, \n",
                                "               depth=-1, max_path_length=-1, verbose=False, show_progress=False):\n",
                                "    \"\"\"\n",
                                "    使用FCI算法进行因果发现，并应用先验知识。\n",
                                "\n",
                                "    参数:\n",
                                "    - dataset (pd.DataFrame): 输入的数据框，包含'X'、'Y'和其他协变量。\n",
                                "    - alpha (float): 显著性水平，默认值为0.05。\n",
                                "    - indep_test (str): 独立性检验方法，默认值为'fisherz'。\n",
                                "    - kernel (str): 核函数类型，默认值为'linear'。\n",
                                "    - verbose (bool): 是否打印详细输出，默认值为False。\n",
                                "    - show_progress (bool): 是否显示算法进度，默认值为False。\n",
                                "\n",
                                "    返回:\n",
                                "    - adj_df (pd.DataFrame): 因果图的邻接矩阵，格式为pivot_table。\n",
                                "    \"\"\"\n",
                                "    \n",
                                "    # 1. 将数据框转换为numpy.ndarray\n",
                                "    data = dataset.values\n",
                                "\n",
                                "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
                                "    data = handle_multicollinearity(data)\n",
                                "    \n",
                                "    # 2. 定义先验知识：'X' → 'Y'\n",
                                "    # 创建GraphNode对象\n",
                                "    try:\n",
                                "        node_X = GraphNode('X')\n",
                                "        node_Y = GraphNode('Y')\n",
                                "    except Exception as e:\n",
                                "        raise ValueError(\"确保数据框中包含名为'X'和'Y'的列。\") from e\n",
                                "    \n",
                                "    # 初始化BackgroundKnowledge对象并添加先验知识\n",
                                "    bk = BackgroundKnowledge().add_required_by_node(node_X, node_Y)\n",
                                "\n",
                                "    # 3. 配置核参数\n",
                                "    if indep_test == 'kci':\n",
                                "        if kernel is None:\n",
                                "            kernel = 'linear'\n",
                                "        if kernel == 'linear':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Linear', \n",
                                "                'kernelY': 'Linear', \n",
                                "                'kernelZ': 'Linear', \n",
                                "                'approx': True,           # 使用伽玛近似\n",
                                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        elif kernel == 'polynomial':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Polynomial', \n",
                                "                'kernelY': 'Polynomial', \n",
                                "                'kernelZ': 'Polynomial', \n",
                                "                'polyd': 3,               # 多项式次数设置为3\n",
                                "                'approx': True,           # 使用伽玛近似\n",
                                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        elif kernel == 'gaussian':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Gaussian', \n",
                                "                'kernelY': 'Gaussian', \n",
                                "                'kernelZ': 'Gaussian', \n",
                                "                'est_width': 'empirical', # 使用经验宽度\n",
                                "                'approx': True,           # 使用伽玛近似\n",
                                "                'nullss': 1000,          # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        elif kernel == 'mix':\n",
                                "            kernel_kwargs = {\n",
                                "                'kernelX': 'Polynomial', \n",
                                "                'kernelY': 'Polynomial', \n",
                                "                'kernelZ': 'Gaussian',     # Z使用高斯核\n",
                                "                'polyd': 3,                # 多项式次数设置为3\n",
                                "                'est_width': 'median',     # Z的高斯核带宽使用中位数技巧\n",
                                "                'approx': True,            # 使用伽玛近似\n",
                                "                'nullss': 1000,           # 原假设下模拟的样本量\n",
                                "            }\n",
                                "        else:\n",
                                "            raise ValueError(f'Unknown kernel: {kernel}')\n",
                                "    else:\n",
                                "        kernel_kwargs = {}\n",
                                "\n",
                                "    # 4. 运行FCI算法，传入先验知识\n",
                                "    try:\n",
                                "        g, edges = fci(data, \n",
                                "                alpha=alpha, \n",
                                "                independence_test_method=indep_test, \n",
                                "                depth=depth,\n",
                                "                max_path_length=max_path_length,\n",
                                "                background_knowledge=bk, \n",
                                "                verbose=verbose, \n",
                                "                show_progress=show_progress,\n",
                                "                **kernel_kwargs\n",
                                "        )\n",
                                "\n",
                                "        # 5. 提取邻接矩阵\n",
                                "        adj_matrix = g.graph\n",
                                "    except Exception as e:\n",
                                "        adj_matrix = np.zeros((data.shape[1], data.shape[1]))\n",
                                "    \n",
                                "    # 6. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
                                "    adj_df = pd.DataFrame(adj_matrix, index=dataset.columns, columns=dataset.columns)\n",
                                "    \n",
                                "    return adj_df\n",
                                "\n",
                                "def FCI_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    estimate_adj_df_bidirectional = FCI_estimate(dataset)  # PAG\n",
                                "    estimate_adj_df_dag = estimate_adj_df_bidirectional.astype('int')\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # 检查变量与'X'和'Y'之间的边\n",
                                "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
                                "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
                                "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
                                "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
                                "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"FCI(v,X)\": v_to_X,\n",
                                "            \"FCI(X,v)\": X_to_v,\n",
                                "            \"FCI(v,Y)\": v_to_Y,\n",
                                "            \"FCI(Y,v)\": Y_to_v,\n",
                                "            \"FCI(X,Y)\": X_to_Y\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # # One-hot encode the edge types (-1, 0, 1, 2)\n",
                                "    # one_hot_columns = [\"FCI(v,X)\", \"FCI(X,v)\", \"FCI(v,Y)\", \"FCI(Y,v)\"]\n",
                                "    \n",
                                "    # for col in one_hot_columns:\n",
                                "    #     one_hot = pd.get_dummies(df[col], prefix=col)\n",
                                "    #     df = pd.concat([df, one_hot], axis=1)\n",
                                "\n",
                                "    # # Remove original edge type columns after one-hot encoding\n",
                                "    # df = df.drop(columns=one_hot_columns)\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\", \"variable\"] + [col for col in df.columns if col not in [\"dataset\", \"variable\"]]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def GRaSP_estimate(dataset, score_func='local_score_BIC', maxP=None):    \n",
                                "    # 1. 将数据框转换为numpy.ndarray\n",
                                "    data = dataset.values\n",
                                "    # 检验相关系数是否奇异，如果存在多重共线性，对存在多重共线性的变量添加随机扰动\n",
                                "    data = handle_multicollinearity(data)\n",
                                "    \n",
                                "    parameters = {\n",
                                "        'kfold': 2,         # 2 折交叉验证\n",
                                "        'lambda': 0.01      # 正则化参数\n",
                                "    }\n",
                                "\n",
                                "    # 2. 运行PC算法，传入先验知识\n",
                                "    G = grasp(\n",
                                "        data, \n",
                                "        score_func=score_func, \n",
                                "        maxP=maxP, \n",
                                "        parameters=parameters\n",
                                "    )\n",
                                "\n",
                                "    # 3. 获取邻接矩阵\n",
                                "    adj_matrix = G.graph\n",
                                "    \n",
                                "    # 4. 将邻接矩阵转换为pandas DataFrame，并设置行列索引为原数据框的列名\n",
                                "    adj_df = pd.DataFrame(adj_matrix, index=dataset.columns, columns=dataset.columns)\n",
                                "    \n",
                                "    return adj_df\n",
                                "\n",
                                "def GRaSP_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"]).tolist()\n",
                                "\n",
                                "    estimate_adj_df = GRaSP_estimate(dataset)\n",
                                "    estimate_adj_df_dag = estimate_adj_df.astype(int)  # 将邻接矩阵转换为整数类型\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # 检查变量与'X'和'Y'之间的边\n",
                                "        v_to_X = estimate_adj_df_dag.loc[variable, 'X']\n",
                                "        X_to_v = estimate_adj_df_dag.loc['X', variable]\n",
                                "        v_to_Y = estimate_adj_df_dag.loc[variable, 'Y']\n",
                                "        Y_to_v = estimate_adj_df_dag.loc['Y', variable]\n",
                                "        X_to_Y = estimate_adj_df_dag.loc['X', 'Y']\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"GRaSP(v,X)\": v_to_X,\n",
                                "            \"GRaSP(X,v)\": X_to_v,\n",
                                "            \"GRaSP(v,Y)\": v_to_Y,\n",
                                "            \"GRaSP(Y,v)\": Y_to_v,\n",
                                "            \"GRaSP(X,Y)\": X_to_Y,\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"DML\"\"\"\n",
                                "def DML_estimate(T_, Y_, X_, data):\n",
                                "    # 设置处理变量、结果变量和控制变量\n",
                                "    T = data[T_].values\n",
                                "    Y = data[Y_].values\n",
                                "    X = data[X_].values\n",
                                "\n",
                                "    # 定义 LassoCV 作为第一阶段模型\n",
                                "    model_t = LassoCV(random_state=42, n_jobs=None)\n",
                                "    model_y = LassoCV(random_state=42, n_jobs=None)\n",
                                "\n",
                                "    # # 定义 \"forest\" 作为第二阶段模型\n",
                                "    # model_t = \"forest\"\n",
                                "    # model_y = \"forest\"\n",
                                "\n",
                                "    # 初始化 CausalForestDML 使用自定义的估计器\n",
                                "    model = CausalForestDML(model_t=model_t, model_y=model_y,\n",
                                "        cv=4, n_estimators=36, n_jobs=None, random_state=42, inference=True)\n",
                                "    \n",
                                "    # # 初始化 LinearDML 使用自定义的估计器\n",
                                "    # model = LinearDML(model_t=model_t, model_y=model_y, \n",
                                "    #     cv=4, n_jobs=None, random_state=42, inference=True)\n",
                                "    \n",
                                "    model.fit(Y, T, X=X)\n",
                                "\n",
                                "    # 平均边际处理效应的推断结果\n",
                                "    amte_inference = model.ate_inference(X=X)\n",
                                "\n",
                                "    return amte_inference\n",
                                "\n",
                                "def DML_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        # 判断v-X的因果效应，设置variables中的其他v和Y为控制变量\n",
                                "        amte_inference1 = DML_estimate(variable, \"X\", [\"Y\"] + list(variables.drop(variable)), dataset)\n",
                                "        # 判断v-Y的因果效应，设置variables中的其他v和X为控制变量\n",
                                "        amte_inference2 = DML_estimate(variable, \"Y\", [\"X\"] + list(variables.drop(variable)), dataset)\n",
                                "        # 判断X-v的因果效应，设置variables中的其他v和Y为控制变量\n",
                                "        amte_inference3 = DML_estimate(\"X\", variable, [\"Y\"] + list(variables.drop(variable)), dataset)\n",
                                "        # 判断Y-v的因果效应，设置variables中的其他v和X为控制变量\n",
                                "        amte_inference4 = DML_estimate(\"Y\", variable, [\"X\"] + list(variables.drop(variable)), dataset)\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"v~X_DML_AMTE\": amte_inference1.mean_point,  \n",
                                "            \"v~X_DML_AMTE_zstat\": amte_inference1.zstat(),\n",
                                "            \"v~X_DML_AMTE_pvalue\": amte_inference1.pvalue(),\n",
                                "            \"v~X_DML_std_point\": amte_inference1.std_point,\n",
                                "            \"v~X_DML_stderr_point\": amte_inference1.stderr_point,\n",
                                "\n",
                                "            \"v~Y_DML_AMTE\": amte_inference2.mean_point,\n",
                                "            \"v~Y_DML_AMTE_zstat\": amte_inference2.zstat(),\n",
                                "            \"v~Y_DML_AMTE_pvalue\": amte_inference2.pvalue(),\n",
                                "            \"v~Y_DML_std_point\": amte_inference2.std_point,\n",
                                "            \"v~Y_DML_stderr_point\": amte_inference2.stderr_point,\n",
                                "\n",
                                "            \"X~v_DML_AMTE\": amte_inference3.mean_point,\n",
                                "            \"X~v_DML_AMTE_zstat\": amte_inference3.zstat(),\n",
                                "            \"X~v_DML_AMTE_pvalue\": amte_inference3.pvalue(),\n",
                                "            \"X~v_DML_std_point\": amte_inference3.std_point,\n",
                                "            \"X~v_DML_stderr_point\": amte_inference3.stderr_point,\n",
                                "\n",
                                "            \"Y~v_DML_AMTE\": amte_inference4.mean_point,\n",
                                "            \"Y~v_DML_AMTE_zstat\": amte_inference4.zstat(),\n",
                                "            \"Y~v_DML_AMTE_pvalue\": amte_inference4.pvalue(),\n",
                                "            \"Y~v_DML_std_point\": amte_inference4.std_point,\n",
                                "            \"Y~v_DML_stderr_point\": amte_inference4.stderr_point,\n",
                                "        })\n",
                                "    \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "\"\"\"PSM\"\"\"\n",
                                "def PSM_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        match = PSMatch(dataset, Y_col='X', T_col=variable, k=3)\n",
                                "        KL_v_X = match.run()\n",
                                "        # match = PSMatch(dataset, Y_col='Y', T_col=variable, k=3)\n",
                                "        # KL_v_Y = match.run()\n",
                                "        # match = PSMatch(dataset, Y_col=variable, T_col='X', k=3)\n",
                                "        # KL_X_v = match.run()\n",
                                "        # match = PSMatch(dataset, Y_col=variable, T_col='Y', k=3)\n",
                                "        # KL_Y_v = match.run()\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"PSM_KL(v,X)\": KL_v_X,\n",
                                "            # \"PSM_KL(v,Y)\": KL_v_Y,\n",
                                "            # \"PSM_KL(X,v)\": KL_X_v,\n",
                                "            # \"PSM_KL(Y,v)\": KL_Y_v,\n",
                                "        })\n",
                                "    \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "###########################################################################\n",
                                "##################################检验特征##################################\n",
                                "###########################################################################\n",
                                "def sobel_test(X, Y, M):\n",
                                "    # Fit X -> M\n",
                                "    X_const = np.column_stack((np.ones(len(X)), X))\n",
                                "    beta_a, _, _, _ = np.linalg.lstsq(X_const, M, rcond=None)\n",
                                "    a = beta_a[1]\n",
                                "    \n",
                                "    # Fit X, M -> Y\n",
                                "    XM_const = np.column_stack((np.ones(len(X)), X, M))\n",
                                "    beta_b, _, _, _ = np.linalg.lstsq(XM_const, Y, rcond=None)\n",
                                "    b = beta_b[2]\n",
                                "    \n",
                                "    # Standard errors\n",
                                "    se_a = np.sqrt(np.sum((M - np.dot(X_const, beta_a))**2) / (len(X) - 2)) / np.sqrt(np.sum((X - np.mean(X))**2))\n",
                                "    se_b = np.sqrt(np.sum((Y - np.dot(XM_const, beta_b))**2) / (len(X) - 3)) / np.sqrt(np.sum((M - np.mean(M))**2))\n",
                                "    \n",
                                "    # Sobel test statistic\n",
                                "    sobel_t = a*b / np.sqrt(b**2 * se_a**2 + a**2 * se_b**2)\n",
                                "    \n",
                                "    # Two-tailed p-value\n",
                                "    p_value = 2 * (1 - stats.norm.cdf(abs(sobel_t)))\n",
                                "    \n",
                                "    return sobel_t, p_value\n",
                                "\n",
                                "def sobel_test_feature(dataset):\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "    \n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        sobel_t, p_value = sobel_test(dataset[\"X\"], dataset[\"Y\"], dataset[variable])\n",
                                "        \n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"sobel_test_statistic\": sobel_t,\n",
                                "            \"sobel_test_p_value\": p_value\n",
                                "        })\n",
                                "    \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "    \n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "    \n",
                                "    return df\n",
                                "\n",
                                "def mediation_effect(data, x, m, y):\n",
                                "    try:\n",
                                "        # Path a: X -> M\n",
                                "        a = stats.linregress(data[x], data[m]).slope\n",
                                "        \n",
                                "        # Path b and c': M -> Y controlling for X\n",
                                "        model = stats.linregress(data[[x, m]], data[y])\n",
                                "        b = model.slope[1]  # Effect of M on Y\n",
                                "        c_prime = model.slope[0]  # Direct effect of X on Y\n",
                                "        \n",
                                "        # Indirect effect\n",
                                "        ab = a * b\n",
                                "        \n",
                                "        return ab, c_prime\n",
                                "    except Exception as e:\n",
                                "        print(f\"Error in mediation_effect: {e}\")\n",
                                "        print(f\"x: {x}, m: {m}, y: {y}\")\n",
                                "        print(f\"Data types: X: {data[x].dtype}, M: {data[m].dtype}, Y: {data[y].dtype}\")\n",
                                "        print(f\"Data shape: {data.shape}\")\n",
                                "        raise\n",
                                "    \n",
                                "def bootstrap_mediation_feature(dataset, num_bootstrap=200):\n",
                                "    variables = dataset.columns.drop(['X', 'Y'])\n",
                                "    df = []\n",
                                "\n",
                                "    for variable in variables:\n",
                                "        # 使用 pingouin 进行中介分析\n",
                                "        try:\n",
                                "            mediation_results = pg.mediation_analysis(data=dataset, x='X', m=variable, y='Y', alpha=0.05, n_boot=num_bootstrap)\n",
                                "\n",
                                "            # 提取间接效应和直接效应的估计值和置信区间\n",
                                "            indirect_effect = mediation_results.loc[mediation_results['path'] == 'Indirect', 'coef'].values[0]\n",
                                "            indirect_ci_lower = mediation_results.loc[mediation_results['path'] == 'Indirect', 'CI[2.5%]'].values[0]\n",
                                "            indirect_ci_upper = mediation_results.loc[mediation_results['path'] == 'Indirect', 'CI[97.5%]'].values[0]\n",
                                "\n",
                                "            direct_effect = mediation_results.loc[mediation_results['path'] == 'Direct', 'coef'].values[0]\n",
                                "            direct_ci_lower = mediation_results.loc[mediation_results['path'] == 'Direct', 'CI[2.5%]'].values[0]\n",
                                "            direct_ci_upper = mediation_results.loc[mediation_results['path'] == 'Direct', 'CI[97.5%]'].values[0]\n",
                                "\n",
                                "            total_effect = mediation_results.loc[mediation_results['path'] == 'Total', 'coef'].values[0]\n",
                                "\n",
                                "            # 将结果添加到列表中\n",
                                "            df.append({\n",
                                "                \"variable\": variable,\n",
                                "                \"bootstrap_indirect_effect_mean\": indirect_effect,\n",
                                "                \"bootstrap_indirect_effect_ci_lower\": indirect_ci_lower,\n",
                                "                \"bootstrap_indirect_effect_ci_upper\": indirect_ci_upper,\n",
                                "                \"bootstrap_direct_effect_mean\": direct_effect,\n",
                                "                \"bootstrap_direct_effect_ci_lower\": direct_ci_lower,\n",
                                "                \"bootstrap_direct_effect_ci_upper\": direct_ci_upper,\n",
                                "                \"bootstrap_total_effect\": total_effect\n",
                                "            })\n",
                                "        except Exception as e:\n",
                                "            df.append({\n",
                                "                \"variable\": variable,\n",
                                "                \"bootstrap_indirect_effect_mean\": None,\n",
                                "                \"bootstrap_indirect_effect_ci_lower\": None,\n",
                                "                \"bootstrap_indirect_effect_ci_upper\": None,\n",
                                "                \"bootstrap_direct_effect_mean\": None,\n",
                                "                \"bootstrap_direct_effect_ci_lower\": None,\n",
                                "                \"bootstrap_total_effect\": None,\n",
                                "            })\n",
                                "    \n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # 调整列的顺序\n",
                                "    df = df[[\"dataset\", \"variable\"] + [col for col in df.columns if col not in [\"dataset\", \"variable\"]]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "###########################################################################\n",
                                "##################################结构方程特征##############################\n",
                                "###########################################################################\n",
                                "def sem_features(dataset):\n",
                                "    \"\"\"\n",
                                "    针对每个变量 v，构建八种 SEM 模型，计算模型拟合指标，生成特征。\n",
                                "\n",
                                "    参数：\n",
                                "    - dataset: 包含 X, Y, v1, v2, ... 的 pandas DataFrame\n",
                                "\n",
                                "    返回：\n",
                                "    - 包含 SEM 特征的 pandas DataFrame\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop(['X', 'Y'])\n",
                                "    df = []\n",
                                "\n",
                                "    # 定义八种关系的 SEM 模型描述\n",
                                "    model_templates = {\n",
                                "        \"Confounder\": \"\"\"\n",
                                "            X ~ a1*v\n",
                                "            Y ~ b1*X + b2*v\n",
                                "            X ~~ X\n",
                                "            v ~~ v\n",
                                "            Y ~~ Y\n",
                                "        \"\"\",\n",
                                "        \"Collider\": \"\"\"\n",
                                "            v ~ a1*X + a2*Y\n",
                                "            Y ~ b1*X\n",
                                "            X ~~ X\n",
                                "            v ~~ v\n",
                                "            Y ~~ Y\n",
                                "        \"\"\",\n",
                                "        \"Mediator\": \"\"\"\n",
                                "            v ~ a1*X\n",
                                "            Y ~ b1*v\n",
                                "            X ~~ X\n",
                                "            v ~~ v\n",
                                "            Y ~~ Y\n",
                                "        \"\"\",\n",
                                "        \"Cause of X\": \"\"\"\n",
                                "            X ~ a1*v\n",
                                "            Y ~ b1*X\n",
                                "            X ~~ X\n",
                                "            v ~~ v\n",
                                "            Y ~~ Y\n",
                                "        \"\"\",\n",
                                "        \"Cause of Y\": \"\"\"\n",
                                "            Y ~ a1*X + a2*v\n",
                                "            X ~~ X\n",
                                "            v ~~ v\n",
                                "            Y ~~ Y\n",
                                "        \"\"\",\n",
                                "        \"Consequence of X\": \"\"\"\n",
                                "            v ~ a1*X\n",
                                "            Y ~ b1*X\n",
                                "            X ~~ X\n",
                                "            v ~~ v\n",
                                "            Y ~~ Y\n",
                                "        \"\"\",\n",
                                "        \"Consequence of Y\": \"\"\"\n",
                                "            v ~ a1*Y\n",
                                "            Y ~ b1*X\n",
                                "            X ~~ X\n",
                                "            v ~~ v\n",
                                "            Y ~~ Y\n",
                                "        \"\"\",\n",
                                "        # \"Independent\": \"\"\"\n",
                                "        #     Y ~ a1*X\n",
                                "        #     X ~~ X\n",
                                "        #     v ~~ v\n",
                                "        #     Y ~~ Y\n",
                                "        # \"\"\"\n",
                                "    }\n",
                                "\n",
                                "    # 对于每个变量 v，构建并拟合八种模型\n",
                                "    for variable in variables:\n",
                                "        # 存储每种模型的拟合指标\n",
                                "        fit_indices_list = []\n",
                                "        for label, model_desc_template in model_templates.items():\n",
                                "            # 替换模型描述中的变量名\n",
                                "            model_desc = model_desc_template.replace('v', variable)\n",
                                "            try:\n",
                                "                # 创建并拟合模型\n",
                                "                model = Model(model_desc)\n",
                                "                # 使用全局优化器，以提高模型拟合的稳定性, 关闭所有警告\n",
                                "                model.fit(dataset, solver='SLSQP')\n",
                                "                # 获取模型拟合指标\n",
                                "                # fit_indices = inspect(model)\n",
                                "                stats = semopy.calc_stats(model)\n",
                                "                # 提取常用的拟合指标\n",
                                "                \n",
                                "                fit_metrics = {\n",
                                "                    'AIC': stats['AIC'].loc['Value'],\n",
                                "                    'BIC': stats['BIC'].loc['Value'],\n",
                                "                    'CFI': stats['CFI'].loc['Value'],\n",
                                "                    'TLI': stats['TLI'].loc['Value'],\n",
                                "                    'RMSEA': stats['RMSEA'].loc['Value'],\n",
                                "                    # 'chi2': stats['chi2'].loc['Value'],\n",
                                "                    # 'chi2_baseline': stats['chi2 Baseline'].loc['Value'],\n",
                                "                    # 'NFI': stats['NFI'].loc['Value'],\n",
                                "                    # 'LogLik': stats['LogLik'].loc['Value'],\n",
                                "                }\n",
                                "            except Exception as e:\n",
                                "                # 如果模型无法收敛，设置拟合指标为缺失值\n",
                                "                print(e)\n",
                                "                print(model_desc_template)\n",
                                "                fit_metrics = {\n",
                                "                    'AIC': None,\n",
                                "                    'BIC': None,\n",
                                "                    'CFI': None,\n",
                                "                    'TLI': None,\n",
                                "                    'RMSEA': None,\n",
                                "                    # 'chi2': None,\n",
                                "                    # 'chi2_baseline': None,\n",
                                "                    # 'NFI': None,\n",
                                "                    # \"logLik\": None,\n",
                                "                }\n",
                                "            fit_metrics['Model'] = label\n",
                                "            fit_indices_list.append(fit_metrics)\n",
                                "        # 将拟合指标列表转换为 DataFrame\n",
                                "        fit_df = pd.DataFrame(fit_indices_list)\n",
                                "        fit_df['variable'] = variable\n",
                                "        df.append(fit_df)\n",
                                "\n",
                                "    # 合并所有变量的结果\n",
                                "    result_df = pd.concat(df, ignore_index=True)\n",
                                "\n",
                                "    # 将模型名称和变量名称组合，展开为列\n",
                                "    pivot_df = result_df.pivot(index='variable', columns='Model')\n",
                                "    pivot_df.columns = ['_'.join(col).strip() for col in pivot_df.columns.values]\n",
                                "    pivot_df.reset_index(inplace=True)\n",
                                "    pivot_df['dataset'] = dataset.name\n",
                                "\n",
                                "    # 返回结果 DataFrame\n",
                                "    return pivot_df\n",
                                "\n",
                                "# 工具函数：替换模型描述中的变量名\n",
                                "def replace_v_in_sem(v0, variables, model_desc_template):\n",
                                "    variables = variables.tolist()\n",
                                "    variables.remove(v0)\n",
                                "    print(v0, variables)\n",
                                "    variables_str_right = ' + '.join(variables)\n",
                                "    variables_str_left = ' , '.join(variables)\n",
                                "    model_desc = model_desc_template.replace('v0', v0).replace('variables_l', variables_str_left).replace('variables', variables_str_right)\n",
                                "    return model_desc\n",
                                "\n",
                                "# 更复杂的结构方程，感觉没用\n",
                                "def sem_features_v2(dataset):\n",
                                "    \"\"\"\n",
                                "    针对每个变量 v，构建八种 SEM 模型，计算模型拟合指标，生成特征。\n",
                                "\n",
                                "    参数：\n",
                                "    - dataset: 包含 X, Y, v1, v2, ... 的 pandas DataFrame\n",
                                "\n",
                                "    返回：\n",
                                "    - 包含 SEM 特征的 pandas DataFrame\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop(['X', 'Y'])\n",
                                "    df = []\n",
                                "\n",
                                "    # 定义八种关系的 SEM 模型描述\n",
                                "    model_templates = {\n",
                                "        \"Confounder\": \"\"\"\n",
                                "            X ~ v0 + variables\n",
                                "            Y ~ X + v0 + variables\n",
                                "            X ~~ X\n",
                                "            Y ~~ Y\n",
                                "            v0 ~~ v0 + variables\n",
                                "        \"\"\",\n",
                                "        \"Collider\": \"\"\"\n",
                                "            v0 ~ X + Y\n",
                                "            X ~ variables\n",
                                "            Y ~ X + variables\n",
                                "            X ~~ X\n",
                                "            Y ~~ Y\n",
                                "            v0 ~~ v0 + variables\n",
                                "        \"\"\",\n",
                                "        \"Mediator\": \"\"\"\n",
                                "            v0 ~ X\n",
                                "            X ~ variables\n",
                                "            Y ~ v0 + X + variables\n",
                                "            X ~~ X\n",
                                "            Y ~~ Y\n",
                                "            v0 ~~ v0 + variables\n",
                                "        \"\"\",\n",
                                "        \"Cause of X\": \"\"\"\n",
                                "            X ~ v0 + variables\n",
                                "            Y ~ X + variables\n",
                                "            X ~~ X\n",
                                "            Y ~~ Y\n",
                                "            v0 ~~ v0 + variables\n",
                                "        \"\"\",\n",
                                "        \"Cause of Y\": \"\"\"\n",
                                "            X ~ v0 + variables\n",
                                "            Y ~ X + v0 + variables\n",
                                "            X ~~ X\n",
                                "            Y ~~ Y\n",
                                "            v0 ~~ v0 + variables\n",
                                "        \"\"\",\n",
                                "        \"Consequence of X\": \"\"\"\n",
                                "            v0 ~ X\n",
                                "            X ~ variables\n",
                                "            Y ~ X + variables\n",
                                "            X ~~ X\n",
                                "            Y ~~ Y\n",
                                "            v0 ~~ v0 + variables\n",
                                "        \"\"\",\n",
                                "        \"Consequence of Y\": \"\"\"\n",
                                "            v0 ~ Y\n",
                                "            X ~ variables\n",
                                "            Y ~ X + variables\n",
                                "            X ~~ X\n",
                                "            Y ~~ Y\n",
                                "            v0 ~~ v0 + variables\n",
                                "        \"\"\",\n",
                                "        \"Independent\": \"\"\"\n",
                                "            X ~ variables\n",
                                "            Y ~ X + variables\n",
                                "            X ~~ X\n",
                                "            Y ~~ Y\n",
                                "        \"\"\"\n",
                                "    }\n",
                                "\n",
                                "    # 对于每个变量 v，构建并拟合八种模型\n",
                                "    for variable in variables:\n",
                                "        # 存储每种模型的拟合指标\n",
                                "        fit_indices_list = []\n",
                                "        for label, model_desc_template in model_templates.items():\n",
                                "            # 替换模型描述中的变量名\n",
                                "            model_desc = replace_v_in_sem(variable, variables, model_desc_template)\n",
                                "            try:\n",
                                "                # 创建并拟合模型\n",
                                "                model = Model(model_desc)\n",
                                "                # 使用全局优化器，以提高模型拟合的稳定性, 关闭所有警告\n",
                                "                model.fit(dataset, solver='SLSQP')\n",
                                "                # 获取模型拟合指标\n",
                                "                # fit_indices = inspect(model)\n",
                                "                stats = semopy.calc_stats(model)\n",
                                "                # 提取常用的拟合指标\n",
                                "                \n",
                                "                fit_metrics = {\n",
                                "                    'AIC': stats['AIC'].loc['Value'],\n",
                                "                    # 'BIC': stats['BIC'].loc['Value'],\n",
                                "                    'CFI': stats['CFI'].loc['Value'],\n",
                                "                    'TLI': stats['TLI'].loc['Value'],\n",
                                "                    # 'RMSEA': stats['RMSEA'].loc['Value'],\n",
                                "                }\n",
                                "            except Exception as e:\n",
                                "                # 如果模型无法收敛，设置拟合指标为缺失值\n",
                                "                print(e)\n",
                                "                print(model_desc_template)\n",
                                "                fit_metrics = {\n",
                                "                    'AIC': None,\n",
                                "                    # 'BIC': None,\n",
                                "                    'CFI': None,\n",
                                "                    'TLI': None,\n",
                                "                    # 'RMSEA': None,\n",
                                "                }\n",
                                "            fit_metrics['Model'] = label\n",
                                "            fit_indices_list.append(fit_metrics)\n",
                                "        # 将拟合指标列表转换为 DataFrame\n",
                                "        fit_df = pd.DataFrame(fit_indices_list)\n",
                                "        fit_df['variable'] = variable\n",
                                "        df.append(fit_df)\n",
                                "\n",
                                "    # 合并所有变量的结果\n",
                                "    result_df = pd.concat(df, ignore_index=True)\n",
                                "\n",
                                "    # 将模型名称和变量名称组合，展开为列\n",
                                "    pivot_df = result_df.pivot(index='variable', columns='Model')\n",
                                "    pivot_df.columns = ['_'.join(col).strip() for col in pivot_df.columns.values]\n",
                                "    pivot_df.reset_index(inplace=True)\n",
                                "    pivot_df['dataset'] = dataset.name\n",
                                "\n",
                                "    # 返回结果 DataFrame\n",
                                "    return pivot_df\n",
                                "\n",
                                "from econml.grf import CausalForest\n",
                                "from sklearn.model_selection import train_test_split\n",
                                "\n",
                                "\n",
                                "def causal_forest_features_econml(dataset, dataset_name='default_dataset'):\n",
                                "    \"\"\"\n",
                                "    使用 EconML 的 CausalForest 生成因果森林相关特征，包括 v 对 Y、v 对 X、X 对 v、Y 对 v 的因果效应。\n",
                                "    \n",
                                "    参数：\n",
                                "    - dataset (pd.DataFrame): 包含 'X', 'Y' 以及其他处理变量 (v1, v2, ...) 的数据框。\n",
                                "    - dataset_name (str, optional): 数据集的名称，默认为 'default_dataset'。\n",
                                "    \n",
                                "    返回：\n",
                                "    - cf_features_df (pd.DataFrame): 包含因果森林特征的 DataFrame。\n",
                                "    \"\"\"\n",
                                "    \n",
                                "    variables = dataset.columns.drop(['X', 'Y'])\n",
                                "    df_list = []\n",
                                "\n",
                                "    # 定义要估计的因果效应类型\n",
                                "    causal_directions = [\n",
                                "        ('v', 'Y'),  # v -> Y\n",
                                "        ('v', 'X'),  # v -> X\n",
                                "        ('X', 'v'),  # X -> v\n",
                                "        ('Y', 'v'),  # Y -> v\n",
                                "    ]\n",
                                "\n",
                                "    for v in variables:\n",
                                "        tmp_dict = {'variable': v}\n",
                                "        for treatment, outcome in causal_directions:\n",
                                "            # 动态定义处理变量和结果变量\n",
                                "            variable_name = outcome\n",
                                "            if outcome == 'v':\n",
                                "                outcome = v\n",
                                "            if treatment == 'v':\n",
                                "                T = dataset[v].values\n",
                                "                # 控制变量：除 outcome 外的所有变量\n",
                                "                X_covariates = dataset.drop(columns=[outcome, v]).values\n",
                                "            elif treatment == 'X':\n",
                                "                T = dataset['X'].values\n",
                                "                # 控制变量：除 outcome 外的所有变量\n",
                                "                X_covariates = dataset.drop(columns=[outcome, 'X']).values\n",
                                "            elif treatment == 'Y':\n",
                                "                T = dataset['Y'].values\n",
                                "                # 控制变量：除 outcome 外的所有变量\n",
                                "                X_covariates = dataset.drop(columns=[outcome, 'Y']).values\n",
                                "            else:\n",
                                "                raise ValueError(f\"未知的处理变量: {treatment}\")\n",
                                "            \n",
                                "            Y_val = dataset[outcome].values\n",
                                "            X_train, X_test, T_train, T_test, Y_train, Y_test = train_test_split(\n",
                                "                X_covariates, T, Y_val, test_size=0.2, random_state=42\n",
                                "            )\n",
                                "            # 初始化因果森林模型\n",
                                "            cf = CausalForest(n_estimators=100, random_state=42)\n",
                                "\n",
                                "            # 拟合模型（使用关键字参数）\n",
                                "            cf.fit(X=X_train, T=T_train, y=Y_train)\n",
                                "\n",
                                "            # 预测个体化因果效应（ITE）\n",
                                "            te_pred = cf.predict(X_test)\n",
                                "\n",
                                "            # 计算 ITE 的统计特征\n",
                                "            ite_mean = np.mean(te_pred)\n",
                                "            ite_std = np.std(te_pred)\n",
                                "\n",
                                "            # 提取变量重要性\n",
                                "            var_importance = cf.feature_importances_\n",
                                "\n",
                                "            # 创建特征字典\n",
                                "            tmp_dict.update({\n",
                                "                f'causal_forest_ite_mean_{treatment}_{variable_name}': ite_mean,\n",
                                "                f'causal_forest_ite_std_{treatment}_{variable_name}': ite_std,\n",
                                "            })\n",
                                "\n",
                                "            # # 将变量重要性添加到特征字典\n",
                                "            # control_vars = dataset.drop(columns=[outcome, v] if treatment == 'v' else \n",
                                "            #                            ([outcome, 'X'] if treatment == 'X' else \n",
                                "            #                             ([outcome, 'Y'] if treatment == 'Y' else []))).columns\n",
                                "            # for idx, col_name in enumerate(control_vars):\n",
                                "            #     feature_dict[f'causal_forest_varimp_{col_name}'] = var_importance[idx]\n",
                                "\n",
                                "        df_list.append(tmp_dict)\n",
                                "\n",
                                "    # 转换为 DataFrame\n",
                                "    cf_features_df = pd.DataFrame(df_list)\n",
                                "\n",
                                "    # 添加数据集名称\n",
                                "    cf_features_df['dataset'] = dataset_name\n",
                                "\n",
                                "    return cf_features_df\n",
                                "\n",
                                "\n",
                                "from copy import deepcopy\n",
                                "\n",
                                "class MDLScore:\n",
                                "    def __init__(self, data):\n",
                                "        self.data = data\n",
                                "        self.N = data.shape[0]  # 数据样本数\n",
                                "        self.variables = data.columns.tolist()\n",
                                "        self.state_names = {var: data[var].unique() for var in self.variables}\n",
                                "        self.cardinality = {var: len(states) for var, states in self.state_names.items()}\n",
                                "\n",
                                "    def compute_local_score(self, var, parents):\n",
                                "        \"\"\"\n",
                                "        计算给定变量和其父节点的 MDL 局部得分\n",
                                "        \"\"\"\n",
                                "        data = self.data\n",
                                "        var_states = self.state_names[var]\n",
                                "        r = self.cardinality[var]\n",
                                "\n",
                                "        if not parents:\n",
                                "            # 没有父节点，计算变量的熵\n",
                                "            counts = data[var].value_counts().values\n",
                                "            N_j = counts.sum()\n",
                                "            H = -np.sum((counts / N_j) * np.log2(counts / N_j + 1e-10))\n",
                                "            # 模型描述长度（参数的编码长度）\n",
                                "            mdl_param = 0.5 * np.log2(self.N) * (r - 1)\n",
                                "            # 数据描述长度\n",
                                "            mdl_data = N_j * H\n",
                                "        else:\n",
                                "            # 有父节点，计算条件熵\n",
                                "            parents_states = [self.state_names[p] for p in parents]\n",
                                "            q = np.prod([self.cardinality[p] for p in parents])\n",
                                "\n",
                                "            # 计算条件频数\n",
                                "            grouped = data.groupby([var] + parents).size().unstack(fill_value=0)\n",
                                "            counts = grouped.values\n",
                                "            N_ij = counts.sum(axis=0)\n",
                                "            N_ijk = counts\n",
                                "\n",
                                "            # 条件熵\n",
                                "            H = 0\n",
                                "            for j in range(q):\n",
                                "                counts_j = N_ijk[:, j]\n",
                                "                N_j = N_ij[j]\n",
                                "                if N_j > 0:\n",
                                "                    probs = counts_j / N_j\n",
                                "                    H_j = -np.sum(probs * np.log2(probs + 1e-10))\n",
                                "                    H += N_j * H_j\n",
                                "            # 模型描述长度\n",
                                "            mdl_param = 0.5 * np.log2(self.N) * (q * (r - 1))\n",
                                "            # 数据描述长度\n",
                                "            mdl_data = H\n",
                                "        # 总的 MDL 得分\n",
                                "        mdl_score = mdl_param + mdl_data\n",
                                "        return mdl_score\n",
                                "\n",
                                "def mdl_structure_learning(dataset):\n",
                                "    data = dataset.copy()\n",
                                "    variables = data.columns.tolist()\n",
                                "    variables.remove('X')\n",
                                "    variables.remove('Y')\n",
                                "    mdl_scorer = MDLScore(data)\n",
                                "\n",
                                "    # 初始网络（只包含 X -> Y）\n",
                                "    current_structure = {var: [] for var in data.columns}\n",
                                "    current_structure['Y'] = ['X']\n",
                                "\n",
                                "    # 初始化 MDL 得分\n",
                                "    current_score = sum(mdl_scorer.compute_local_score(var, current_structure[var]) for var in data.columns)\n",
                                "\n",
                                "    # 可操作的变量（除了 X 和 Y）\n",
                                "    candidate_variables = variables.copy()\n",
                                "\n",
                                "    # 贪心搜索算法\n",
                                "    max_iter = 100\n",
                                "    for _ in range(max_iter):\n",
                                "        improved = False\n",
                                "        best_structure = None\n",
                                "        best_score = current_score\n",
                                "\n",
                                "        # 尝试对每个候选变量添加边到 X 或 Y，或者从 X 或 Y 添加边到候选变量\n",
                                "        for var in candidate_variables:\n",
                                "            for target in ['X', 'Y']:\n",
                                "                for direction in ['from_var_to_target', 'from_target_to_var']:\n",
                                "                    new_structure = deepcopy(current_structure)\n",
                                "                    if direction == 'from_var_to_target':\n",
                                "                        if target not in new_structure[var]:\n",
                                "                            new_structure[target].append(var)\n",
                                "                    else:\n",
                                "                        if var not in new_structure[target]:\n",
                                "                            new_structure[var].append(target)\n",
                                "\n",
                                "                    # 检查是否形成环路（简单的环路检测）\n",
                                "                    if forms_cycle(new_structure):\n",
                                "                        continue\n",
                                "\n",
                                "                    # 计算新结构的 MDL 得分\n",
                                "                    try:\n",
                                "                        new_score = sum(mdl_scorer.compute_local_score(v, new_structure[v]) for v in data.columns)\n",
                                "                    except Exception as e:\n",
                                "                        continue\n",
                                "\n",
                                "                    if new_score < best_score:\n",
                                "                        best_score = new_score\n",
                                "                        best_structure = new_structure\n",
                                "                        improved = True\n",
                                "\n",
                                "        if improved:\n",
                                "            current_structure = best_structure\n",
                                "            current_score = best_score\n",
                                "        else:\n",
                                "            break  # 如果没有改进，停止搜索\n",
                                "\n",
                                "    # 构建边列表\n",
                                "    edges = []\n",
                                "    for child, parents in current_structure.items():\n",
                                "        for parent in parents:\n",
                                "            edges.append((parent, child))\n",
                                "\n",
                                "    edge_set = set(edges)\n",
                                "\n",
                                "    # 构建结果 DataFrame\n",
                                "    df = []\n",
                                "    for variable in variables:\n",
                                "        v_to_X = int((variable, 'X') in edge_set)\n",
                                "        X_to_v = int(('X', variable) in edge_set)\n",
                                "        v_to_Y = int((variable, 'Y') in edge_set)\n",
                                "        Y_to_v = int(('Y', variable) in edge_set)\n",
                                "        X_to_Y = int(('X', 'Y') in edge_set)\n",
                                "        Y_to_X = int(('Y', 'X') in edge_set)\n",
                                "\n",
                                "        df.append({\n",
                                "            \"variable\": variable,\n",
                                "            \"MDL(v,X)\": v_to_X,\n",
                                "            \"MDL(X,v)\": X_to_v,\n",
                                "            \"MDL(v,Y)\": v_to_Y,\n",
                                "            \"MDL(Y,v)\": Y_to_v,\n",
                                "            \"MDL(X,Y)\": X_to_Y,\n",
                                "            \"MDL(Y,X)\": Y_to_X\n",
                                "        })\n",
                                "\n",
                                "    df = pd.DataFrame(df)\n",
                                "    df[\"dataset\"] = getattr(dataset, 'name', 'dataset')\n",
                                "\n",
                                "    # 调整列的顺序\n",
                                "    df = df[[\"dataset\"] + [col for col in df.columns if col != \"dataset\"]]\n",
                                "\n",
                                "    return df\n",
                                "\n",
                                "def forms_cycle(structure):\n",
                                "    \"\"\"\n",
                                "    检查给定的结构是否形成环路\n",
                                "    \"\"\"\n",
                                "    from collections import defaultdict, deque\n",
                                "\n",
                                "    graph = defaultdict(list)\n",
                                "    for child, parents in structure.items():\n",
                                "        for parent in parents:\n",
                                "            graph[parent].append(child)\n",
                                "\n",
                                "    visited = set()\n",
                                "    stack = set()\n",
                                "\n",
                                "    def visit(node):\n",
                                "        if node in stack:\n",
                                "            return True  # 有环\n",
                                "        if node in visited:\n",
                                "            return False\n",
                                "        visited.add(node)\n",
                                "        stack.add(node)\n",
                                "        for neighbor in graph[node]:\n",
                                "            if visit(neighbor):\n",
                                "                return True\n",
                                "        stack.remove(node)\n",
                                "        return False\n",
                                "\n",
                                "    for node in structure:\n",
                                "        if visit(node):\n",
                                "            return True\n",
                                "    return False\n",
                                "\n",
                                "\n",
                                "\n",
                                "import statsmodels.api as sm\n",
                                "\n",
                                "def anm_features(dataset):\n",
                                "    \"\"\"\n",
                                "    Given a dataset, compute features based on an additive noise model for each variable v (excluding X and Y).\n",
                                "\n",
                                "    For each variable v:\n",
                                "\n",
                                "    - Fit linear regression v ~ X + X^2, compute residuals ε_vX, compute correlation between ε_vX and X\n",
                                "    - Fit linear regression v ~ Y + Y^2, compute residuals ε_vY, compute correlation between ε_vY and Y\n",
                                "    - Fit linear regression v ~ X + Y + X^2 + Y^2 + X*Y, compute residuals ε_vXY, compute correlations between ε_vXY and X and Y\n",
                                "    - Compute R^2 values for the regressions\n",
                                "    - Fit linear regression Y ~ X + X^2, compute R²_YX\n",
                                "    - Fit linear regression Y ~ X + v + X^2 + v^2 + X*v, compute R²_YXv\n",
                                "    - Compute difference ΔR²_YXv = R²_YXv - R²_YX\n",
                                "\n",
                                "    Collect these features into a DataFrame.\n",
                                "    \"\"\"\n",
                                "    variables = dataset.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df_list = []\n",
                                "\n",
                                "    for variable in variables:\n",
                                "        # Create a dictionary to store features\n",
                                "        features = {\"variable\": variable}\n",
                                "\n",
                                "        # Create polynomial features for X\n",
                                "        X_poly = pd.DataFrame({\n",
                                "            \"const\": 1,\n",
                                "            \"X\": dataset[\"X\"],\n",
                                "            \"X^2\": dataset[\"X\"] ** 2,\n",
                                "        })\n",
                                "        # Regress v on X and X^2\n",
                                "        model_vX = sm.OLS(dataset[variable], X_poly).fit()\n",
                                "        residuals_vX = model_vX.resid\n",
                                "        features[\"R2_v~X+X^2\"] = model_vX.rsquared\n",
                                "        features[\"corr(ε_vX, X)\"] = np.corrcoef(residuals_vX, dataset[\"X\"])[0,1]\n",
                                "\n",
                                "        # Create polynomial features for Y\n",
                                "        Y_poly = pd.DataFrame({\n",
                                "            \"const\": 1,\n",
                                "            \"Y\": dataset[\"Y\"],\n",
                                "            \"Y^2\": dataset[\"Y\"] ** 2,\n",
                                "        })\n",
                                "        # Regress v on Y and Y^2\n",
                                "        model_vY = sm.OLS(dataset[variable], Y_poly).fit()\n",
                                "        residuals_vY = model_vY.resid\n",
                                "        features[\"R2_v~Y+Y^2\"] = model_vY.rsquared\n",
                                "        features[\"corr(ε_vY, Y)\"] = np.corrcoef(residuals_vY, dataset[\"Y\"])[0,1]\n",
                                "\n",
                                "        # Create polynomial and interaction terms for X and Y\n",
                                "        XY_poly = pd.DataFrame({\n",
                                "            \"const\": 1,\n",
                                "            \"X\": dataset[\"X\"],\n",
                                "            \"Y\": dataset[\"Y\"],\n",
                                "            \"X^2\": dataset[\"X\"] ** 2,\n",
                                "            \"Y^2\": dataset[\"Y\"] ** 2,\n",
                                "            \"X*Y\": dataset[\"X\"] * dataset[\"Y\"],\n",
                                "        })\n",
                                "        # Regress v on X, Y, X^2, Y^2, X*Y\n",
                                "        model_vXY = sm.OLS(dataset[variable], XY_poly).fit()\n",
                                "        residuals_vXY = model_vXY.resid\n",
                                "        features[\"R2_v~poly(X,Y)\"] = model_vXY.rsquared\n",
                                "        features[\"corr(ε_vXY, X)\"] = np.corrcoef(residuals_vXY, dataset[\"X\"])[0,1]\n",
                                "        features[\"corr(ε_vXY, Y)\"] = np.corrcoef(residuals_vXY, dataset[\"Y\"])[0,1]\n",
                                "\n",
                                "        # For regressions of Y\n",
                                "\n",
                                "        # Regress Y on X and X^2\n",
                                "        model_YX = sm.OLS(dataset[\"Y\"], X_poly).fit()\n",
                                "        R2_YX = model_YX.rsquared\n",
                                "\n",
                                "        # Regress Y on X, v, and quadratic terms\n",
                                "        Xv_poly = pd.DataFrame({\n",
                                "            \"const\": 1,\n",
                                "            \"X\": dataset[\"X\"],\n",
                                "            variable: dataset[variable],\n",
                                "            \"X^2\": dataset[\"X\"] ** 2,\n",
                                "            f\"{variable}^2\": dataset[variable] ** 2,\n",
                                "            f\"X*{variable}\": dataset[\"X\"] * dataset[variable],\n",
                                "        })\n",
                                "        model_YXv = sm.OLS(dataset[\"Y\"], Xv_poly).fit()\n",
                                "        R2_YXv = model_YXv.rsquared\n",
                                "        features[\"ΔR2_Y~X+v\"] = R2_YXv - R2_YX\n",
                                "\n",
                                "        df_list.append(features)\n",
                                "\n",
                                "    df = pd.DataFrame(df_list)\n",
                                "    df[\"dataset\"] = dataset.name\n",
                                "\n",
                                "    # Reorder columns to put \"dataset\" first\n",
                                "    cols = [\"dataset\"] + [col for col in df.columns if col != \"dataset\"]\n",
                                "\n",
                                "    df = df[cols]\n",
                                "\n",
                                "    return df\n"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 14,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:31:25.402707Z",
                                        "start_time": "2024-09-18T09:31:25.388081Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "def label(adjacency_matrix):\n",
                                "    \"\"\"\n",
                                "    Given a graph as adjacency_matrix, create the class labels of each variable.\n",
                                "    \"\"\"\n",
                                "\n",
                                "    adjacency_graph, adjacency_label = create_graph_label()\n",
                                "    labels = get_labels(adjacency_matrix, adjacency_label)\n",
                                "    variables = adjacency_matrix.columns.drop([\"X\", \"Y\"])\n",
                                "\n",
                                "    df = pd.DataFrame({\n",
                                "        \"variable\": variables,\n",
                                "        \"label\": [labels[variable] for variable in variables],\n",
                                "    })\n",
                                "    df[\"dataset\"] = adjacency_matrix.name\n",
                                "\n",
                                "    # Reorder columns:\n",
                                "    df = df[[\"dataset\"] + [colname for colname in df.columns if colname != \"dataset\"]]\n",
                                "\n",
                                "    return df"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 15,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:31:26.636102Z",
                                        "start_time": "2024-09-18T09:31:26.629666Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "def create_some_columns(names_datasets, function):\n",
                                "    \"\"\"\n",
                                "    Apply an embedding function to a list of datasets.\n",
                                "    \"\"\"\n",
                                "\n",
                                "    df = []\n",
                                "    for name, dataset in tqdm(names_datasets.items()):\n",
                                "        dataset = names_datasets[name]\n",
                                "        dataset.name = name\n",
                                "    \n",
                                "        try:\n",
                                "            df_dataset = function(dataset)\n",
                                "        except ValueError as e:\n",
                                "            print(name, e)\n",
                                "            raise NotImplementedError\n",
                                "\n",
                                "        df_dataset[\"dataset\"] = name\n",
                                "        df.append(df_dataset)\n",
                                "\n",
                                "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
                                "    return df"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 16,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:31:27.391364Z",
                                        "start_time": "2024-09-18T09:31:27.380030Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "def create_some_columns_parallel(names_datasets, function, n_jobs=-1):\n",
                                "    \"\"\"\n",
                                "    Apply an embedding function to a list of datasets.\n",
                                "\n",
                                "    Parallel version.\n",
                                "    \"\"\"\n",
                                "\n",
                                "    def f(name, dataset, function):\n",
                                "        dataset.name = name\n",
                                "        df_dataset = function(dataset)\n",
                                "        df_dataset[\"dataset\"] = name\n",
                                "        return df_dataset\n",
                                "\n",
                                "    df = joblib.Parallel(n_jobs=n_jobs)(\n",
                                "        joblib.delayed(f)(name, dataset, function)\n",
                                "        for name, dataset in tqdm(names_datasets.items())\n",
                                "    )\n",
                                "\n",
                                "    df = pd.concat(df, axis=\"index\").reset_index(drop=True)\n",
                                "    return df"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 17,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:31:28.411964Z",
                                        "start_time": "2024-09-18T09:31:28.402880Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "def create_all_columns(functions_names_datasets, n_jobs=-1, create_dimension_feature = False):\n",
                                "    \"\"\"\n",
                                "    given a dictionary of {function1:names, function2:names,...} apply\n",
                                "    the desired functions to the list of datasets and merge all of them\n",
                                "    in a single X_y_group dataframe.\n",
                                "    \"\"\"\n",
                                "\n",
                                "    columns = []\n",
                                "    if create_dimension_feature:\n",
                                "        dimension_feature = create_some_columns(functions_names_datasets[list(functions_names_datasets.keys())[0]], add_dimension_feature)\n",
                                "        columns.append(dimension_feature)\n",
                                "    \n",
                                "    for function, names_datasets in functions_names_datasets.items():\n",
                                "        print(f\"set: {function.__name__}\")\n",
                                "\n",
                                "        if n_jobs != 1:\n",
                                "            feature_set = create_some_columns_parallel(names_datasets, function, n_jobs=n_jobs)\n",
                                "        else:\n",
                                "            feature_set = create_some_columns(names_datasets, function)\n",
                                "\n",
                                "        columns.append(feature_set)\n",
                                "\n",
                                "    # Merge all feature sets into a single dataframe:\n",
                                "    columns = functools.reduce(\n",
                                "        lambda left, right: pd.merge(left, right, on=[\"dataset\", \"variable\"]),\n",
                                "        columns,\n",
                                "    )\n",
                                "\n",
                                "    return columns"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 18,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:44:41.738202Z",
                                        "start_time": "2024-09-18T09:44:40.143933Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [
                                {
                                        "name": "stdout",
                                        "output_type": "stream",
                                        "text": [
                                                "(142910, 308)\n",
                                                "['dataset', 'variable', 'dimension', 'corr(v,X)', 'corr(v,Y)', 'max(corr(v, others))', 'min(corr(v, others))', 'mean(corr(v, others))', 'std(corr(v, others))', 'corr(X,Y)', 'ttest(v,X)', 'pvalue(ttest(v,X))<=0.05', 'ttest(v,Y)', 'pvalue(ttest(v,Y))<=0.05', 'ttest(X,Y)', 'pvalue(ttest(X,Y))<=0.05', 'MI(v,X)', 'MI(v,Y)', 'max(MI(v, others))', 'min(MI(v, others))', 'mean(MI(v, others))', 'std(MI(v, others))', 'MI(X,Y)', 'label', 'y', 'spearman_corr(v,X)', 'spearman_corr(v,Y)', 'spearman_corr(X,Y)', 'kendall_corr(v,X)', 'kendall_corr(v,Y)', 'kendall_corr(X,Y)', 'dcor(v,X)', 'dcor(v,Y)', 'dcor(X,Y)', 'MI(v,X)^2', 'MI(v,Y)^2', 'MI(X,Y)^2', 'max(MI(v, others))^2', 'min(MI(v, others))^2', 'conditional_MI(v,X|Y)', 'conditional_MI(v,Y|X)', 'conditional_MI(X,Y|v)', 'square_dimension', 'max(spearman_corr(v, others))', 'min(spearman_corr(v, others))', 'mean(spearman_corr(v, others))', 'std(spearman_corr(v, others))', 'max(kendall_corr(v, others))', 'min(kendall_corr(v, others))', 'mean(kendall_corr(v, others))', 'std(kendall_corr(v, others))', 'partial_corr(v,X|Y)', 'partial_corr(v,Y|X)', 'partial_corr(X,Y|v)', 'v~Y_ridge_coefficient', 'v~X_ridge_coefficient', 'X~Y_ridge_coefficient', 'ExactSearch(v,X)', 'ExactSearch(X,v)', 'ExactSearch(v,Y)', 'ExactSearch(Y,v)', 'ExactSearch(X,Y)', 'PC(v,X)', 'PC(X,v)', 'PC(v,Y)', 'PC(Y,v)', 'PC(X,Y)', 'FCI(v,X)', 'FCI(X,v)', 'FCI(v,Y)', 'FCI(Y,v)', 'FCI(X,Y)', 'PPS(v,X)', 'PPS(X,v)', 'PPS(v,Y)', 'PPS(Y,v)', 'PPS(X,Y)', 'max(PPS(v,others))', 'mean(PPS(v,others))', 'copula_entropy(v,X)', 'copula_entropy(v,Y)', 'copula_entropy(X,Y)', 'v~Y_coefficient', 'v_squared~Y_coefficient', 'v*X~Y_coefficient', 'v_cos~Y_coefficient', 'v_sin~Y_coefficient', 'v~X_coefficient', 'v_squared~X_coefficient', 'v_cos~X_coefficient', 'v_sin~X_coefficient', 'X~v_coefficient', 'X_squared~v_coefficient', 'X_cos~v_coefficient', 'X_sin~v_coefficient', 'Y~v_coefficient', 'Y_squared~v_coefficient', 'Y_cos~v_coefficient', 'Y_sin~v_coefficient', 'X*Y~v_coefficient', 'X~Y_coefficient', 'v~X_DML_AMTE', 'v~X_DML_AMTE_zstat', 'v~X_DML_AMTE_pvalue', 'v~X_DML_std_point', 'v~X_DML_stderr_point', 'v~Y_DML_AMTE', 'v~Y_DML_AMTE_zstat', 'v~Y_DML_AMTE_pvalue', 'v~Y_DML_std_point', 'v~Y_DML_stderr_point', 'X~v_DML_AMTE', 'X~v_DML_AMTE_zstat', 'X~v_DML_AMTE_pvalue', 'X~v_DML_std_point', 'X~v_DML_stderr_point', 'Y~v_DML_AMTE', 'Y~v_DML_AMTE_zstat', 'Y~v_DML_AMTE_pvalue', 'Y~v_DML_std_point', 'Y~v_DML_stderr_point', 'v~X_piecewise_coef1', 'v~X_piecewise_coef2', 'v~Y_piecewise_coef1', 'v~Y_piecewise_coef2', 'X~v_piecewise_coef1', 'X~v_piecewise_coef2', 'Y~v_piecewise_coef1', 'Y~v_piecewise_coef2', 'v~X_sliding_coef1', 'v~X_sliding_coef2', 'v~X_sliding_coef3', 'v~X_sliding_coef4', 'v~X_sliding_coef5', 'v~Y_sliding_coef1', 'v~Y_sliding_coef2', 'v~Y_sliding_coef3', 'v~Y_sliding_coef4', 'v~Y_sliding_coef5', 'X~v_sliding_coef1', 'X~v_sliding_coef2', 'X~v_sliding_coef3', 'X~v_sliding_coef4', 'X~v_sliding_coef5', 'Y~v_sliding_coef1', 'Y~v_sliding_coef2', 'Y~v_sliding_coef3', 'Y~v_sliding_coef4', 'Y~v_sliding_coef5', 'GRaSP(v,X)', 'GRaSP(X,v)', 'GRaSP(v,Y)', 'GRaSP(Y,v)', 'GRaSP(X,Y)', 'v~X_piecewise_coef3', 'v~X_piecewise_coef4', 'v~Y_piecewise_coef3', 'v~Y_piecewise_coef4', 'X~v_piecewise_coef3', 'X~v_piecewise_coef4', 'Y~v_piecewise_coef3', 'Y~v_piecewise_coef4', 'v~X_quadratic_coef1', 'v~X_quadratic_coef2', 'v~X_quadratic_coef3', 'v~X_quadratic_coef4', 'v~Y_quadratic_coef1', 'v~Y_quadratic_coef2', 'v~Y_quadratic_coef3', 'v~Y_quadratic_coef4', 'X~v_quadratic_coef1', 'X~v_quadratic_coef2', 'X~v_quadratic_coef3', 'X~v_quadratic_coef4', 'Y~v_quadratic_coef1', 'Y~v_quadratic_coef2', 'Y~v_quadratic_coef3', 'Y~v_quadratic_coef4', 'AIC_Cause of X', 'AIC_Cause of Y', 'AIC_Collider', 'AIC_Confounder', 'AIC_Consequence of X', 'AIC_Consequence of Y', 'AIC_Mediator', 'BIC_Cause of X', 'BIC_Cause of Y', 'BIC_Collider', 'BIC_Confounder', 'BIC_Consequence of X', 'BIC_Consequence of Y', 'BIC_Mediator', 'CFI_Cause of X', 'CFI_Cause of Y', 'CFI_Collider', 'CFI_Confounder', 'CFI_Consequence of X', 'CFI_Consequence of Y', 'CFI_Mediator', 'TLI_Cause of X', 'TLI_Cause of Y', 'TLI_Collider', 'TLI_Confounder', 'TLI_Consequence of X', 'TLI_Consequence of Y', 'TLI_Mediator', 'RMSEA_Cause of X', 'RMSEA_Cause of Y', 'RMSEA_Collider', 'RMSEA_Confounder', 'RMSEA_Consequence of X', 'RMSEA_Consequence of Y', 'RMSEA_Mediator', 'max(abs_rolling_corr(v, X))', 'min(abs_rolling_corr(v, X))', 'mean(abs_rolling_corr(v, X))', 'max(abs_rolling_corr(v, Y))', 'min(abs_rolling_corr(v, Y))', 'mean(abs_rolling_corr(v, Y))', 'variable_mean', 'variable_std', 'X_mean', 'Y_mean', 'grid(v,X)(0,0)', 'grid(v,X)(0,1)', 'grid(v,X)(0,2)', 'grid(v,X)(0,3)', 'grid(v,X)(0,4)', 'grid(v,X)(1,0)', 'grid(v,X)(1,1)', 'grid(v,X)(1,2)', 'grid(v,X)(1,3)', 'grid(v,X)(1,4)', 'grid(v,X)(2,0)', 'grid(v,X)(2,1)', 'grid(v,X)(2,2)', 'grid(v,X)(2,3)', 'grid(v,X)(2,4)', 'grid(v,X)(3,0)', 'grid(v,X)(3,1)', 'grid(v,X)(3,2)', 'grid(v,X)(3,3)', 'grid(v,X)(3,4)', 'grid(v,X)(4,0)', 'grid(v,X)(4,1)', 'grid(v,X)(4,2)', 'grid(v,X)(4,3)', 'grid(v,X)(4,4)', 'grid(v,Y)(0,0)', 'grid(v,Y)(0,1)', 'grid(v,Y)(0,2)', 'grid(v,Y)(0,3)', 'grid(v,Y)(0,4)', 'grid(v,Y)(1,0)', 'grid(v,Y)(1,1)', 'grid(v,Y)(1,2)', 'grid(v,Y)(1,3)', 'grid(v,Y)(1,4)', 'grid(v,Y)(2,0)', 'grid(v,Y)(2,1)', 'grid(v,Y)(2,2)', 'grid(v,Y)(2,3)', 'grid(v,Y)(2,4)', 'grid(v,Y)(3,0)', 'grid(v,Y)(3,1)', 'grid(v,Y)(3,2)', 'grid(v,Y)(3,3)', 'grid(v,Y)(3,4)', 'grid(v,Y)(4,0)', 'grid(v,Y)(4,1)', 'grid(v,Y)(4,2)', 'grid(v,Y)(4,3)', 'grid(v,Y)(4,4)', 'JH(v,x)', 'JH(v,y)', 'CH(v|x)', 'CH(v|y)', 'I(v,x)', 'I(v,y)', 'AMI(v,x)', 'AMI(v,y)', 'NPE(v,x)', 'NPE(v,y)', 'H_baseline', 'H_entropy', 'uniform_div', 'IGCI(v,x)', 'IGCI(v,y)', 'IGCI(x,v)', 'IGCI(y,v)', 'MM(v^2 * x)', 'MM(v^2 * y)', 'MM(v^2 * x^2)', 'MM(v^2 * y^2)', 'MM(v^3 * x)', 'MM(v^3 * y)', 'fit(v,x)', 'fit(v,y)', 'fit_error(v,x)', 'fit_error(v,y)', 'fit_noise_entropy(v,x)', 'fit_noise_entropy(v,y)', 'fit_noise_skewness(v,x)', 'fit_noise_skewness(v,y)', 'fit_noise_kurtosis(v,x)', 'fit_noise_kurtosis(v,y)', 'cond_dist_sim(v,x)', 'cond_dist_sim(v,y)']\n"
                                        ]
                                }
                        ],
                        "source": [
                                "X_y_group_train = pd.read_csv('mid_data/X_y_group_train_updated_v13.7_discrete.csv')\n",
                                "print(X_y_group_train.shape)\n",
                                "print(X_y_group_train.columns.tolist())"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 19,
                        "metadata": {},
                        "outputs": [
                                {
                                        "name": "stdout",
                                        "output_type": "stream",
                                        "text": [
                                                "set: anm_features\n"
                                        ]
                                },
                                {
                                        "name": "stderr",
                                        "output_type": "stream",
                                        "text": [
                                                "100%|██████████| 23500/23500 [06:32<00:00, 59.90it/s]\n"
                                        ]
                                },
                                {
                                        "name": "stdout",
                                        "output_type": "stream",
                                        "text": [
                                                "X_y_group_train.shape (142910, 318)\n",
                                                "去重后X_y_group_train.shape (142910, 316)\n"
                                        ]
                                }
                        ],
                        "source": [
                                "names_datasets_train = X_train\n",
                                "names_graphs_train = y_train\n",
                                "X_y_group_train_additional = create_all_columns(\n",
                                "    {\n",
                                "        # conditional_independence_tests: names_datasets_train,\n",
                                "        # partial_distance_covariance_dataset: names_datasets_train,\n",
                                "        # PC_feature: names_datasets_train,\n",
                                "        # FCI_feature: names_datasets_train,\n",
                                "        # copula_entropy: names_datasets_train,\n",
                                "        # linear_regression_feature: names_datasets_train,\n",
                                "        # DML_feature: names_datasets_train,\n",
                                "        # piecewise_linear_regression_feature: names_datasets_train,\n",
                                "        # rolling_abs_pearson_correlation: names_datasets_train,\n",
                                "        # conditional_independence_tests: names_datasets_train,\n",
                                "        # GRaSP_feature: names_datasets_train, # good feature\n",
                                "        # conditional_mutual_information_new: names_datasets_train,\n",
                                "        # descirbe_feature: names_datasets_train,\n",
                                "        # grid_feature: names_datasets_train,\n",
                                "        # causal_forest_features_econml: names_datasets_train,\n",
                                "        anm_features: names_datasets_train,\n",
                                "    },\n",
                                "    n_jobs=-1,\n",
                                ")\n",
                                "X_y_group_train = pd.concat([X_y_group_train, X_y_group_train_additional], axis=1)\n",
                                "print('X_y_group_train.shape', X_y_group_train.shape)\n",
                                "# 去掉重复的列\n",
                                "X_y_group_train = X_y_group_train.loc[:,~X_y_group_train.columns.duplicated()]\n",
                                "print('去重后X_y_group_train.shape', X_y_group_train.shape)"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": 20,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "X_y_group_train.to_csv('./mid_data/X_y_group_train_updated_v13.11_anm.csv', index=False)\n"
                        ]
                },
                {
                        "cell_type": "markdown",
                        "metadata": {},
                        "source": [
                                "set: double_machine_learning\n",
                                "  1%|          | 288/23500 [02:17<2:48:55,  2.29it/s]"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": null,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:34:49.212104Z",
                                        "start_time": "2024-09-18T09:34:48.478623Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "print(\"Adding numeric labels y\")\n",
                                "le = LabelEncoder()\n",
                                "X_y_group_train[\"y\"] = le.fit_transform(X_y_group_train[\"label\"])\n",
                                "# reordering columns:\n",
                                "X_y_group_train = X_y_group_train[[\"dataset\", \"variable\"] + X_y_group_train.columns.drop([\"dataset\", \"variable\", \"label\", \"y\"]).tolist() + [\"label\", \"y\"]]\n",
                                "\n",
                                "numeric_columns = X_y_group_train.select_dtypes(include=[np.number]).columns\n",
                                "X_y_group_train[numeric_columns] = X_y_group_train[numeric_columns].fillna(X_y_group_train[numeric_columns].mean())\n",
                                "\n",
                                "blacklist = [\"ttest(v,X)\", \"pvalue(ttest(v,X))<=0.05\", \"ttest(v,Y)\", \"pvalue(ttest(v,Y))<=0.05\", \"ttest(X,Y)\", \"pvalue(ttest(X,Y))<=0.05\", \"max(energy_dist(v, others))\", \"min(energy_dist(v, others))\",\"mean(energy_dist(v, others))\", \"std(energy_dist(v, others))\", \"25%(energy_dist(v, others))\", \"75%(energy_dist(v, others))\"]\n",
                                "columns_to_drop = [col for col in blacklist if col in X_y_group_train.columns]\n",
                                "X_y_group_train = X_y_group_train.drop(columns=columns_to_drop)\n",
                                "\n",
                                "display(X_y_group_train)\n",
                                "\n",
                                "print(\"Extracting X_train, y_train, and group\")\n",
                                "X_train = X_y_group_train.drop([\"variable\", \"dataset\", \"label\", \"y\"], axis=\"columns\")\n",
                                "y_train = X_y_group_train[\"y\"]\n",
                                "group_train = X_y_group_train[\"dataset\"]"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": null,
                        "metadata": {
                                "ExecuteTime": {
                                        "end_time": "2024-09-18T09:40:09.797137Z",
                                        "start_time": "2024-09-18T09:39:33.165697Z"
                                },
                                "metadata": {}
                        },
                        "outputs": [],
                        "source": [
                                "model = RandomForestClassifier(\n",
                                "    n_estimators=100,\n",
                                "    max_depth=13,\n",
                                "    n_jobs=-1,\n",
                                "    class_weight=\"balanced\",\n",
                                "    random_state=42\n",
                                ")\n",
                                "display(model)\n",
                                "\n",
                                "cv = GroupKFold(n_splits=4)\n",
                                "results = cross_val_score(\n",
                                "    model,\n",
                                "    X_train,\n",
                                "    y_train,\n",
                                "    groups=group_train,\n",
                                "    cv=cv,\n",
                                "    verbose=True,\n",
                                "    scoring=\"balanced_accuracy\"\n",
                                ")\n",
                                "\n",
                                "print(f\"multiclass balanced accuracy: mean={results.mean()}\")"
                        ]
                },
                {
                        "cell_type": "markdown",
                        "metadata": {},
                        "source": [
                                "before:0.489\n",
                                "after: 0.488, 差别不大...\n",
                                "\n",
                                "0.488120992009518"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": null,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "from sklearn.model_selection import train_test_split\n",
                                "from sklearn.metrics import accuracy_score\n",
                                "import matplotlib.pyplot as plt\n",
                                "\n",
                                "\n",
                                "X = X_train\n",
                                "y = y_train\n",
                                "# Assuming X and y are your features and target variables\n",
                                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                "\n",
                                "model = RandomForestClassifier(\n",
                                "    n_estimators=100,\n",
                                "    max_depth=13,\n",
                                "    n_jobs=-1,\n",
                                "    class_weight=\"balanced\",\n",
                                "    random_state=42\n",
                                ")\n",
                                "# Fit the model\n",
                                "model.fit(X_train, y_train)\n",
                                "\n",
                                "# Get predictions\n",
                                "y_pred = model.predict(X_test)\n",
                                "\n",
                                "\n",
                                "# Calculate accuracy for each dimension\n",
                                "dimensions = sorted(X_test['dimension'].unique())\n",
                                "accuracies = []\n",
                                "\n",
                                "for dim in dimensions:\n",
                                "    mask = X_test['dimension'] == dim\n",
                                "    y_true_dim = y_test[mask]\n",
                                "    y_pred_dim = y_pred[mask]\n",
                                "    acc = accuracy_score(y_true_dim, y_pred_dim)\n",
                                "    accuracies.append(acc)\n",
                                "\n",
                                "# Visualize the results\n",
                                "plt.figure(figsize=(12, 6))\n",
                                "plt.plot(dimensions, accuracies, marker='o')\n",
                                "plt.xlabel('Dimension')\n",
                                "plt.ylabel('Accuracy')\n",
                                "plt.title('Model Accuracy vs Dimension')\n",
                                "plt.grid(True)\n",
                                "plt.xticks(dimensions)  # Ensure all dimensions are shown on x-axis\n",
                                "plt.tight_layout()\n",
                                "plt.show()\n",
                                "\n",
                                "# Print accuracies\n",
                                "print(\"Accuracies for each dimension:\")\n",
                                "for dim, acc in zip(dimensions, accuracies):\n",
                                "    print(f\"Dimension {dim}: Accuracy = {acc:.4f}\")\n",
                                "\n",
                                "# Calculate and print overall accuracy\n",
                                "overall_accuracy = accuracy_score(y_test, y_pred)\n",
                                "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")\n",
                                "\n",
                                "# Get feature importances\n",
                                "importances = model.feature_importances_\n",
                                "feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n",
                                "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
                                "\n",
                                "# Display top 20 most important features\n",
                                "print(\"\\nTop 20 most important features:\")\n",
                                "print(feature_importances)\n",
                                "\n",
                                "\n",
                                "print(\"\\nModel and feature importances have been saved.\")"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": null,
                        "metadata": {},
                        "outputs": [],
                        "source": [
                                "feature_importances"
                        ]
                },
                {
                        "cell_type": "code",
                        "execution_count": null,
                        "metadata": {},
                        "outputs": [],
                        "source": []
                }
        ],
        "metadata": {
                "kernelspec": {
                        "display_name": "adia_py311",
                        "language": "python",
                        "name": "python3"
                },
                "language_info": {
                        "codemirror_mode": {
                                "name": "ipython",
                                "version": 3
                        },
                        "file_extension": ".py",
                        "mimetype": "text/x-python",
                        "name": "python",
                        "nbconvert_exporter": "python",
                        "pygments_lexer": "ipython3",
                        "version": "3.10.14"
                }
        },
        "nbformat": 4,
        "nbformat_minor": 2
}
